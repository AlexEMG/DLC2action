{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to DLC2Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DLC2Action is a package for automatic behavior prediction. It offers implementation of SOTA models and keeps track of experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how it works, we will experiment on a relatively small [publically available](https://github.com/ETHZ-INS/DLCAnalyzer/tree/master/data/OFT) dataset (Sturman, 2020). Run the code below to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1BhAAy_YYxWXYVSpCmYXIAOEV9waoMwcK\n",
    "!apt-get install unzip\n",
    "!unzip OFT.zip -d OFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... installation\n",
    "\n",
    "for now:\n",
    "```\n",
    "git clone https://github.com/AlexEMG/DLC2Action\n",
    "cd DLC2Action\n",
    "conda create --name DLC2Action python=3.9\n",
    "conda activate DLC2Action\n",
    "python -m pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc2action.project import Project\n",
    "import os\n",
    "\n",
    "# CURRENT_PATH = os.getcwd()\n",
    "# DATA_PATH = os.path.join(CURRENT_PATH, \"OFT\", \"OFT\", \"Output_DLC\")\n",
    "# LABELS_PATH = os.path.join(CURRENT_PATH, \"OFT\", \"OFT\", \"Labels\")\n",
    "# PROJECTS_PATH = os.path.join(CURRENT_PATH, \"DLC2Action\")\n",
    "\n",
    "DATA_PATH = \"/home/liza/data/OFT/Output_DLC\"\n",
    "LABELS_PATH = \"/home/liza/data/OFT/Labels\"\n",
    "PROJECTS_PATH = \"/home/liza/DLC2Action_tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-level methods in DLC2Action are almost exclusively accessed through the `dlc2action.project.Project` class. A project instance should loosely correspond to a specific goal (e.g. generating automatic annotations for dataset A with input format X). You can use it to optimize hyperparameters, run experiments, analyze results and generate new data.\n",
    "\n",
    "**Best practices**\n",
    "- When you need to do something with a different data type or unrelated files, it's better to create a new project to keep the experiment history easy to understand.\n",
    "- Each project is associated with a folder on your computer that contains all settings, meta files and experiment outputs. Those folders are created in the folder at `projects_path`. It's generally a good idea to choose one and stick to it throughout projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a project\n",
    "\n",
    "Let's begin!\n",
    "\n",
    "We will create a project called `\"oft\"`, with `\"dlc_track\"` input and `\"csv\"` annotation format. \n",
    "\n",
    "You can run `Project.print_data_types()` and `Project.print_annotation_types()` to find out more about other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project.remove_project(\"oft\", projects_path=PROJECTS_PATH)\n",
    "project = Project(\n",
    "    \"oft\",\n",
    "    data_path=DATA_PATH,\n",
    "    annotation_path=LABELS_PATH,\n",
    "    projects_path=PROJECTS_PATH,\n",
    "    data_type=\"dlc_track\",\n",
    "    annotation_type=\"csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projects have a generalized `help` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project.print_data_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking if our files are organized correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.help(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data structure is sorted out, it's time to configure the parameter settings. \n",
    "\n",
    "Every project keeps fixed settings written in configuration files in the project folder. When you want to change something, you have two options: modifying the configuration files so that this new value becomes the default (with `project.set_main_parameters` or `project.update_parameters`) or passing a `parameters_update` argument to your function so that the settings only change for this run.\n",
    "\n",
    "We will start with setting the defaults.\n",
    "\n",
    "As we saw in the help message, the first step is checking which essential parameters are missing with `project.list_blanks()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.list_blanks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can copy this code, fill in the information and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.update_parameters(\n",
    "    {\n",
    "        \"data\": {\n",
    "            \"data_suffix\": \"DeepCut_resnet50_Blockcourse1May9shuffle1_1030000.csv\", # set; the data files should have the format of {video_id}{data_suffix}, e.g. video1_suffix.pickle, where video1 is the video is and _suffix.pickle is the suffix\n",
    "            \"canvas_shape\": [928, 576], # list; the size of the canvas where the pose was defined\n",
    "            \"annotation_suffix\": \".csv\", # str | set, optional the suffix or the set of suffices such that the annotation files are named {video_id}{annotation_suffix}, e.g, video1_suffix.pickle where video1 is the video id and _suffix.pickle is the suffix\n",
    "            \"fps\": 25 # int; fps (assuming the annotations are given in seconds, otherwise set any value)\n",
    "        },\n",
    "        \"general\": {\n",
    "            \"exclusive\": True, # bool; if true, single-label classification is used; otherwise multi-label\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're all set and can start training models. However, we can take an extra step and check out what other parameters are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this function is only showing us the parameters for the metric functions and the model that are selected as default at the moment. If you want to see what parameters there are for other models and metrics, try setting them first with `project.set_main_parameters()` or with `project.update_parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.list_basic_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can copy this into a code cell to make it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.update_parameters(\n",
    "    {\n",
    "        \"general\": {\n",
    "            \"model_name\": \"c2f_tcn\", # str; model name (run project.help(\"model\") for more info)\n",
    "            \"metric_functions\": {'precision', 'recall', 'f1'}, # set; set of metric names (run project.help(\"metrics\") for more info)\n",
    "            \"ignored_clips\": None, # list; a list of string clip ids (agent names) to be ignored\n",
    "            \"len_segment\": 512, # int; the length of segments (in frames) to cut the videos into\n",
    "            \"overlap\": 0.75, # int; the overlap (in frames) between neighboring segments\n",
    "            \"interactive\": False, # bool; if true, annotations are assigned and features are computed for pairs of clips (animals)\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"data_suffix\": \"DeepCut_resnet50_Blockcourse1May9shuffle1_1030000.csv\", # set; the data files should have the format of {video_id}{data_suffix}, e.g. video1_suffix.pickle, where video1 is the video is and _suffix.pickle is the suffix\n",
    "            \"feature_suffix\": None, # str; the feature files should be stored in the data folder and named {video_id}{feature_suffix}\n",
    "            \"annotation_suffix\": \".csv\", # str | set, optional the suffix or the set of suffices such that the annotation files are named {video_id}{annotation_suffix}, e.g, video1_suffix.pickle where video1 is the video id and _suffix.pickle is the suffix\n",
    "            \"canvas_shape\": [928, 576], # list; the size of the canvas where the pose was defined\n",
    "            \"ignored_bodyparts\": None, # set; the set of string names of bodyparts to ignore\n",
    "            \"likelihood_threshold\": 0, # float; coordinates with lower likelihood values will be ignored\n",
    "            \"behaviors\": None, # set; the behaviors to predict (if null, if will be inferred from the data; !!PLEASE SET IT MANUALLY if different files can have different behavior sets!!)\n",
    "            \"filter_annotated\": True, # bool; discard long unannotated intervals during training\n",
    "            \"filter_background\": True, # bool; only label frames as background if a behavior is annotated somewhere close\n",
    "            \"visibility_min_score\": 0, # float; the minimum visibility score for visibility filtering\n",
    "            \"visibility_min_frac\": 0, # float; the minimum fraction of visible frames for visibility filtering\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"lr\": 0.001, # float; learning rate\n",
    "            \"device\": \"auto\", # str; device\n",
    "            \"num_epochs\": 50, # int; number of epochs\n",
    "            \"to_ram\": False, # bool; transfer the dataset to RAM for training (preferred if the dataset fits in working memory)\n",
    "            \"batch_size\": 64, # int; batch size\n",
    "            \"normalize\": True, # bool; if true, normalization statistics will be computed on the training set and applied to all data\n",
    "            \"temporal_subsampling_size\": 0.85, # float; this fraction of frames in each segment is randomly sampled at training time\n",
    "            \"parallel\": False, # bool; if true, the model will be trained on all gpus visible in the system (use os.environ[“CUDA_VISIBLE_DEVICES”] =“{indices}” to exclude gpus in this mode)\n",
    "            \"val_frac\": 0.2, # float; fraction of dataset to use as validation\n",
    "            \"test_frac\": 0, # float; fraction of dataset to use as test\n",
    "            \"partition_method\": \"random\", # str; the train/test/val partitioning method (for more info run project.help(\"partition_method\"))\n",
    "        },\n",
    "        \"losses\": {\n",
    "            \"ms_tcn\": {\n",
    "                \"focal\": True, # bool; if True, focal loss will be used\n",
    "                \"gamma\": 2, # float; the gamma parameter of focal loss\n",
    "                \"alpha\": 0.001, # float; the weight of consistency loss\n",
    "            },\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"f1\": {\n",
    "                \"average\": \"macro\", # ['macro', 'micro', 'none']; averaging method for classes\n",
    "                \"ignored_classes\": None, # set; a set of class ids to ignore in calculation\n",
    "                \"threshold_value\": 0.5, # float; the probability threshold for positive samples\n",
    "            },\n",
    "            \"recall\": {\n",
    "                \"average\": \"macro\", # ['macro', 'micro', 'none']; averaging method for classes\n",
    "                \"ignored_classes\": None, # set; a set of class ids to ignore in calculation\n",
    "                \"threshold_value\": 0.5, # float; the probability threshold for positive samples\n",
    "            },\n",
    "            \"precision\": {\n",
    "                \"average\": \"macro\", # ['macro', 'micro', 'none']; averaging method for classes\n",
    "                \"ignored_classes\": None, # set; a set of class ids to ignore in calculation\n",
    "                \"threshold_value\": 0.5, # float; the probability threshold for positive samples\n",
    "            },\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"num_f_maps\": 128, # int; number of maps\n",
    "            \"feature_dim\": None, # int; if not null, intermediate features are generated with this dimension and then passed to a 2-layer MLP for classification (useful for SSL)\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"keys\": None, # set; a list of names of the features to extract (a subset of available keys; run project.help(\"features\") for more info)\n",
    "            \"averaging_window\": 1, # int; if >1, features are averaged with a moving window of this size (in frames)\n",
    "            \"distance_pairs\": None, # list; a list of bodypart name tuples (e.g. `[(\"tail\", \"nose\")]`) to compute distances for when `\"intra_distance\"` is in `keys` (by default all distances are computed)\n",
    "            \"angle_pairs\": None, # list; a list of bodypart name tuples (e.g. `[(\"ear1\", \"nose\", \"ear2\")]`) for the angle between `\"ear1\"--\"nose\"` and `\"nose\"--\"ear2\"` lines) to compute angle cosines for when `\"angles\"` is in `keys` (by default no angles are computed)\n",
    "            \"zone_vertices\": None, # dict; a dictionary of bodypart name tuples of any length >= 3 that define zones for `\"zone_bools\"`and `\"zone_distances\"` features; keys should be zone names and values should be tuples that define the polygons (e.g. `{\"main_area\": (\"x_min\", \"x_max\", \"y_max\", \"y_min\"))}`)\n",
    "            \"zone_bools\": None, # list; a list of zone and bodypart name tuples to compute binary identifiers for (1 if an animal is within the polygon or 0 if it's outside) (e.g. `[(\"main_area\", \"nose\")]`); the zones should be defined in the `zone_vertices` parameter; this is only computed if `\"zone_bools\"` is in `keys`\n",
    "            \"zone_distances\": None, # list; a list of zone and bodypart name tuples to compute distances for (distance from the bodypart to the closest of the boundaries) (e.g. `[(\"main_area\", \"nose\")]`); the zones should be defined in the `zone_vertices` parameter; this is only computed if `\"zone_distances\"` is in `keys`\n",
    "            \"area_vertices\": None, # list; a list of bodypart name tuples of any length >= 3 (e.g. `[(\"ear1\", \"nose\", \"ear2\", \"spine1\")]`) that define polygons to compute areas for when `\"areas\"` is in `keys` (by default no areas are computed)\n",
    "        },\n",
    "        \"augmentations\": {\n",
    "            \"augmentations\": {'add_noise'}, # set; a set of augmentations that can be applied to your data without losing information (choose any number from 'rotate', 'real_lens', 'add_noise', 'shift', 'zoom', 'mirror', 'switch', e.g. {'add_noise'. 'mirror'})\n",
    "            \"rotation_limits\": [-1.57, 1.57], # list; list of rotation angle limits in radians ([low, high])\n",
    "            \"mirror_dim\": {0}, # set; set of dimensions that can be mirrored (0 for x, 1 for y, 2 for z)\n",
    "            \"noise_std\": 0.003, # float; standard deviation of noise\n",
    "            \"zoom_limits\": [0.5, 1.5], # list; list of float zoom limits ([low, high])\n",
    "            \"masking_probability\": 0.1, # float; the probability of masking a joint\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the options are for metrics and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.help(\"model\")\n",
    "project.help(\"metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which models and metrics do you want to try out?\n",
    "\n",
    "You can choose any metrics but the code will run much faster if you avoid those marked \"not advisable for training\". We can compute those later with a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME1 = \"c2f_tcn\"\n",
    "MODEL_NAME2 = \"transformer\"\n",
    "METRICS = [\"f1\", \"precision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will configure the metrics in the default parameters, as well as a smaller number of epochs. The model will be updated in each experiment independently, since we want to try out more than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.update_parameters(\n",
    "    {\n",
    "        \"general\": {\"metric_functions\": METRICS},\n",
    "        \"training\": {\"num_epochs\": 15}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done! Now we're ready to run experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many hyperparameters in model training, like the number of layers in a model or loss coefficients. The default settings for those parameters should generate reasonable results on most datasets but in order to get the most out of our data we can run a hyperparameter search.\n",
    "\n",
    "Searches in `DLC2Action` use the `optuna` auto-ML package to sample the hyperpameter space efficiently and save time on trials that go badly from the beginning. \n",
    "\n",
    "There are two methods for running a search: `project.run_hyperparameter_search()` and `project.run_default_hyperparameter_search()`. As you might have guessed from the names, the second method will just load the default set of parameters for a given model while the first allows you to optimize whatever you want. For the vast majority of cases `project.run_default_hyperparameter_search()` should be enough, so that is what we will use in thi tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [MODEL_NAME1, MODEL_NAME2]:\n",
    "    project.run_default_hyperparameter_search(\n",
    "        f\"{model}_search\",\n",
    "        model_name=model,\n",
    "        metric=METRICS[0],\n",
    "        num_epochs=5,\n",
    "        n_trials=10,\n",
    "        prune=True,\n",
    "        best_n=3 # we will compare trials based on the average of three best epochs to account for noise\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those searches will optimize the hyperparameters to maximize the first metric in our list. Note how some trials stop before reaching 10 epochs. That is happening because we have set `prune=True` to interrupt experiments when they are unlikely to beat the best score.\n",
    "\n",
    "Generally, it is better to set both the number of trials and the number of epochs much higher (30-50 and around 150, respectively, is usually a good choice). We are setting them low here to save time but keep in mind that it does mean that the parameters those searches find are probably not actually optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The searches have created a bunch of datasets that we are not going to use again so it's a good idea to clean up the memory at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.remove_saved_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DLC2Action` needs pre-computed features to run an experiment. It keeps track of features that are stored on your system and does not re-compute them if they have already been created in other experiments with the same data parameters. They can take up a lot of space, however, so it's good practice to remove them sometimes. \n",
    "\n",
    "Do not be afraid to run `project.remove_saved_features()`, you never lose any information when you do. The features will just be computed again if you need them. In addition, if you are running low on space, it might be more convenient to pass `remove_saved_features=True` to project methods to remove the features as soon as they are not needed anymore.\n",
    "\n",
    "Another function that helps clean up the memory is `project.remove_extra_checkpoints()`. DLC2Action saves a model checkpoint every 5 epochs by default (you can change this interval at `\"training/model_save_interval\"`). Running this method will remove all the checkpoints except for the last one in each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the searches are done, we can check out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [MODEL_NAME1, MODEL_NAME2]:\n",
    "    _ = project.list_best_parameters(f\"{model}_search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The searches can be used to update the default parameters with `project.update_parameters(load_search=search_name)` but in this case it's more convenient to load them in the relevant episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train models with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [MODEL_NAME1, MODEL_NAME2]:\n",
    "    project.run_episode(\n",
    "        f\"{model}_best\",\n",
    "        load_search=f\"{model}_search\", # loading the search\n",
    "        force=True, # when force=True, if an episode with this name already exists it will be overwritten -> use with caution!\n",
    "        parameters_update={\n",
    "            \"general\": {\"model_name\": model} # note that you do need to set the model explicitly, it is not loaded with the search\n",
    "        },\n",
    "        n_seeds=3 # we will repeat the experiment 3 times (same parameters, different random seed) to get an estimation for how stable our results are\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our best models, we can analyze the results. Note that most statistics are aggregated over the 3 runs (random seeds) automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_episodes = [f\"{model}_best\" for model in [MODEL_NAME1, MODEL_NAME2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.plot_episodes(\n",
    "    best_episodes,\n",
    "    metrics=METRICS, \n",
    "    episode_labels=[\"model_1\", \"model_2\"], \n",
    "    add_hlines=[(0.42, \"a line\")], # we'll add a random horizontal line here but you can use this parameter to mark important thresholds\n",
    "    title=\"Best model training curves\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bold lines here are the means over the three runs of each episode and the transparent lines are the individual runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check out more metrics now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in best_episodes:\n",
    "    project.evaluate(\n",
    "        [episode],\n",
    "        parameters_update={\n",
    "            \"general\": {\"metric_functions\": [\"segmental_f1\", \"mAP\", \"f1\"]},\n",
    "            \"metrics\": {\n",
    "                \"f1\": {\"average\": \"none\"}\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to summarize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.get_results_table(best_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_summary` method will only show the metrics computed during training while `get_results_table` also pulls up the results of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in best_episodes:\n",
    "    _ = project.get_summary([episode])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your models are trained, there's a few ways you can use them.\n",
    "\n",
    "If you're still not quite happy with the results, you can train the model for a few more epochs with `project.continue_episode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.continue_episode(f\"{MODEL_NAME1}_best\", num_epochs=20, n_seeds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also train another episode with different parameters that loads the weights of your model as an initialization. Note that it will use all three runs as starting points for its own three runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.run_episode(\n",
    "    f\"{MODEL_NAME1}_best_lr1e-5\",\n",
    "    load_episode=f\"{MODEL_NAME1}_best\",\n",
    "    load_search=f\"{MODEL_NAME1}_search\",\n",
    "    parameters_update={\n",
    "        \"general\": {\"model_name\": MODEL_NAME1},\n",
    "        \"training\": {\"lr\": 1e-5, \"num_epochs\": 5}\n",
    "    },\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot such episodes together as one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.plot_episodes(\n",
    "    [[f\"{MODEL_NAME1}_best\", f\"{MODEL_NAME1}_best_lr1e-5\"], f\"{MODEL_NAME2}_best\"], \n",
    "    episode_labels=[\"combined_model1\", \"model2\"],\n",
    "    metrics=METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the episodes you have run are stored in the project memory and you can pull up the history with all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = project.list_episodes(\n",
    "    display_parameters=[\"general/model_name\", \"meta/training_time\", \"meta/time\", f\"results/{METRICS[0]}\"], # choose the parameters to display\n",
    "    value_filter=\"training/lr::>1e-5\" # filter the entries by any parameter or result\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same goes for hyperparameter searches and predictions. This function also returns a `pandas` table you can process yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches = project.list_searches(print_results=False)\n",
    "searches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you find that you are happy with the results, you can use the model to generate predictions for new data. \n",
    "\n",
    "Predictions here are probabilities of each behavior being seen in each frame while suggestions are suggested intervals generated from those probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a prediction with one of our models and look at one of the resulting files. Note that you can use multiple models and average over their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.run_prediction(\n",
    "    f\"{MODEL_NAME1}_best_prediction\",\n",
    "    episode_names=[f\"{MODEL_NAME1}_best_lr1e-5\"],\n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "# picking a random file from the prediction folder\n",
    "prediction_folder = project.prediction_path(f\"{MODEL_NAME1}_best_prediction\")\n",
    "prediction_file = os.listdir(prediction_folder)[0]\n",
    "prediction_file = os.path.join(prediction_folder, prediction_file)\n",
    "\n",
    "with open(prediction_file, \"rb\") as f: # open the file\n",
    "    prediction = pickle.load(f)\n",
    "\n",
    "for key, value in prediction.items(): # explore the contents\n",
    "    if key not in [\"max_frames\", \"min_frames\", \"video_tag\", \"behaviors\"]:\n",
    "        print(f'{key}: {value.shape}')\n",
    "    \n",
    "behaviors_order = prediction[\"behaviors\"]\n",
    "\n",
    "start = 50\n",
    "end = 70\n",
    "action = \"attack\"\n",
    "\n",
    "index = behaviors_order.index(action)\n",
    "\n",
    "print(f'The mean probability of {action} between frames {start} and {end} is {prediction[\"1+2\"][index, start: end].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.remove_saved_features()\n",
    "project.remove_extra_checkpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with custom data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to work with your own data, first check `Project.print_data_types()` and `Project.print_annotation_types()` to check if we already have the format that you need.\n",
    "\n",
    "If we don't, you can choose either of them to transform your data to but the easiest is probably `DeepLabCut` track or tracklet for the pose estimation data and our custom `\"dlc\"` format for annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the latter in more detail.\n",
    "\n",
    "It assumes every annotation file to be a pickled nested dictionary where first-level keys are individual names, second-level keys are behavior names and values are lists of intervals. Here is an example.\n",
    "```\n",
    "{\n",
    "    \"ind0\": {\n",
    "        \"running\": [[20, 30], [40, 50]]\n",
    "    },\n",
    "    \"ind1\": {\n",
    "        \"eating\": [[10, 56]]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Here `\"ind0\"` is running in frames 20 to 30 and 40 to 50 while `\"ind1\"` is eating in frames 10 to 56.\n",
    "\n",
    "In case your dataset contains interactive actions, connect the individual names with a plus in alphabetical order (e.g. `\"ind0+ind1\"`). At the moment, we can only support two scenarios: either all actions are interactive or all are individual. In case they are interactive, update the parameters with `project.update_parameters({\"general\": {\"interactive\": True}})` before running experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, let's generate a file in this format from one of the CSV files we've been working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "filename = [x for x in os.listdir(LABELS_PATH) if x.endswith(\".csv\")][0] # load a random .csv file\n",
    "filepath = os.path.join(LABELS_PATH, filename)\n",
    "\n",
    "data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaviors here are not interactive. We don't have a name for the animal so we will call it `\"ind0\"` (that is the default name for single animal DLC track files as well in DLC2Action but you can set something else at `\"data/default_agent_name\"`, just make sure that the names are consistent between annotation and input data). The annotations are encoded as start and end times in seconds. If we download the original videos and check the frame rate we can see that it's 25 fps.\n",
    "\n",
    "Furthermore, the StartEnd behavior doesn't appear to be meaningful for us.\n",
    "\n",
    "Given this information, let's transform this into the DLC format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 25\n",
    "OMITTED_BEHAVIORS = [\"StartEnd\"]\n",
    "\n",
    "for filename in [x for x in os.listdir(LABELS_PATH) if x.endswith(\".csv\")]:\n",
    "    if filename == \"AllLabDataOFT_final.csv\": # this is the raw data with all videos together\n",
    "        continue\n",
    "    filepath = os.path.join(LABELS_PATH, filename)\n",
    "    data = pd.read_csv(filepath)\n",
    "    dlc_dict = {\"ind0\": defaultdict(lambda: [])} # this is just a dictionary that returns an empty list when we try to use a key that doesn't exist\n",
    "    for _, row in data.iterrows():\n",
    "        behavior = row[\"type\"]\n",
    "        if behavior in OMITTED_BEHAVIORS: # ignore extra behaviors\n",
    "            continue\n",
    "        if any([np.isnan(x) for x in [row[\"from\"], row[\"to\"]]]): # skip nans\n",
    "            continue\n",
    "        start = int(row[\"from\"] * FPS)\n",
    "        end = int(row[\"to\"] * FPS)\n",
    "        dlc_dict[\"ind0\"][behavior].append([start, end])\n",
    "    new_name = filepath.split(\".\")[0] + \"_dlc.pickle\" # the filenames need to start with the video name\n",
    "    dlc_dict[\"ind0\"] = dict(dlc_dict[\"ind0\"]) # we need to convert the defaultdict back to a standard dictionary before saving\n",
    "    with open(new_name, \"wb\") as f: # save the annotation file\n",
    "        pickle.dump(dlc_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we did everything right. We will create a new project with the new data type and run a small experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project.remove_project(\"oft_dlc\", projects_path=PROJECTS_PATH)\n",
    "\n",
    "from dlc2action.project import Project\n",
    "import os\n",
    "\n",
    "# CURRENT_PATH = os.getcwd()\n",
    "# DATA_PATH = os.path.join(CURRENT_PATH, \"OFT\", \"OFT\", \"Output_DLC\")\n",
    "# LABELS_PATH = os.path.join(CURRENT_PATH, \"OFT\", \"OFT\", \"Labels\")\n",
    "# PROJECTS_PATH = os.path.join(CURRENT_PATH, \"DLC2Action\")\n",
    "\n",
    "DATA_PATH = \"/home/liza/data/OFT/Output_DLC\"\n",
    "LABELS_PATH = \"/home/liza/data/OFT/Labels\"\n",
    "PROJECTS_PATH = \"/home/liza/DLC2Action_tmp\"\n",
    "\n",
    "project = Project(\n",
    "    \"oft_dlc\",\n",
    "    data_path=DATA_PATH,\n",
    "    annotation_path=LABELS_PATH,\n",
    "    data_type=\"dlc_track\",\n",
    "    annotation_type=\"dlc\",\n",
    "    projects_path=PROJECTS_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.update_parameters(\n",
    "    {\n",
    "        \"data\": {\n",
    "            \"data_suffix\": \"DeepCut_resnet50_Blockcourse1May9shuffle1_1030000.csv\", # set; the data files should have the format of {video_id}{data_suffix}, e.g. video1_suffix.pickle, where video1 is the video is and _suffix.pickle is the suffix\n",
    "            \"canvas_shape\": [928, 576], # list; the size of the canvas where the pose was defined\n",
    "            \"annotation_suffix\": \"_dlc.pickle\", # str | set, optional the suffix or the set of suffices such that the annotation files are named {video_id}{annotation_suffix}, e.g, video1_suffix.pickle where video1 is the video id and _suffix.pickle is the suffix\n",
    "            \"fps\": 25 # int; fps (assuming the annotations are given in seconds, otherwise set any value)\n",
    "        },\n",
    "        \"general\": {\n",
    "            \"exclusive\": True, # bool; if true, single-label classification is used; otherwise multi-label\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.run_episode(\"test\", force=True, parameters_update={\"training\": {\"num_epochs\": 1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `DLC2Action` to generate new annotation files. Install our interface following those instructions and try running it with any video you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Installation\n",
    "\n",
    "for now:\n",
    "```\n",
    "git clone https://github.com/amathislab/dlc2action_annotation\n",
    "cd dlc2action_annotation\n",
    "conda env create -f AnnotationGUI.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, go to the `dlc2action_annotation` folder and run `python annotator.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('DLC2Action')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f5f7675fb80040f0ca1448b5f0ffd03eab1e03daeb908de054f4ee2164beb96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
