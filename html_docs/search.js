window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "dlc2action", "modulename": "dlc2action", "type": "module", "doc": "<p><code>dlc2action</code> is an action segmentation package that makes running and tracking experiments easy.</p>\n\n<h1 id=\"usage\">Usage</h1>\n\n<p><code>dlc2action</code> is designed to be modular.\nYou can use the high-level project interface for convenient experiment management or just import a metric\nor an SSL module if you want more freedom. Here are some tutorials for using the package.</p>\n\n<h2 id=\"project\">Project</h2>\n\n<p>Project is the class that can create and maintain configuration files and keep track of your experiments.</p>\n\n<h3 id=\"creating\">Creating</h3>\n\n<p>To start a new project, you can create a new <code>dlc2action.project.project.Project</code> instance in python. \nCheck <code>dlc2action.project.project.Project.print_data_types</code> and <code>dlc2action.project.project.Project.print_annotation_types</code> to see the implemented data \nand annotation types.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">dlc2action.project</span> <span class=\"kn\">import</span> <span class=\"n\">Project</span>\n\n<span class=\"n\">project</span> <span class=\"o\">=</span> <span class=\"n\">Project</span><span class=\"p\">(</span>\n    <span class=\"s1\">&#39;project_name&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">data_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;data_type&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">annotation_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;annotation_type&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">data_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;path/to/data/folder&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">annotation_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;path/to/annotation/folder&#39;</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<p>A new folder will be created at <code>projects_path/project_name</code> with all the necessary files. The default projects path is\na <code>DLC2Action</code> folder that will be created in your home directory.\nThe project structure looks like this.</p>\n\n<pre><code>.\nproject_name\n\u251c\u2500\u2500 config                                             # Settings files\n\u251c\u2500\u2500 meta                                               # Project meta files (experiment records)\n\u251c\u2500\u2500 saved_datasets                                     # Pre-computed dataset files\n\u2514\u2500\u2500 results\n    \u251c\u2500\u2500 logs                                           # Training logs (human readable)\n    \u2502   \u2514\u2500\u2500 episode.txt\n    \u251c\u2500\u2500 models                                         # Model checkpoints\n    \u2502   \u2514\u2500\u2500 episode\n    \u2502       \u251c\u2500\u2500 epoch25.pt\n    \u2502       \u2514\u2500\u2500 epoch50.pt\n    \u251c\u2500\u2500 searches                                       # Hyperparameter search results (graphs)\n    \u2502   \u2514\u2500\u2500 search\n    \u2502       \u251c\u2500\u2500 search_param_importances.html_docs\n    \u2502       \u2514\u2500\u2500 search_contour.html_docs\n    \u251c\u2500\u2500 splits                                         # Split files\n    \u2502       \u251c\u2500\u2500 time_25.0%validation_10.0%test.txt\n    \u2502       \u2514\u2500\u2500 random_20.0%validation_10.0%test.txt\n    \u251c\u2500\u2500 suggestions                                    # Suggestion and active learning files\n    \u2502       \u2514\u2500\u2500 active_learning\n    \u2502           \u251c\u2500\u2500 video1_suggestion.pickle\n    \u2502           \u251c\u2500\u2500 video2_suggestion.pickle\n    \u2502           \u2514\u2500\u2500 al_points.pickle\n    \u2514\u2500\u2500 predictions                                    # Prediction files (pickled dictionaries)\n            \u251c\u2500\u2500 episode_epoch25.pickle\n            \u2514\u2500\u2500 episode_epoch50_newdata.pickle\n</code></pre>\n\n<p>You can find a more detailed explanation of the structure at <code>dlc2action.project</code>.</p>\n\n<p>After the project is created you can modify the parameters with the <code>dlc2action.project.project.Project.update_parameters</code> function.\nMake sure to start by filling out the required parameters. You can get a list of those and a code hint with \n<code>dlc2action.project.project.Project.list_blanks</code>.</p>\n\n<p>Run <code>project.help()</code> to find out more about other parameters you might want to modify.</p>\n\n<h3 id=\"training\">Training</h3>\n\n<p>When you want to start your experiments, just create a <code>dlc2action.project.project.Project</code> instance again\n(or use the one you created\nto initialize the project). This time you don't have to set any parameters except the project name (and, if\nyou used it when creating the project, <code>projects_path</code>).</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">dlc2action.project</span> <span class=\"kn\">import</span> <span class=\"n\">Project</span>\n<span class=\"n\">project</span> <span class=\"o\">=</span> <span class=\"n\">Project</span><span class=\"p\">(</span><span class=\"s1\">&#39;project_name&#39;</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<p>The first thing you will want to do is train some models. There are three ways to run a <em>training episode</em>\nin <code>dlc2action</code>.</p>\n\n<ol>\n<li><p><strong>Run a single episode</strong></p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">run_episode</span><span class=\"p\">(</span><span class=\"s1\">&#39;episode_1&#39;</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<p>We have now run a training episode with the default project parameters (read from the configuration files)\nand saved it in the meta files under the name <code>episode_1</code>.</p></li>\n<li><p><strong>Run multiple episodes in a row</strong></p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">run_episodes</span><span class=\"p\">([</span><span class=\"s1\">&#39;episode_2&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;episode_3&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;episode_4&#39;</span><span class=\"p\">])</span>\n</code></pre></div>\n\n<p>That way the <code>dlc2action.task.universal_task.Task</code> instance will not be\nre-created every time, which might save you some time.</p></li>\n<li><p><strong>Continue a previously run episode</strong></p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">continue_episode</span><span class=\"p\">(</span><span class=\"s1\">&#39;episode_2&#39;</span><span class=\"p\">,</span> <span class=\"n\">num_epochs</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<p>In case you decide you want to continue an older episode, you can load all parameters and state dictionaries\nand set a new number of epochs. That way training will go on from where it has stopped and in the end the\nepisode will be re-saved under the same name. Note that <code>num_epochs</code> denotes the <strong>new total number of epochs</strong>,\nso if <code>episode_2</code> has already been trained for 300 epochs, for example, it will now run for 200 epochs more,\nnot 500.</p></li>\n</ol>\n\n<p>Of course, changing the default parameters every time you want to run a new configuration is not very convenient.\nAnd, luckily, you don't have to do that. Instead you can add a <code>parameters_update</code> parameter to\n<code>dlc2action.project.project.Project.run_episode</code> (or <code>parameters_updates</code> to\n<code>dlc2action.project.project.Project.run_episodes</code>; all other parameters generalize to multiple episodes\nin a similar way). The third\nfunction does not take many additional parameters since it aims to continue an experiment from exactly where\nit ended.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">run_episode</span><span class=\"p\">(</span>\n    <span class=\"s1\">&#39;episode_5&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">parameters_update</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s1\">&#39;general&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;ssl&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">&#39;contrastive&#39;</span><span class=\"p\">]},</span>\n        <span class=\"s1\">&#39;ssl&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;contrastive&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;ssl_weight&#39;</span><span class=\"p\">:</span> <span class=\"mf\">0.01</span><span class=\"p\">}},</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<p>In order to find the parameters you can modify, just open the <em>config</em> folder of your project and browse through\nthe files or call <code>dlc2action_fill</code> in your terminal (see [creating] section). The first-level keys\nare the filenames without the extension (<code>'augmentations'</code>, <code>'data'</code>, <code>'general'</code>, <code>'losses'</code>,\n<code>'metrics'</code>, <code>'ssl'</code>, <code>'training'</code>). Note that there are no\n<em>model.yaml</em> or <em>features.yaml</em> files there for the <code>'model'</code> and <code>'features'</code> keys in the parameter\ndictionaries. Those parameters are <strong>read from the files in the <em>model</em> and <em>features</em> folders</strong> that correspond\nto the options you set in the <code>'general'</code> dictionary. For example, if at <em>general.yaml</em> <code>model_name</code> is set to\n<code>'ms_tcn3'</code>, the <code>'model'</code> dictionary will be read from <em>model/ms_tcn3.yaml</em>.</p>\n\n<p>If you want to create a new episode with modified parameters that loads a previously trained model, you can do that\nby adding a <code>load_episode</code> parameter. By default we will load the last saved epoch, but you can also specify\nwhich epoch you want with <code>load_epoch</code>. In that case the closest checkpoint will be chosen.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">run_episode</span><span class=\"p\">(</span>\n    <span class=\"s1\">&#39;episode_6&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">parameters_update</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;training&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;batch_size&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">}},</span>\n    <span class=\"n\">load_episode</span><span class=\"o\">=</span><span class=\"s1\">&#39;episode_2&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">load_epoch</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<h3 id=\"optimizing\">Optimizing</h3>\n\n<p><code>dlc2action</code> also has tools for easy hyperparameter optimization. We use the <code>optuna</code> auto-ML package to perform\nthe searches and then save the best parameters and the search graphs (contour and parameter importance).\nThe best parameters\ncan then be loaded when you are run an episode or saved as the defaults in the configuration files.</p>\n\n<p>To start a search, you need to run either the <code>project.project.Project.run_hyperparameter_search</code> or the \n<code>project.project.Project.run_default_hyperparameter_search</code> command. The first one allows you to optimize any hyperparameter you want\nwhile the second provides you with a good default search space for a given model name. </p>\n\n<p>To run a default search, you just need to know the name of the model you want to experiment with.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">run_default_hyperparameter_search</span><span class=\"p\">(</span>\n    <span class=\"s2\">&quot;c2f_tcn_search&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;c2f_tcn&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"s2\">&quot;f1&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">prune</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<p>Defining your own search space is a bit more complicated. Let's\nsay we want to optimize for four parameters: overlap length, SSL task type, learning rate and\nnumber of feature maps in the model. Here is the process\nto figure out what we want to run.</p>\n\n<ol>\n<li><p><strong>Find the parameter names</strong></p>\n\n<p>We check our parameter dictionary with <code>project.list_basic_parameters()</code> and find them in <code>\"data\"</code>, <code>\"general\"</code>, \n<code>\"training\"</code> and <code>\"model\"</code> categories, respectively.\nThat means that our parameter names are <code>'data/overlap'</code>, <code>'general/ssl'</code>, <code>'training/lr'</code> and\n<code>'model/num_f_maps'</code>.</p></li>\n<li><p><strong>Define the search space</strong></p>\n\n<p>There are five types of search spaces in <code>dlc2action</code>:</p>\n\n<ul>\n<li><code>int</code>: integer values (uniform sampling),</li>\n<li><code>int_log</code>: integer values (logarithmic scale sampling),</li>\n<li><code>float</code>: float values (uniform sampling),</li>\n<li><code>float_log</code>: float values (logarithmic sampling),</li>\n<li><code>categorical</code>: choice between several values.</li>\n</ul>\n\n<p>The first four are defined with their minimum and maximum value while <code>categorical</code> requires a list of\npossible values. So the search spaces are described with tuples that look either like\n<code>(search_space_type, min, max)</code> or like <code>(\"categorical\", list_of_values)</code>.</p>\n\n<p>We suspect that the optimal overlap is somewhere between 10 and 80 frames, SSL tasks should be either\nonly contrastive or contrastive and masked_features together, sensible learning rate is between\n10<sup>-2</sup> and 10<sup>-4</sup> and the number of feature maps should be between 8 and 64.\nThat makes our search spaces <code>(\"int\", 10, 80)</code> for the overlap,\n<code>(\"categorical\", [[\"contrastive\"], [\"contrastive\", \"masked_features\"]])</code> for the SSL tasks,\n<code>(\"float_log\", 1e-4, 1e-2)</code> for the learning rate and (\"int_log\", 8, 64) for the feature maps.</p></li>\n<li><p><strong>Choose the search parameters</strong></p>\n\n<p>You need to decide which metric you are optimizing for and for how many trials. Note that it has to be one\nof the metrics you are computing: check <code>metric_functions</code> at <em>general.yaml</em> and add your\nmetric if it's not there. The <code>direction</code> parameter determines whether this metric is minimized or maximized.\nThe metric can also be averaged over a few of the most successful epochs (<code>average</code> parameter). If you want to\nuse <code>optuna</code>'s pruning feature, set <code>prune</code> to <code>True</code>.</p>\n\n<p>You can also use parameter updates and load older experiments, as in the [training] section of this tutorial.</p>\n\n<p>Here we will maximize the recall averaged over 5 top epochs and run 50 trials with pruning.</p></li>\n</ol>\n\n<p>Now we are ready to run!</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">run_hyperparameter_search</span><span class=\"p\">(</span>\n    <span class=\"n\">search_space</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s2\">&quot;data/overlap&quot;</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"s2\">&quot;int&quot;</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">80</span><span class=\"p\">),</span>\n        <span class=\"s2\">&quot;general/ssl&quot;</span><span class=\"p\">:</span> <span class=\"p\">(</span>\n            <span class=\"s2\">&quot;categorical&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">[[</span><span class=\"s2\">&quot;contrastive&quot;</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">&quot;contrastive&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;masked_features&quot;</span><span class=\"p\">]]</span>\n        <span class=\"p\">),</span>\n        <span class=\"s2\">&quot;training/lr&quot;</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"s2\">&quot;float_log&quot;</span><span class=\"p\">:</span> <span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"mf\">1e-2</span><span class=\"p\">),</span>\n        <span class=\"s2\">&quot;model/num_f_maps&quot;</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"s2\">&quot;int_log&quot;</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span>\n    <span class=\"p\">},</span>\n    <span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"s2\">&quot;recall&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">n_trials</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span>\n    <span class=\"n\">average</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">search_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;search_1&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">direction</span><span class=\"o\">=</span><span class=\"s2\">&quot;maximize&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">prune</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<p>After a search is finished, the best parameters are saved in the meta files and search graphs are saved at\n<em>project_name/results/searches/search_1</em>. You can see the best parameters by running\n<code>project.project.Project.list_best_parameters</code>:</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">list_best_parameters</span><span class=\"p\">(</span><span class=\"s1\">&#39;search_1&#39;</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<p>Those results can also be loaded in a training episode or saved in the configuration files directly.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">update_parameters</span><span class=\"p\">(</span>\n    <span class=\"n\">load_search</span><span class=\"o\">=</span><span class=\"s1\">&#39;search_1&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">load_parameters</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;training/lr&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;model/num_f_maps&#39;</span><span class=\"p\">],</span>\n    <span class=\"n\">round_to_binary</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;model/num_f_maps&#39;</span><span class=\"p\">],</span>\n<span class=\"p\">)</span>\n<span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">run_episode</span><span class=\"p\">(</span>\n    <span class=\"s1\">&#39;episode_best_params&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">load_search</span><span class=\"o\">=</span><span class=\"s1\">&#39;search_1&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">load_parameters</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;data/overlap&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;general/ssl&#39;</span><span class=\"p\">],</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<p>In this example we saved the learning rate and the number of feature maps in the configuration files and\nloaded the other parameters to run\nthe <code>episode_best_params</code> training episode. Note how we used the <code>round_to_binary</code> parameter.\nIt will round the number of feature maps to the closest power of two (7 to 8, 35 to 32 and so on). This is useful\nfor parameters like the number of features or the batch size.</p>\n\n<h3 id=\"exploring-results\">Exploring results</h3>\n\n<p>After you run a bunch of experiments you will likely want to get an overview.</p>\n\n<h4 id=\"visualization\">Visualization</h4>\n\n<p>You can get a feeling for the predictions made by a model by running <code>project.project.Project.plot_predictions</code>.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">plot_predictions</span><span class=\"p\">(</span><span class=\"s1\">&#39;episode_1&#39;</span><span class=\"p\">,</span> <span class=\"n\">load_epoch</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<p>This command will generate a prediction for a random sample and visualize it compared to the ground truth.\nThere is a lot of parameters you can customize, check them out in the documentation).</p>\n\n<p>You can also analyze the results with <code>project.project.Project.plot_confusion_matrix</code>.</p>\n\n<p>Another available visualization type is training curve comparison with <code>project.project.Project.plot_episodes</code>.\nYou can compare different metrics and modes across several episode or whithin one. For example, this command\nwill plot the validation accuracy curves for the two episodes.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">plot_episodes</span><span class=\"p\">([</span><span class=\"s1\">&#39;episode_1&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;episode_2&#39;</span><span class=\"p\">],</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;accuracy&#39;</span><span class=\"p\">])</span>\n</code></pre></div>\n\n<p>And this will plot training and validation recall curves for <code>episode_3</code>.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">plot_episodes</span><span class=\"p\">([</span><span class=\"s1\">&#39;episode_3&#39;</span><span class=\"p\">],</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;recall&#39;</span><span class=\"p\">],</span> <span class=\"n\">modes</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;train&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;val&#39;</span><span class=\"p\">])</span>\n</code></pre></div>\n\n<p>You can also plot several episodes as one curve. That can be useful, for example, with episodes 2 and 6\nin our tutorial, since <code>episode_6</code> loaded the model from <code>episode_2</code>.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">plot_episodes</span><span class=\"p\">([[</span><span class=\"s1\">&#39;episode_2&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;episode_6&#39;</span><span class=\"p\">],</span> <span class=\"s1\">&#39;episode_4&#39;</span><span class=\"p\">],</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;precision&#39;</span><span class=\"p\">])</span>\n</code></pre></div>\n\n<h4 id=\"tables\">Tables</h4>\n\n<p>Alternatively, you can start analyzing your experiments by putting them in a table. You can get a summary of\nyour training episodes with <code>project.project.Project.list_episodes</code>. It provides you with three ways to filter\nthe data.</p>\n\n<ol>\n<li><p><strong>Episode names</strong></p>\n\n<p>You can directly say which episodes you want to look at and disregard all others. Let's say we want to see\nepisodes 1, 2, 3 and 4.</p></li>\n<li><p><strong>Parameters</strong></p>\n\n<p>In a similar fashion, you can choose to only display certain parameters. See all available parameters by\nrunning <code>project.list_episodes().columns</code>. Here we are only interested in the time of adding the record,\nthe recall results and the learning rate.</p></li>\n<li><p><strong>Value filter</strong></p>\n\n<p>The last option is to filter by parameter values. Filters are defined by strings that look like this:\n<code>'{parameter_name}::{sign}{value}'</code>. You can use as many filters as ypu want and separate them with a comma.\nThe parameter names are the same as in point 2, the sign can be either\n<code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code> or <code>=</code> and you choose the value. Let's say we want to only get the episodes that took\nat least 30 minutes to train and got to a recall that is higher than 0.4. That translates to\n<code>'results/recall::&gt;0.4,meta/training_time::&gt;=00:30:00'</code>.</p></li>\n</ol>\n\n<p>Putting it all together, we get this command.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">list_episodes</span><span class=\"p\">(</span>\n    <span class=\"n\">episodes</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;episode_1&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;episode_2&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;episode_3&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;episode_4&#39;</span><span class=\"p\">],</span>\n    <span class=\"n\">display_parameters</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;meta/time&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;results/recall&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;training/lr&#39;</span><span class=\"p\">],</span>\n    <span class=\"n\">episode_filter</span><span class=\"o\">=</span><span class=\"s1\">&#39;results/recall::&gt;0.4,meta/training_time::&gt;=00:30:00&#39;</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<p>There are similar functions for summarizing other history files: <code>project.project.Project.list_searches</code>,\n<code>project.project.Project.list_predictions</code>, <code>project.project.Project.list_suggestions</code> and they all follow the\nsame pattern.</p>\n\n<p>You can also get a summary of your experiment results with <code>project.project.Project.get_results_table</code> and\n<code>project.project.Project.get_summary</code>. Those methods will aggregate the metrics over all runs of you experiment and allow \nfor easy comparison between models or other parameters.</p>\n\n<h3 id=\"making-predictions\">Making predictions</h3>\n\n<p>When you feel good about one of the models, you can move on to making predictions. There are two ways to do that:\ngenerate pickled prediction dictionaries with <code>project.project.Project.run_prediction</code> or active learning\nand suggestion files for the [GUI] with <code>project.project.Project.run_suggestion</code>.</p>\n\n<p>By default the predictions will be made for the entire dataset at the data path of the project. However, you\ncan also choose to only make them for the training, validation or test subset (set with <code>mode</code> parameter) or\neven for entirely new data (set with <code>data_path</code> parameter). Note that if you set the <code>data_path</code>\nvalue, <strong>the split\nparameters will be disregarded</strong> and <code>mode</code> will be forced to <code>'all'</code>!</p>\n\n<p>Here is an example of running these functions.</p>\n\n<div class=\"pdoc-code codehilite\"><pre><span></span><code><span class=\"n\">project</span><span class=\"o\">.</span><span class=\"n\">run_prediction</span><span class=\"p\">(</span><span class=\"s1\">&#39;prediction_1&#39;</span><span class=\"p\">,</span> <span class=\"n\">episode_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;episode_3&#39;</span><span class=\"p\">,</span> <span class=\"n\">load_epoch</span><span class=\"o\">=</span><span class=\"mi\">150</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;test&#39;</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<p>This command will generate a prediction dictionary with the model from epoch 150 of <code>episode_3</code> for the test\nsubset of the project data (split according to the configuration files) and save it at\n<em>project_name/results/predictions/prediction_1.pickle</em>. </p>\n\n<p>The history of prediction runs is recorded and at the [exploring] section\nwe have described how to access it.</p>\n"}, {"fullname": "dlc2action.data", "modulename": "dlc2action.data", "type": "module", "doc": "<h2 id=\"dataset-and-related-objects\">Dataset and related objects</h2>\n\n<h3 id=\"dataset\">Dataset</h3>\n\n<p>The dataset class in <code>dlc2action</code> is <code>dataset.BehaviorDataset</code>. It defines all high-level interaction between\n<code>Task</code>,\ninput data and annotations (like loading, filtering or adding pseudo-labels). It is a single class that works\nfor all data types and is not meant to be inherited from. All customisation happens in <em>store</em> classes instead.\nEvery dataset has an <em>input store</em> and an <em>annotation store</em> that perform the actual data operations.</p>\n\n<h3 id=\"store\">Store</h3>\n\n<p><em>Stores</em> are defined by an abstract data handling parent class.\nIt is inherited from by <code>base_store.InputStore</code> and <code>base_store.AnnotationStore</code> and implementations of\nthese classes (for input and\nannotation data, respectively, see <code>input_store</code> and <code>annotation_store</code>) are used by datasets. In other words,\nadding a new dataset to <code>dlc2action</code> means\nimplementing a list of abstract functions for the input and annotation data.</p>\n\n<p>That list of functions was created\nwith several <strong>assumptions</strong> about the structure of the data in mind. Specifically, we are assuming that you are\nworking with video-related information that is defined at the frame level. Videos can be separated into <em>clips</em>.\n<em>Clips</em> here are elementary parts that are associated with behaviour labels. In the simplest scenario, clips are\ntracks of pose estimation key points associated with single individuals, for example. Every frame of every clip\nis associated with a feature vector (like key point coordinates or image pixel values) and at least some of the\nframes have behaviour labels. Every video has a unique video id and every clip inside a video has a unique clip\nid (clip ids in different videos don't have to be different). After this data is loaded and preprocessed,\na store can take an integer index and return an input data sample and a tuple of <em>original coordinates</em> that\ncan be used to map that sample to a specific place in the original data (meaning video id, clip id and frame\nindices). The indexing is consistent across time and stores (the features at index 42 an an input frame\nshould correspond to labels at index 42 at an annotation store for the same dataset). That is checked at runtime\nby comparing the original coordinates arrays of the two stores.</p>\n\n<p><img src=\"https://i.ibb.co/Y8zc43H/data.png\" alt=\"image\" /></p>\n\n<p>In addition, every store is defined by a tuple of <em>key objects</em> (e.g. the input data array, the original\ncoordinates array and a dictionary with lengths of the original videos). When these key objects are saved and a\nnew store is created from them, it behaves identically to the original. Finally, when initialising a dataset,\ninput stores are always created first and annotations stores second, if at all. If there is any information that\nneeds to be passed from an input store to an annotation store, it is packed in a dictionary, termed <em>annotation\nobjects</em>. <code>base_store.AnnotationStore</code> child classes have a <code>required_annotation_objects</code> attribute that contains\nthe keys that\nneed to be passed in any case, but you can add optional fields too.</p>\n\n<p>Data is usually stored either as a <code>torch.Tensor</code> (in <code>base_store.AnnotationStore</code> instances), a\n<code>dlc2action.utils.TensorDict</code> (in <code>base_store.InputStore</code> instances where all data fits in RAM) or a <code>numpy.ndarray</code>\nof filenames (in <code>base_store.InputStore</code> instances with large amounts of data).</p>\n"}, {"fullname": "dlc2action.data.annotation_store", "modulename": "dlc2action.data.annotation_store", "type": "module", "doc": "<p>Specific implementations of <code>dlc2action.data.base_store.AnnotationStore</code> are defined here</p>\n"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore", "type": "class", "doc": "<p>A class that generates annotation from video information and stores it</p>\n\n<p>Processes input video information and generates ordered arrays of annotation samples and corresponding unique\noriginal coordinates, as well as some meta objects.\nIt is assumed that the input videos are separated into clips (e.g. corresponding to different individuals).\nEach video and each clip inside the video has a unique id (video_id and clip_id, correspondingly).\nThe original coordinates object contains information about the video_id, clip_id and start time of the\nsamples in the original input data.\nAn AnnotationStore has to be fully defined with a tuple of key objects.\nThe annotation array can be accessed with integer indices.\nThe samples can be stored as a torch.Tensor in RAM or as an array of file paths to be loaded on runtime.\nWhen no arguments are passed a blank class instance should be created that can later be filled with\ninformation from key objects</p>\n", "bases": "dlc2action.data.base_store.AnnotationStore"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.__init__", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\nkey_objects : tuple, optional\n    a tuple of key objects</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    key_objects: Tuple = None,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.remove", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.remove", "type": "function", "doc": "<p>Remove the samples corresponding to indices</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>indices : int\n    a list of integer indices to remove</p>\n", "signature": "(self, indices: List) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.key_objects", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.key_objects", "type": "function", "doc": "<p>Return a tuple of the key objects necessary to re-create the Store</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>key_objects : tuple\n    a tuple of key objects</p>\n", "signature": "(self) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.load_from_key_objects", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.load_from_key_objects", "type": "function", "doc": "<p>Load the information from a tuple of key objects</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>key_objects : tuple\n    a tuple of key objects</p>\n", "signature": "(self, key_objects: Tuple) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.to_ram", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.to_ram", "type": "function", "doc": "<p>Transfer the data samples to RAM if they were previously stored as file paths</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.get_original_coordinates", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.get_original_coordinates", "type": "function", "doc": "<p>Return the original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>np.ndarray\n    an array that contains the coordinates of the data samples in original input data (video id, clip id,\n    start frame)</p>\n", "signature": "(self) -> numpy.ndarray", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.create_subsample", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.create_subsample", "type": "function", "doc": "<p>Create a new store that contains a subsample of the data</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>indices : list\n    the indices to be included in the subsample\nssl_indices : list, optional\n    the indices to be included in the subsample without the annotation data</p>\n", "signature": "(self, indices: List, ssl_indices: List = None)", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.get_file_ids", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(cls, *args, **kwargs) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.get_len", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.get_len", "type": "function", "doc": "<p>Get the length of the subsample of labeled/unlabeled data</p>\n\n<p>If return_unlabeled is True, the index is in the subsample of unlabeled data, if False in labeled\nand if return_unlabeled is None the index is already correct</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>return_unlabeled : bool\n    the identifier for the subsample</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>length : int\n    the length of the subsample</p>\n", "signature": "(self, return_unlabeled: bool) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.get_idx", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.get_idx", "type": "function", "doc": "<p>Convert from an index in the subsample of labeled/unlabeled data to an index in the full array</p>\n\n<p>If return_unlabeled is True, the index is in the subsample of unlabeled data, if False in labeled\nand if return_unlabeled is None the index is already correct</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>index : int\n    the index in the subsample\nreturn_unlabeled : bool\n    the identifier for the subsample</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>corrected_index : int\n    the index in the full dataset</p>\n", "signature": "(self, index: int, return_unlabeled: bool) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.count_classes", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.count_classes", "type": "function", "doc": "<p>Get a dictionary with class-wise frame counts</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>frac : bool, default False\n    if True, a fraction of the total frame count is returned</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>count_dictionary : dict\n    a dictionary with class indices as keys and frame counts as values</p>\n", "signature": "(\n    self,\n    frac: bool = False,\n    zeros: bool = False,\n    bouts: bool = False\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.behaviors_dict", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.behaviors_dict", "type": "function", "doc": "<p>Get a dictionary of class names</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>behavior_dictionary: dict\n    a dictionary with class indices as keys and class names as values</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.annotation_class", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.annotation_class", "type": "function", "doc": "<p>Get the type of annotation ('exclusive_classification', 'nonexclusive_classification', more coming soon)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>annotation_class : str\n    the type of annotation</p>\n", "signature": "(self) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.size", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.size", "type": "function", "doc": "<p>Get the total number of frames in the data</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>size : int\n    the total number of frames</p>\n", "signature": "(self) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.filtered_indices", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.filtered_indices", "type": "function", "doc": "<p>Return the indices of the samples that should be removed</p>\n\n<p>Choosing the indices can be based on any kind of filering defined in the __init__ function by the data\nparameters</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>indices_to_remove : list\n    a list of integer indices that should be removed</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.EmptyAnnotationStore.set_pseudo_labels", "modulename": "dlc2action.data.annotation_store", "qualname": "EmptyAnnotationStore.set_pseudo_labels", "type": "function", "doc": "<p>Set pseudo labels to the unlabeled data</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>labels : torch.Tensor\n    a tensor of pseudo-labels for the unlabeled data</p>\n", "signature": "(self, labels: torch.Tensor) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore", "type": "class", "doc": "<p>A general realization of an annotation store for action segmentation tasks</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>annotation_path\n\u251c\u2500\u2500 video1_annotation.pickle\n\u2514\u2500\u2500 video2_labels.pickle\n</code></pre>\n\n<p>Here <code>annotation_suffix</code> is <code>{'_annotation.pickle', '_labels.pickle'}</code>.</p>\n", "bases": "dlc2action.data.base_store.AnnotationStore"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.__init__", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\nmin_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip start frames (not passed if creating from key objects)\nmax_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip end frames (not passed if creating from key objects)\nvisibility : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    visibility score arrays (not passed if creating from key objects or if irrelevant for the dataset)\nexclusive : bool, default True\n    if True, the annotation is single-label; if False, multi-label\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nbehaviors : set, optional\n    the list of behaviors to put in the annotation (not passed if creating a blank instance or if behaviors are\n    loaded from a file)\nignored_classes : set, optional\n    the list of behaviors from the behaviors list or file to not annotate\nignored_clips : set, optional\n    clip ids to ignore\nannotation_suffix : str | set, optional\n    the suffix or the set of suffices such that the annotation files are named {video_id}{annotation_suffix}\n    (not passed if creating from key objects or if irrelevant for the dataset)\nannotation_path : str | set, optional\n    the path or the set of paths to the folder where the annotation files are stored (not passed if creating\n    from key objects)\nbehavior_file : str, optional\n    the path to an .xlsx behavior file (not passed if creating from key objects or if irrelevant for the dataset)\ncorrection : dict, optional\n    a dictionary of corrections for the labels (e.g. {'sleping': 'sleeping', 'calm locomotion': 'locomotion'},\n    can be used to correct for variations in naming or to merge several labels in one\nframe_limit : int, default 0\n    the smallest possible length of a clip (shorter clips are discarded)\nfilter_annotated : bool, default False\n    if True, the samples that do not have any labels will be filtered\nfilter_background : bool, default False\n    if True, only the unlabeled frames that are close to annotated frames will be labeled as background\nerror_class : str, optional\n    the name of the error class (the annotations that intersect with this label will be discarded)\nmin_frames_action : int, default 0\n    the minimum length of an action (shorter actions are not annotated)\nkey_objects : tuple, optional\n    the key objects to load the AnnotationStore from\nvisibility_min_score : float, default 5\n    the minimum visibility score for visibility filtering\nvisibility_min_frac : float, default 0.7\n    the minimum fraction of visible frames for visibility filtering\nmask : dict, optional\n    a masked value dictionary (for active learning simulation experiments)\nuse_hard_negatives : bool, default False\n    mark hard negatives as 2 instead of 0 or 1, for loss functions that have options for hard negative processing\ninteractive : bool, default False\n    if <code>True</code>, annotation is assigned to pairs of individuals</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    min_frames: Dict = None,\n    max_frames: Dict = None,\n    visibility: Dict = None,\n    exclusive: bool = True,\n    len_segment: int = 128,\n    overlap: int = 0,\n    behaviors: Set = None,\n    ignored_classes: Set = None,\n    ignored_clips: Set = None,\n    annotation_suffix: Union[Set, str] = None,\n    annotation_path: Union[Set, str] = None,\n    behavior_file: str = None,\n    correction: Dict = None,\n    frame_limit: int = 0,\n    filter_annotated: bool = False,\n    filter_background: bool = False,\n    error_class: str = None,\n    min_frames_action: int = None,\n    key_objects: Tuple = None,\n    visibility_min_score: float = 0.2,\n    visibility_min_frac: float = 0.7,\n    mask: Dict = None,\n    use_hard_negatives: bool = False,\n    interactive: bool = False,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.remove", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.remove", "type": "function", "doc": "<p>Remove the samples corresponding to indices</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>indices : list\n    a list of integer indices to remove</p>\n", "signature": "(self, indices: List) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.key_objects", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.key_objects", "type": "function", "doc": "<p>Return a tuple of the key objects necessary to re-create the Store</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>key_objects : tuple\n    a tuple of key objects</p>\n", "signature": "(self) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.load_from_key_objects", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.load_from_key_objects", "type": "function", "doc": "<p>Load the information from a tuple of key objects</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>key_objects : tuple\n    a tuple of key objects</p>\n", "signature": "(self, key_objects: Tuple) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.to_ram", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.to_ram", "type": "function", "doc": "<p>Transfer the data samples to RAM if they were previously stored as file paths</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.get_original_coordinates", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.get_original_coordinates", "type": "function", "doc": "<p>Return the video_indices array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>original_coordinates : numpy.ndarray\n    an array that contains the coordinates of the data samples in original input data</p>\n", "signature": "(self) -> numpy.ndarray", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.create_subsample", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.create_subsample", "type": "function", "doc": "<p>Create a new store that contains a subsample of the data</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>indices : list\n    the indices to be included in the subsample\nssl_indices : list, optional\n    the indices to be included in the subsample without the annotation data</p>\n", "signature": "(self, indices: List, ssl_indices: List = None)", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.get_len", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.get_len", "type": "function", "doc": "<p>Get the length of the subsample of labeled/unlabeled data</p>\n\n<p>If return_unlabeled is True, the index is in the subsample of unlabeled data, if False in labeled\nand if return_unlabeled is None the index is already correct</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>return_unlabeled : bool\n    the identifier for the subsample</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>length : int\n    the length of the subsample</p>\n", "signature": "(self, return_unlabeled: bool) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.get_indices", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.get_indices", "type": "function", "doc": "<p>Get a list of indices of samples in the labeled/unlabeled subset</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>return_unlabeled : bool\n    the identifier for the subsample (<code>True</code> for unlabeled, <code>False</code> for labeled, <code>None</code> for the\n    whole dataset)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>indices : list\n    a list of indices that meet the criteria</p>\n", "signature": "(self, return_unlabeled: bool) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.count_classes", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.count_classes", "type": "function", "doc": "<p>Get a dictionary with class-wise frame counts</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>perc : bool, default False\n    if <code>True</code>, a fraction of the total frame count is returned\nzeros : bool, default False\n    if <code>True</code> and annotation is not exclusive, zero counts are returned\nbouts : bool, default False\n    if <code>True</code>, instead of frame counts segment counts are returned</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>count_dictionary : dict\n    a dictionary with class indices as keys and frame counts as values</p>\n", "signature": "(\n    self,\n    perc: bool = False,\n    zeros: bool = False,\n    bouts: bool = False\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.behaviors_dict", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.behaviors_dict", "type": "function", "doc": "<p>Get a dictionary of class names</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>behavior_dictionary: dict\n    a dictionary with class indices as keys and class names as values</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.annotation_class", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.annotation_class", "type": "function", "doc": "<p>Get the type of annotation ('exclusive_classification', 'nonexclusive_classification')</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>annotation_class : str\n    the type of annotation</p>\n", "signature": "(self) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.size", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.size", "type": "function", "doc": "<p>Get the total number of frames in the data</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>size : int\n    the total number of frames</p>\n", "signature": "(self) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.filtered_indices", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.filtered_indices", "type": "function", "doc": "<p>Return the indices of the samples that should be removed</p>\n\n<p>Choosing the indices can be based on any kind of filering defined in the __init__ function by the data\nparameters</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>indices_to_remove : list\n    a list of integer indices that should be removed</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.set_pseudo_labels", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.set_pseudo_labels", "type": "function", "doc": "<p>Set pseudo labels to the unlabeled data</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>labels : torch.Tensor\n    a tensor of pseudo-labels for the unlabeled data</p>\n", "signature": "(self, labels: torch.Tensor) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.ActionSegmentationStore.get_file_ids", "modulename": "dlc2action.data.annotation_store", "qualname": "ActionSegmentationStore.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>annotation_path : str | set\n    the path or the set of paths to the folder where the annotation files are stored\nannotation_suffix : str | set, optional\n    the suffix or the set of suffices such that the annotation files are named {video_id}{annotation_suffix}</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(\n    cls,\n    annotation_path: Union[str, Set],\n    annotation_suffix: Union[str, Set],\n    *args,\n    **kwargs\n) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.FileAnnotationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "FileAnnotationStore", "type": "class", "doc": "<p>A generalized implementation of <code>ActionSegmentationStore</code> for datasets where one file corresponds to one video</p>\n", "bases": "ActionSegmentationStore"}, {"fullname": "dlc2action.data.annotation_store.SequenceAnnotationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "SequenceAnnotationStore", "type": "class", "doc": "<p>A generalized implementation of <code>ActionSegmentationStore</code> for datasets where one file corresponds to multiple videos</p>\n", "bases": "ActionSegmentationStore"}, {"fullname": "dlc2action.data.annotation_store.SequenceAnnotationStore.get_file_ids", "modulename": "dlc2action.data.annotation_store", "qualname": "SequenceAnnotationStore.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filenames : list, optional\n    a list of annotation file paths\nannotation_path : str, optional\n    path to the annotation folder</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(\n    cls,\n    filenames: List = None,\n    annotation_path: str = None,\n    *args,\n    **kwargs\n) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.DLCAnnotationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "DLCAnnotationStore", "type": "class", "doc": "<p>DLC type annotation data</p>\n\n<p>The files are either the DLC2Action GUI output or a pickled dictionary of the following structure:\n    - nested dictionary,\n    - first-level keys are individual IDs,\n    - second-level keys are labels,\n    - values are lists of intervals,\n    - the lists of intervals is formatted as <code>[start_frame, end_frame, ambiguity]</code>,\n    - ambiguity is 1 if the action is ambiguous (!!at the moment DLC2Action will IGNORE those intervals!!) or 0 if it isn't.</p>\n\n<p>A minimum working example of such a dictionary is:</p>\n\n<pre><code>{\n    \"ind0\": {},\n    \"ind1\": {\n        \"running\": [60, 70, 0]],\n        \"eating\": []\n    }\n}\n</code></pre>\n\n<p>Here there are two animals: <code>\"ind0\"</code> and <code>\"ind1\"</code>, and two actions: running and eating.\nThe only annotated action is eating for <code>\"ind1\"</code> between frames 60 and 70.</p>\n\n<p>If you generate those files manually, run this code for a sanity check:</p>\n\n<pre><code>import pickle\n\nwith open(\"/path/to/annotation.pickle\", \"rb\") as f:\ndata = pickle.load(f)\n\nfor ind, ind_dict in data.items():\n    print(f'individual {ind}:')\n    for label, intervals in ind_dict.items():\n        for start, end, ambiguity in intervals:\n            if ambiguity == 0:\n                print(f'  from {start} to {end} frame: {label}')\n</code></pre>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>annotation_path\n\u251c\u2500\u2500 video1_annotation.pickle\n\u2514\u2500\u2500 video2_labels.pickle\n</code></pre>\n\n<p>Here <code>annotation_suffix</code> is <code>{'_annotation.pickle', '_labels.pickle'}</code>.</p>\n", "bases": "FileAnnotationStore"}, {"fullname": "dlc2action.data.annotation_store.BorisAnnotationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "BorisAnnotationStore", "type": "class", "doc": "<p>BORIS type annotation data</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>annotation_path\n\u251c\u2500\u2500 video1_annotation.pickle\n\u2514\u2500\u2500 video2_labels.pickle\n</code></pre>\n\n<p>Here <code>annotation_suffix</code> is <code>{'_annotation.pickle', '_labels.pickle'}</code>.</p>\n", "bases": "FileAnnotationStore"}, {"fullname": "dlc2action.data.annotation_store.BorisAnnotationStore.__init__", "modulename": "dlc2action.data.annotation_store", "qualname": "BorisAnnotationStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\nmin_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip start frames (not passed if creating from key objects)\nmax_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip end frames (not passed if creating from key objects)\nvisibility : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    visibility score arrays (not passed if creating from key objects or if irrelevant for the dataset)\nexclusive : bool, default True\n    if True, the annotation is single-label; if False, multi-label\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nbehaviors : set, optional\n    the list of behaviors to put in the annotation (not passed if creating a blank instance or if behaviors are\n    loaded from a file)\nignored_classes : set, optional\n    the list of behaviors from the behaviors list or file to not annotate\nannotation_suffix : str | set, optional\n    the suffix or the set of suffices such that the annotation files are named {video_id}{annotation_suffix}\n    (not passed if creating from key objects or if irrelevant for the dataset)\nannotation_path : str | set, optional\n    the path or the set of paths to the folder where the annotation files are stored (not passed if creating\n    from key objects)\nbehavior_file : str, optional\n    the path to an .xlsx behavior file (not passed if creating from key objects or if irrelevant for the dataset)\ncorrection : dict, optional\n    a dictionary of corrections for the labels (e.g. {'sleping': 'sleeping', 'calm locomotion': 'locomotion'},\n    can be used to correct for variations in naming or to merge several labels in one\nframe_limit : int, default 0\n    the smallest possible length of a clip (shorter clips are discarded)\nfilter_annotated : bool, default False\n    if True, the samples that do not have any labels will be filtered\nfilter_background : bool, default False\n    if True, only the unlabeled frames that are close to annotated frames will be labeled as background\nerror_class : str, optional\n    the name of the error class (the annotations that intersect with this label will be discarded)\nmin_frames_action : int, default 0\n    the minimum length of an action (shorter actions are not annotated)\nkey_objects : tuple, optional\n    the key objects to load the AnnotationStore from\nvisibility_min_score : float, default 5\n    the minimum visibility score for visibility filtering\nvisibility_min_frac : float, default 0.7\n    the minimum fraction of visible frames for visibility filtering\nmask : dict, optional\n    a masked value dictionary (for active learning simulation experiments)\nuse_hard_negatives : bool, default False\n    mark hard negatives as 2 instead of 0 or 1, for loss functions that have options for hard negative processing\ninteractive : bool, default False\n    if <code>True</code>, annotation is assigned to pairs of individuals\nignored_clips : set, optional\n    a set of clip ids to ignore</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    min_frames: Dict = None,\n    max_frames: Dict = None,\n    visibility: Dict = None,\n    exclusive: bool = True,\n    len_segment: int = 128,\n    overlap: int = 0,\n    behaviors: Set = None,\n    ignored_classes: Set = None,\n    annotation_suffix: Union[Set, str] = None,\n    annotation_path: Union[Set, str] = None,\n    behavior_file: str = None,\n    correction: Dict = None,\n    frame_limit: int = 0,\n    filter_annotated: bool = False,\n    filter_background: bool = False,\n    error_class: str = None,\n    min_frames_action: int = None,\n    key_objects: Tuple = None,\n    visibility_min_score: float = 0.2,\n    visibility_min_frac: float = 0.7,\n    mask: Dict = None,\n    use_hard_negatives: bool = False,\n    default_agent_name: str = 'ind0',\n    interactive: bool = False,\n    ignored_clips: Set = None,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.PKUMMDAnnotationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "PKUMMDAnnotationStore", "type": "class", "doc": "<p>PKU-MMD annotation data</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>annotation_path\n\u251c\u2500\u2500 0364-L.txt\n...\n\u2514\u2500\u2500 0144-M.txt\n</code></pre>\n", "bases": "FileAnnotationStore"}, {"fullname": "dlc2action.data.annotation_store.PKUMMDAnnotationStore.__init__", "modulename": "dlc2action.data.annotation_store", "qualname": "PKUMMDAnnotationStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\nmin_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip start frames (not passed if creating from key objects)\nmax_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip end frames (not passed if creating from key objects)\nvisibility : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    visibility score arrays (not passed if creating from key objects or if irrelevant for the dataset)\nexclusive : bool, default True\n    if True, the annotation is single-label; if False, multi-label\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nignored_classes : set, optional\n    the list of behaviors from the behaviors list or file to not annotate\nannotation_path : str | set, optional\n    the path or the set of paths to the folder where the annotation files are stored (not passed if creating\n    from key objects)\nbehavior_file : str, optional\n    the path to an .xlsx behavior file (not passed if creating from key objects or if irrelevant for the dataset)\ncorrection : dict, optional\n    a dictionary of corrections for the labels (e.g. {'sleping': 'sleeping', 'calm locomotion': 'locomotion'},\n    can be used to correct for variations in naming or to merge several labels in one\nframe_limit : int, default 0\n    the smallest possible length of a clip (shorter clips are discarded)\nfilter_annotated : bool, default False\n    if True, the samples that do not have any labels will be filtered\nfilter_background : bool, default False\n    if True, only the unlabeled frames that are close to annotated frames will be labeled as background\nerror_class : str, optional\n    the name of the error class (the annotations that intersect with this label will be discarded)\nmin_frames_action : int, default 0\n    the minimum length of an action (shorter actions are not annotated)\nkey_objects : tuple, optional\n    the key objects to load the AnnotationStore from\nvisibility_min_score : float, default 5\n    the minimum visibility score for visibility filtering\nvisibility_min_frac : float, default 0.7\n    the minimum fraction of visible frames for visibility filtering\nmask : dict, optional\n    a masked value dictionary (for active learning simulation experiments)\nuse_hard_negatives : bool, default False\n    mark hard negatives as 2 instead of 0 or 1, for loss functions that have options for hard negative processing\ninteractive : bool, default False\n    if <code>True</code>, annotation is assigned to pairs of individuals\nignored_clips : set, optional\n    a set of clip ids to ignore</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    min_frames: Dict = None,\n    max_frames: Dict = None,\n    visibility: Dict = None,\n    exclusive: bool = True,\n    len_segment: int = 128,\n    overlap: int = 0,\n    ignored_classes: Set = None,\n    annotation_path: Union[Set, str] = None,\n    behavior_file: str = None,\n    correction: Dict = None,\n    frame_limit: int = 0,\n    filter_annotated: bool = False,\n    filter_background: bool = False,\n    error_class: str = None,\n    min_frames_action: int = None,\n    key_objects: Tuple = None,\n    visibility_min_score: float = 0,\n    visibility_min_frac: float = 0,\n    mask: Dict = None,\n    use_hard_negatives: bool = False,\n    interactive: bool = False,\n    ignored_clips: Set = None,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.PKUMMDAnnotationStore.get_file_ids", "modulename": "dlc2action.data.annotation_store", "qualname": "PKUMMDAnnotationStore.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>annotation_path : str | set\n    the path or the set of paths to the folder where the annotation files are stored</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(cls, annotation_path: Union[str, Set], *args, **kwargs) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.CalMS21AnnotationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "CalMS21AnnotationStore", "type": "class", "doc": "<p>CalMS21 annotation data</p>\n\n<p>Use the <code>'random:test_from_name:{name}'</code> and <code>'val-from-name:{val_name}:test-from-name:{test_name}'</code>\npartitioning methods with <code>'train'</code>, <code>'test'</code> and <code>'unlabeled'</code> names to separate into train, test and validation\nsubsets according to the original files. For example, with <code>'val-from-name:test:test-from-name:unlabeled'</code>\nthe data from the test file will go into validation and the unlabeled files will be the test.</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>annotation_path\n\u251c\u2500\u2500 calms21_task_train.npy\n\u251c\u2500\u2500 calms21_task_test.npy\n\u251c\u2500\u2500 calms21_unlabeled_videos_part1.npy\n\u251c\u2500\u2500 calms21_unlabeled_videos_part2.npy\n\u2514\u2500\u2500 calms21_unlabeled_videos_part3.npy\n</code></pre>\n", "bases": "SequenceAnnotationStore"}, {"fullname": "dlc2action.data.annotation_store.CalMS21AnnotationStore.__init__", "modulename": "dlc2action.data.annotation_store", "qualname": "CalMS21AnnotationStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>task_n : [1, 2]\n    the number of the task\ninclude_task1 : bool, default True\n    include task 1 data to training set\nvideo_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\nmin_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip start frames (not passed if creating from key objects)\nmax_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip end frames (not passed if creating from key objects)\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nignored_classes : set, optional\n    the list of behaviors from the behaviors list or file to not annotate\nannotation_path : str | set, optional\n    the path or the set of paths to the folder where the annotation files are stored (not passed if creating\n    from key objects)\nkey_objects : tuple, optional\n    the key objects to load the AnnotationStore from\ntreba_files : bool, default False\n    if <code>True</code>, TREBA feature files will be loaded</p>\n", "signature": "(\n    self,\n    task_n: int = 1,\n    include_task1: bool = True,\n    video_order: List = None,\n    min_frames: Dict = None,\n    max_frames: Dict = None,\n    len_segment: int = 128,\n    overlap: int = 0,\n    ignored_classes: Set = None,\n    annotation_path: Union[Set, str] = None,\n    key_objects: Tuple = None,\n    treba_files: bool = False,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.CalMS21AnnotationStore.get_file_ids", "modulename": "dlc2action.data.annotation_store", "qualname": "CalMS21AnnotationStore.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>task_n : {1, 2, 3}\n    the index of the CalMS21 challenge task\ninclude_task1 : bool, default False\n    if <code>True</code>, the training file of the task 1 will be loaded\ntreba_files : bool, default False\n    if <code>True</code>, the TREBA feature files will be loaded\nfilenames : set, optional\n    a set of string filenames to search for (only basenames, not the whole paths)\nannotation_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(\n    cls,\n    task_n: int = 1,\n    include_task1: bool = False,\n    treba_files: bool = False,\n    annotation_path: Union[str, Set] = None,\n    file_paths=None,\n    *args,\n    **kwargs\n) -> collections.abc.Iterable", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.CSVAnnotationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "CSVAnnotationStore", "type": "class", "doc": "<p>CSV type annotation data</p>\n\n<p>Assumes that files are saved as .csv tables with at least the following columns:</p>\n\n<ul>\n<li>from / start : start of action,</li>\n<li>to / end : end of action,</li>\n<li>class / behavior / behaviour / label / type : action label.</li>\n</ul>\n\n<p>If the times are set in seconds instead of frames, don't forget to set the <code>fps</code> parameter to your frame rate.</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>annotation_path\n\u251c\u2500\u2500 video1_annotation.csv\n\u2514\u2500\u2500 video2_labels.csv\n</code></pre>\n\n<p>Here <code>annotation_suffix</code> is <code>{'_annotation.csv', '_labels.csv'}</code>.</p>\n", "bases": "FileAnnotationStore"}, {"fullname": "dlc2action.data.annotation_store.CSVAnnotationStore.__init__", "modulename": "dlc2action.data.annotation_store", "qualname": "CSVAnnotationStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\nmin_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip start frames (not passed if creating from key objects)\nmax_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip end frames (not passed if creating from key objects)\nvisibility : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    visibility score arrays (not passed if creating from key objects or if irrelevant for the dataset)\nexclusive : bool, default True\n    if True, the annotation is single-label; if False, multi-label\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nbehaviors : set, optional\n    the list of behaviors to put in the annotation (not passed if creating a blank instance or if behaviors are\n    loaded from a file)\nignored_classes : set, optional\n    the list of behaviors from the behaviors list or file to not annotate\nannotation_suffix : str | set, optional\n    the suffix or the set of suffices such that the annotation files are named {video_id}{annotation_suffix}\n    (not passed if creating from key objects or if irrelevant for the dataset)\nannotation_path : str | set, optional\n    the path or the set of paths to the folder where the annotation files are stored (not passed if creating\n    from key objects)\nbehavior_file : str, optional\n    the path to an .xlsx behavior file (not passed if creating from key objects or if irrelevant for the dataset)\ncorrection : dict, optional\n    a dictionary of corrections for the labels (e.g. {'sleping': 'sleeping', 'calm locomotion': 'locomotion'},\n    can be used to correct for variations in naming or to merge several labels in one\nframe_limit : int, default 0\n    the smallest possible length of a clip (shorter clips are discarded)\nfilter_annotated : bool, default False\n    if True, the samples that do not have any labels will be filtered\nfilter_background : bool, default False\n    if True, only the unlabeled frames that are close to annotated frames will be labeled as background\nerror_class : str, optional\n    the name of the error class (the annotations that intersect with this label will be discarded)\nmin_frames_action : int, default 0\n    the minimum length of an action (shorter actions are not annotated)\nkey_objects : tuple, optional\n    the key objects to load the AnnotationStore from\nvisibility_min_score : float, default 5\n    the minimum visibility score for visibility filtering\nvisibility_min_frac : float, default 0.7\n    the minimum fraction of visible frames for visibility filtering\nmask : dict, optional\n    a masked value dictionary (for active learning simulation experiments)\ndefault_agent_name : str, default \"ind0\"\n    the clip id to use when there is no given\nseparator : str, default \",\"\n    the separator in the csv files\nfps : int, default 30\n    frames per second in the videos</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    min_frames: Dict = None,\n    max_frames: Dict = None,\n    visibility: Dict = None,\n    exclusive: bool = True,\n    len_segment: int = 128,\n    overlap: int = 0,\n    behaviors: Set = None,\n    ignored_classes: Set = None,\n    annotation_suffix: Union[Set, str] = None,\n    annotation_path: Union[Set, str] = None,\n    behavior_file: str = None,\n    correction: Dict = None,\n    frame_limit: int = 0,\n    filter_annotated: bool = False,\n    filter_background: bool = False,\n    error_class: str = None,\n    min_frames_action: int = None,\n    key_objects: Tuple = None,\n    visibility_min_score: float = 0.2,\n    visibility_min_frac: float = 0.7,\n    mask: Dict = None,\n    default_agent_name: str = 'ind0',\n    separator: str = ',',\n    fps: int = 30,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.annotation_store.SIMBAAnnotationStore", "modulename": "dlc2action.data.annotation_store", "qualname": "SIMBAAnnotationStore", "type": "class", "doc": "<p>SIMBA paper format data</p>\n\n<p>Assumes the following file structure:\n```\nannotation_path\n\u251c\u2500\u2500 Video1.csv\n...\n\u2514\u2500\u2500 Video9.csv</p>\n", "bases": "FileAnnotationStore"}, {"fullname": "dlc2action.data.annotation_store.SIMBAAnnotationStore.__init__", "modulename": "dlc2action.data.annotation_store", "qualname": "SIMBAAnnotationStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\nmin_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip start frames (not passed if creating from key objects)\nmax_frames : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    clip end frames (not passed if creating from key objects)\nvisibility : dict, optional\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    visibility score arrays (not passed if creating from key objects or if irrelevant for the dataset)\nexclusive : bool, default True\n    if True, the annotation is single-label; if False, multi-label\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nbehaviors : set, optional\n    the list of behaviors to put in the annotation (not passed if creating a blank instance or if behaviors are\n    loaded from a file)\nignored_classes : set, optional\n    the list of behaviors from the behaviors list or file to not annotate\nignored_clips : set, optional\n    clip ids to ignore\nannotation_path : str | set, optional\n    the path or the set of paths to the folder where the annotation files are stored (not passed if creating\n    from key objects)\nbehavior_file : str, optional\n    the path to an .xlsx behavior file (not passed if creating from key objects or if irrelevant for the dataset)\ncorrection : dict, optional\n    a dictionary of corrections for the labels (e.g. {'sleping': 'sleeping', 'calm locomotion': 'locomotion'},\n    can be used to correct for variations in naming or to merge several labels in one\nfilter_annotated : bool, default False\n    if True, the samples that do not have any labels will be filtered\nfilter_background : bool, default False\n    if True, only the unlabeled frames that are close to annotated frames will be labeled as background\nerror_class : str, optional\n    the name of the error class (the annotations that intersect with this label will be discarded)\nmin_frames_action : int, default 0\n    the minimum length of an action (shorter actions are not annotated)\nkey_objects : tuple, optional\n    the key objects to load the AnnotationStore from\nvisibility_min_score : float, default 5\n    the minimum visibility score for visibility filtering\nvisibility_min_frac : float, default 0.7\n    the minimum fraction of visible frames for visibility filtering\nmask : dict, optional\n    a masked value dictionary (for active learning simulation experiments)\nuse_hard_negatives : bool, default False\n    mark hard negatives as 2 instead of 0 or 1, for loss functions that have options for hard negative processing\nannotation_suffix : str | set, optional\n    the suffix or the set of suffices such that the annotation files are named {video_id}{annotation_suffix}\n    (not passed if creating from key objects or if irrelevant for the dataset)</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    min_frames: Dict = None,\n    max_frames: Dict = None,\n    visibility: Dict = None,\n    exclusive: bool = True,\n    len_segment: int = 128,\n    overlap: int = 0,\n    behaviors: Set = None,\n    ignored_classes: Set = None,\n    ignored_clips: Set = None,\n    annotation_path: Union[Set, str] = None,\n    correction: Dict = None,\n    filter_annotated: bool = False,\n    filter_background: bool = False,\n    error_class: str = None,\n    min_frames_action: int = None,\n    key_objects: Tuple = None,\n    visibility_min_score: float = 0.2,\n    visibility_min_frac: float = 0.7,\n    mask: Dict = None,\n    use_hard_negatives: bool = False,\n    annotation_suffix: str = None,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store", "modulename": "dlc2action.data.base_store", "type": "module", "doc": "<p>Abstract parent classes for the store objects</p>\n"}, {"fullname": "dlc2action.data.base_store.Store", "modulename": "dlc2action.data.base_store", "qualname": "Store", "type": "class", "doc": "<p>A general parent class for <code>AnnotationStore</code> and <code>InputStore</code></p>\n\n<p>Processes input video information and generates ordered arrays of data samples and corresponding unique\noriginal coordinates, as well as some meta objects.\nIt is assumed that the input videos are separated into clips (e.g. corresponding to different individuals).\nEach video and each clip inside the video has a unique id (video_id and clip_id, correspondingly).\nThe original coordinates object contains information about the video_id, clip_id and start time of the\nsamples in the original input data.\nA Store has to be fully defined with a tuple of key objects.\nThe data array can be accessed with integer indices.\nThe samples can be stored as a tensor or TensorDict in RAM or as an array of file paths to be loaded on runtime.</p>\n", "bases": "abc.ABC"}, {"fullname": "dlc2action.data.base_store.Store.remove", "modulename": "dlc2action.data.base_store", "qualname": "Store.remove", "type": "function", "doc": "<p>Remove the samples corresponding to indices</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>indices : int\n    a list of integer indices to remove</p>\n", "signature": "(self, indices: List) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.Store.key_objects", "modulename": "dlc2action.data.base_store", "qualname": "Store.key_objects", "type": "function", "doc": "<p>Return a tuple of the key objects necessary to re-create the Store</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>key_objects : tuple\n    a tuple of key objects</p>\n", "signature": "(self) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.Store.load_from_key_objects", "modulename": "dlc2action.data.base_store", "qualname": "Store.load_from_key_objects", "type": "function", "doc": "<p>Load the information from a tuple of key objects</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>key_objects : tuple\n    a tuple of key objects</p>\n", "signature": "(self, key_objects: Tuple) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.Store.to_ram", "modulename": "dlc2action.data.base_store", "qualname": "Store.to_ram", "type": "function", "doc": "<p>Transfer the data samples to RAM if they were previously stored as file paths</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.Store.get_original_coordinates", "modulename": "dlc2action.data.base_store", "qualname": "Store.get_original_coordinates", "type": "function", "doc": "<p>Return the original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>np.ndarray\n    an array that contains the coordinates of the data samples in original input data (video id, clip id,\n    start frame)</p>\n", "signature": "(self) -> numpy.ndarray", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.Store.create_subsample", "modulename": "dlc2action.data.base_store", "qualname": "Store.create_subsample", "type": "function", "doc": "<p>Create a new store that contains a subsample of the data</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>indices : list\n    the indices to be included in the subsample\nssl_indices : list, optional\n    the indices to be included in the subsample without the annotation data</p>\n", "signature": "(self, indices: List, ssl_indices: List = None)", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.Store.get_file_ids", "modulename": "dlc2action.data.base_store", "qualname": "Store.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(cls, *args, **kwargs) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.Store.get_parameters", "modulename": "dlc2action.data.base_store", "qualname": "Store.get_parameters", "type": "function", "doc": "<p>Generate a list of parameter names for the __init__ function</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>parameter_names: list\n    a list of necessary parameter names</p>\n", "signature": "(cls) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.Store.new", "modulename": "dlc2action.data.base_store", "qualname": "Store.new", "type": "function", "doc": "<p>Create a new instance of the same class</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>new_instance : Store\n    a new instance of the same class</p>\n", "signature": "(cls)", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore", "modulename": "dlc2action.data.base_store", "qualname": "InputStore", "type": "class", "doc": "<p>A class that generates model input data from video information and stores it</p>\n\n<p>Processes input video information and generates ordered arrays of data samples and corresponding unique\noriginal coordinates, as well as some meta objects.\nIt is assumed that the input videos are separated into clips (e.g. corresponding to different individuals).\nEach video and each clip inside the video has a unique id (video_id and clip_id, correspondingly).\nThe original coordinates object contains information about the video_id, clip_id and start time of the\nsamples in the original input data.\nAn InputStore has to be fully defined with a tuple of key objects.\nThe data array can be accessed with integer indices.\nThe samples can be stored as a TensorDict in RAM or as an array of file paths to be loaded on runtime.\nWhen no arguments are passed a blank class instance should be created that can later be filled with\ninformation from key objects</p>\n", "bases": "Store"}, {"fullname": "dlc2action.data.base_store.InputStore.__init__", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\nkey_objects : tuple, optional\n    a tuple of key objects\ndata_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)\nfeature_save_path : str, optional\n    the path to the folder where pre-processed files are stored (not passed if creating from key objects)\nfeature_extraction_pars : dict, optional\n    a dictionary of feature extraction parameters (not passed if creating from key objects)</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    key_objects: Tuple = None,\n    data_path: Union[str, List] = None,\n    file_paths: List = None,\n    feature_save_path: str = None,\n    feature_extraction_pars: Dict = None,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_video_id", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_video_id", "type": "function", "doc": "<p>Get the video id from an element of original coordinates</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>coords : tuple\n    an element of the original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_id: str\n    the id of the video that the coordinates point to</p>\n", "signature": "(self, coords: Tuple) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_clip_id", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_clip_id", "type": "function", "doc": "<p>Get the clip id from an element of original coordinates</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>coords : tuple\n    an element of the original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>clip_id : str\n    the id of the clip that the coordinates point to</p>\n", "signature": "(self, coords: Tuple) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_clip_length", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_clip_length", "type": "function", "doc": "<p>Get the clip length from the id</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_id : str\n    the video id\nclip_id : str\n    the clip id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>clip_length : int\n    the length of the clip</p>\n", "signature": "(self, video_id: str, clip_id: str) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_clip_start_end", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_clip_start_end", "type": "function", "doc": "<p>Get the clip start and end frames from an element of original coordinates</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>coords : tuple\n    an element of original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>start: int\n    the start frame of the clip that the coordinates point to\nend : int\n    the end frame of the clip that the coordinates point to</p>\n", "signature": "(self, coords: Tuple) -> Tuple[int, int]", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_clip_start", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_clip_start", "type": "function", "doc": "<p>Get the clip start frame from the video id and the clip id</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_id : str\n    the video id\nclip_id : str\n    the clip id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>clip_start : int\n    the start frame of the clip</p>\n", "signature": "(self, video_id: str, clip_id: str) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_visibility", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_visibility", "type": "function", "doc": "<p>Get the fraction of the frames in that have a visibility score better than a hard_threshold</p>\n\n<p>For example, in the case of keypoint data the visibility score can be the number of identified keypoints.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_id : str\n    the video id of the frames\nclip_id : str\n    the clip id of the frames\nstart : int\n    the start frame\nend : int\n    the end frame\nscore : float\n    the visibility score hard_threshold</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>frac_visible: float\n    the fraction of frames with visibility above the hard_threshold</p>\n", "signature": "(\n    self,\n    video_id: str,\n    clip_id: str,\n    start: int,\n    end: int,\n    score: float\n) -> float", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_annotation_objects", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_annotation_objects", "type": "function", "doc": "<p>Get a dictionary of objects necessary to create an AnnotationStore</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>annotation_objects : dict\n    a dictionary of objects to be passed to the AnnotationStore constructor where the keys are the names of\n    the objects</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_folder", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_folder", "type": "function", "doc": "<p>Get the input folder that the file with this video id was read from</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_id : str\n    the video id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>folder : str\n    the path to the directory that contains the input file associated with the video id</p>\n", "signature": "(self, video_id: str) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_clip_length_from_coords", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_clip_length_from_coords", "type": "function", "doc": "<p>Get the length of a clip from an element of the original coordinates array</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>coords : tuple\n    an element of the original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>clip_length : int\n    the length of the clip</p>\n", "signature": "(self, coords: Tuple) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_folder_order", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_folder_order", "type": "function", "doc": "<p>Get a list of folders corresponding to the data samples</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>folder_order : list\n    a list of string folder basenames corresponding to the data samples (e.g. 'folder2'\n    if the corresponding file was read from '/path/to/folder1/folder2')</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_video_id_order", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_video_id_order", "type": "function", "doc": "<p>Get a list of video ids corresponding to the data samples</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_id_order : list\n    a list of string names of the video ids corresponding to the data samples</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_tag", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_tag", "type": "function", "doc": "<p>Return a tag object corresponding to an index</p>\n\n<p>Tags can carry meta information (like annotator id) and are accepted by models that require\nthat information and by metrics (some metrics have options for averaging over the tags).\nWhen a tag is <code>None</code>, it is not passed to the model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>idx : int\n    the index</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tag : int\n    the tag index</p>\n", "signature": "(self, idx: int) -> Optional[int]", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_indices", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_indices", "type": "function", "doc": "<p>Get a list of indices of samples that have a specific meta tag</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tag : int\n    the meta tag for the subsample (<code>None</code> for the whole dataset)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>indices : list\n    a list of indices that meet the criteria</p>\n", "signature": "(self, tag: int) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.InputStore.get_tags", "modulename": "dlc2action.data.base_store", "qualname": "InputStore.get_tags", "type": "function", "doc": "<p>Get a list of all meta tags</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tags: List\n    a list of unique meta tag values</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.AnnotationStore", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore", "type": "class", "doc": "<p>A class that generates annotation from video information and stores it</p>\n\n<p>Processes input video information and generates ordered arrays of annotation samples and corresponding unique\noriginal coordinates, as well as some meta objects.\nIt is assumed that the input videos are separated into clips (e.g. corresponding to different individuals).\nEach video and each clip inside the video has a unique id (video_id and clip_id, correspondingly).\nThe original coordinates object contains information about the video_id, clip_id and start time of the\nsamples in the original input data.\nAn AnnotationStore has to be fully defined with a tuple of key objects.\nThe annotation array can be accessed with integer indices.\nThe samples can be stored as a torch.Tensor in RAM or as an array of file paths to be loaded on runtime.\nWhen no arguments are passed a blank class instance should be created that can later be filled with\ninformation from key objects</p>\n", "bases": "Store"}, {"fullname": "dlc2action.data.base_store.AnnotationStore.__init__", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\nkey_objects : tuple, optional\n    a tuple of key objects\nannotation_path : str | set, optional\n    the path or the set of paths to the folder where the annotation files are stored (not passed if creating\n    from key objects)</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    key_objects: Tuple = None,\n    annotation_path: Union[str, Set] = None,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.AnnotationStore.required_objects", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore.required_objects", "type": "variable", "doc": "<p>A list of string names of the objects required from the input store</p>\n", "default_value": " = []"}, {"fullname": "dlc2action.data.base_store.AnnotationStore.get_len", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore.get_len", "type": "function", "doc": "<p>Get the length of the subsample of labeled/unlabeled data</p>\n\n<p>If return_unlabeled is True, the index is in the subsample of unlabeled data, if False in labeled\nand if return_unlabeled is None the index is already correct</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>return_unlabeled : bool\n    the identifier for the subsample</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>length : int\n    the length of the subsample</p>\n", "signature": "(self, return_unlabeled: bool) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.AnnotationStore.count_classes", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore.count_classes", "type": "function", "doc": "<p>Get a dictionary with class-wise frame counts</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>frac : bool, default False\n    if <code>True</code>, a fraction of the total frame count is returned\nzeros : bool. default False\n    if <code>True</code>, the number of known negative samples is counted (only if the annotation is multi-label)\nbouts : bool, default False\n    if <code>True</code>, instead of frame counts segment counts are returned</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>count_dictionary : dict\n    a dictionary with class indices as keys and frame counts as values</p>\n", "signature": "(\n    self,\n    frac: bool = False,\n    zeros: bool = False,\n    bouts: bool = False\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.AnnotationStore.behaviors_dict", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore.behaviors_dict", "type": "function", "doc": "<p>Get a dictionary of class names</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>behavior_dictionary: dict\n    a dictionary with class indices as keys and class names as values</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.AnnotationStore.annotation_class", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore.annotation_class", "type": "function", "doc": "<p>Get the type of annotation ('exclusive_classification', 'nonexclusive_classification', more coming soon)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>annotation_class : str\n    the type of annotation</p>\n", "signature": "(self) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.AnnotationStore.size", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore.size", "type": "function", "doc": "<p>Get the total number of frames in the data</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>size : int\n    the total number of frames</p>\n", "signature": "(self) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.AnnotationStore.filtered_indices", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore.filtered_indices", "type": "function", "doc": "<p>Return the indices of the samples that should be removed</p>\n\n<p>Choosing the indices can be based on any kind of filering defined in the __init__ function by the data\nparameters</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>indices_to_remove : list\n    a list of integer indices that should be removed</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.AnnotationStore.set_pseudo_labels", "modulename": "dlc2action.data.base_store", "qualname": "AnnotationStore.set_pseudo_labels", "type": "function", "doc": "<p>Set pseudo labels to the unlabeled data</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>labels : torch.Tensor\n    a tensor of pseudo-labels for the unlabeled data</p>\n", "signature": "(self, labels: torch.Tensor) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.PoseInputStore", "modulename": "dlc2action.data.base_store", "qualname": "PoseInputStore", "type": "class", "doc": "<p>A subclass of InputStore for pose estimation data</p>\n\n<p>Contains methods used by pose estimation feature extractors.\nAll methods receive a data dictionary as input. This dictionary is the same as what is passed to the\nfeature extractor and the only limitations for the structure are that it has to relate to one video id\nand have clip ids as keys. Read the documentation at <code>dlc2action.data</code> to find out more about videos\nand clips.</p>\n", "bases": "InputStore"}, {"fullname": "dlc2action.data.base_store.PoseInputStore.get_likelihood", "modulename": "dlc2action.data.base_store", "qualname": "PoseInputStore.get_likelihood", "type": "function", "doc": "<p>Get the likelihood values</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_dict : dict\n    the data dictionary\nclip_id : str\n    the clip id\nbodypart : str\n    the name of the body part</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>likelihoods: np.ndarrray | None\n    <code>None</code> if the dataset doesn't have likelihoods or an array of shape (#timestamps)</p>\n", "signature": "(\n    self,\n    data_dict: Dict,\n    clip_id: str,\n    bodypart: str\n) -> Optional[numpy.ndarray]", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.PoseInputStore.get_coords", "modulename": "dlc2action.data.base_store", "qualname": "PoseInputStore.get_coords", "type": "function", "doc": "<p>Get the coordinates array of a specific body part in a specific clip</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_dict : dict\n    the data dictionary\nclip_id : str\n    the clip id\nbodypart : str\n    the name of the body part</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>coords : np.ndarray\n    the coordinates array of shape (#timesteps, #coordinates)</p>\n", "signature": "(self, data_dict: Dict, clip_id: str, bodypart: str) -> numpy.ndarray", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.PoseInputStore.get_bodyparts", "modulename": "dlc2action.data.base_store", "qualname": "PoseInputStore.get_bodyparts", "type": "function", "doc": "<p>Get a list of bodypart names</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>bodyparts : list\n    a list of string or integer body part names</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.base_store.PoseInputStore.get_n_frames", "modulename": "dlc2action.data.base_store", "qualname": "PoseInputStore.get_n_frames", "type": "function", "doc": "<p>Get the length of the clip</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_dict : dict\n    the data dictionary\nclip_id : str\n    the clip id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>n_frames : int\n    the length of the clip</p>\n", "signature": "(self, data_dict: Dict, clip_id: str) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset", "modulename": "dlc2action.data.dataset", "type": "module", "doc": "<p>Behavior dataset (class that manages high-level data interactions)</p>\n"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset", "type": "class", "doc": "<p>A generalized dataset class</p>\n\n<p>Data and annotation are stored in separate InputStore and AnnotationStore objects; the dataset class\nmanages their interactions.</p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.__init__", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_type : str\n    the data type (see available types by running BehaviorDataset.data_types())\nannotation_type : str\n    the annotation type (see available types by running BehaviorDataset.annotation_types())\nssl_transformations : list\n    a list of functions that take a sample dictionary as input and return an (ssl input, ssl target) tuple\nsaved_data_path : str\n    the path to a pre-computed pickled dataset\ninput_store : InputStore\n    a pre-computed input store\nannotation_store : AnnotationStore\n    a precomputed annotation store\nonly_load_annotated : bool\n    if True, the input files that don't have a matching annotation file will be disregarded\n*data_parameters : dict\n    parameters to initialize the input and annotation stores</p>\n", "signature": "(\n    self,\n    data_type: str,\n    annotation_type: str = 'none',\n    ssl_transformations: List = None,\n    saved_data_path: str = None,\n    input_store: dlc2action.data.base_store.InputStore = None,\n    annotation_store: dlc2action.data.base_store.AnnotationStore = None,\n    only_load_annotated: bool = False,\n    recompute_annotation: bool = False,\n    ids: List = None,\n    **data_parameters\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_tags", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_tags", "type": "function", "doc": "<p>Get a list of all meta tags</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tags: List\n    a list of unique meta tag values</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.save", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.save", "type": "function", "doc": "<p>Save the dictionary</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>save_path : str\n    the path where the pickled file will be stored</p>\n", "signature": "(self, save_path: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.to_ram", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.to_ram", "type": "function", "doc": "<p>Transfer the dataset to RAM</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.generate_full_length_gt", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.generate_full_length_gt", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.generate_full_length_prediction", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.generate_full_length_prediction", "type": "function", "doc": "<p>Map predictions for the equal-length pieces to predictions for the original data</p>\n\n<p>Probabilities are averaged over predictions on overlapping intervals.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predicted: torch.Tensor\n    a tensor of predicted probabilities of shape <code>(N, #classes, #frames)</code></p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>full_length_prediction : dict\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and values are\n    averaged probability tensors</p>\n", "signature": "(self, predicted: torch.Tensor) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.find_valleys", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.find_valleys", "type": "function", "doc": "<p>Find the intervals where the probability of a certain class is below or above a certain hard_threshold</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predicted : torch.Tensor | dict\n    either a tensor of predictions for the data prompts or the output of\n    <code>BehaviorDataset.generate_full_length_prediction</code>\nthreshold : float, default 0.5\n    the main hard_threshold\nmin_frames : int, default 0\n    the minimum length of the intervals\nvisibility_min_score : float, default 0\n    the minimum visibility score in the intervals\nvisibility_min_frac : float, default 0\n    fraction of the interval that has to have the visibility score larger than visibility_score_thr\nmain_class : int, default 1\n    the index of the class the function is inspecting\nlow : bool, default True\n    if True, the probability in the intervals has to be below the hard_threshold, and if False, it has to be above\npredicted_error : torch.Tensor, optional\n    a tensor of error predictions for the data prompts\nerror_threshold : float, default 0.5\n    maximum possible probability of error at the intervals\nhysteresis: bool, default False\n    if True, the function will apply a hysteresis hard_threshold with the soft hard_threshold defined by threshold_diff\nthreshold_diff: float, optional\n    the difference between the soft and hard hard_threshold if hysteresis is used; if hysteresis is True, low is False and threshold_diff is None, the soft hard_threshold condition is set to the main_class having a larger probability than other classes\nmin_frames_error: int, optional\n    if not None, the intervals will only be considered where the error probability is below error_threshold at at least min_frames_error consecutive frames</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>valleys : dict\n    a dictionary where keys are video ids and values are lists of (start, end, individual name) tuples that denote the chosen intervals</p>\n", "signature": "(\n    self,\n    predicted: Union[torch.Tensor, Dict],\n    threshold: float = 0.5,\n    min_frames: int = 0,\n    visibility_min_score: float = 0,\n    visibility_min_frac: float = 0,\n    main_class: int = 1,\n    low: bool = True,\n    predicted_error: torch.Tensor = None,\n    error_threshold: float = 0.5,\n    hysteresis: bool = False,\n    threshold_diff: float = None,\n    min_frames_error: int = None,\n    smooth_interval: int = 1,\n    cut_annotated: bool = False\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.valleys_union", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.valleys_union", "type": "function", "doc": "<p>Find the intersection of two valleys dictionaries</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>valleys_list : list\n    a list of valleys dictionaries</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>intersection : dict\n    a new valleys dictionary with the intersection of the input intervals</p>\n", "signature": "(self, valleys_list) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.valleys_intersection", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.valleys_intersection", "type": "function", "doc": "<p>Find the intersection of two valleys dictionaries</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>valleys_list : list\n    a list of valleys dictionaries</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>intersection : dict\n    a new valleys dictionary with the intersection of the input intervals</p>\n", "signature": "(self, valleys_list) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.partition_train_test_val", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.partition_train_test_val", "type": "function", "doc": "<p>Partition the dataset into three new datasets</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>use_test : float, default 0\n    The fraction of the test dataset to be used in training without labels\nsplit_path : str, optional\n    The path to load the split information from (if <code>'file'</code> method is used) and to save it to\n    (if <code>'save_split'</code> is <code>True</code>)\nmethod : {'random', 'random:test-from-name', 'random:test-from-name:{name}',\n    'val-from-name:{val_name}:test-from-name:{test_name}',\n    'random:equalize:segments', 'random:equalize:videos',\n    'folders', 'time', 'time:strict', 'file'}\n    The partitioning method:\n    - <code>'random'</code>: sort videos into subsets randomly,\n    - <code>'random:test-from-name'</code> (or <code>'random:test-from-name:{name}'</code>): sort videos into training and validation\n        subsets randomly and create\n        the test subset from the video ids that start with a speific substring (<code>'test'</code> by default, or <code>name</code>\n        if provided),\n    - <code>'random:equalize:segments'</code> and <code>'random:equalize:videos'</code>: sort videos into subsets randomly but\n        making sure that for the rarest classes at least <code>0.8 * val_frac</code> of the videos/segments that contain\n        occurences of the class get into the validation subset and <code>0.8 * test_frac</code> get into the test subset;\n        this in ensured for all classes in order of increasing number of occurences until the validation and\n        test subsets are full\n    - <code>'val-from-name:{val_name}:test-from-name:{test_name}'</code>: create the validation and test\n        subsets from the video ids that start with specific substrings (<code>val_name</code> for validation\n        and <code>test_name</code> for test) and sort all other videos into the training subset\n    - <code>'folders'</code>: read videos from folders named <em>test</em>, <em>train</em> and <em>val</em> into corresponding subsets,\n    - <code>'time'</code>: split each video into training, validation and test subsequences,\n    - <code>'time:strict'</code>: split each video into validation, test and training subsequences\n        and throw out the last segments in validation and test (to get rid of overlaps),\n    - <code>'file'</code>: split according to a split file.\nval_frac : float, default 0\n    The fraction of the dataset to be used in validation\ntest_frac : float, default 0\n    The fraction of the dataset to be used in test\nsave_split : bool, default False\n    Save a split file if True</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>train_dataset : BehaviorDataset\n    train dataset</p>\n\n<p>val_dataset : BehaviorDataset\n    validation dataset</p>\n\n<p>test_dataset : BehaviorDataset\n    test dataset</p>\n", "signature": "(\n    self,\n    use_test: float = 0,\n    split_path: str = None,\n    method: str = 'random',\n    val_frac: float = 0,\n    test_frac: float = 0,\n    save_split: bool = False,\n    normalize: bool = False,\n    skip_normalization_keys: List = None,\n    stats: Dict = None\n) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.class_weights", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.class_weights", "type": "function", "doc": "<p>Calculate class weights in inverse proportion to number of samples</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>weights: list\n    a list of class weights</p>\n", "signature": "(self, proportional=False) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.boundary_class_weight", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.boundary_class_weight", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.count_classes", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.count_classes", "type": "function", "doc": "<p>Get a class counter dictionary</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>bouts : bool, default False\n    if <code>True</code>, instead of frame counts segment counts are returned</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>count_dictionary : dict\n    a dictionary with class indices as keys and frame or bout counts as values</p>\n", "signature": "(self, bouts: bool = False) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.behaviors_dict", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.behaviors_dict", "type": "function", "doc": "<p>Get a behavior dictionary</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dict\n    behavior dictionary</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.bodyparts_order", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.bodyparts_order", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.features_shape", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.features_shape", "type": "function", "doc": "<p>Get the shapes of the input features</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>shapes : Dict\n    a dictionary with the shapes of the features</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.num_classes", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.num_classes", "type": "function", "doc": "<p>Get the number of classes in the data</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>num_classes : int\n    the number of classes</p>\n", "signature": "(self) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.len_segment", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.len_segment", "type": "function", "doc": "<p>Get the segment length in the data</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>len_segment : int\n    the segment length</p>\n", "signature": "(self) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.set_ssl_transformations", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.set_ssl_transformations", "type": "function", "doc": "<p>Set new SSL transformations</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>ssl_transformations : list\n    a list of functions that take a sample feature dictionary as input and output ssl_inputs and ssl_targets\n    lists</p>\n", "signature": "(self, ssl_transformations: List) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.new", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.new", "type": "function", "doc": "<p>Create a new object of the same class</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>new_instance: BehaviorDataset\n    a new instance of the same class</p>\n", "signature": "(cls, *args, **kwargs)", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_parameters", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_parameters", "type": "function", "doc": "<p>Get parameters necessary for initialization</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_type : str\n    the data type\nannotation_type : str\n    the annotation type</p>\n", "signature": "(cls, data_type: str, annotation_type: str) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.data_types", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.data_types", "type": "function", "doc": "<p>List available data types</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>data_types : list\n    available data types</p>\n", "signature": "() -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.annotation_types", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.annotation_types", "type": "function", "doc": "<p>List available annotation types</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>annotation_types : list\n    available annotation types</p>\n", "signature": "() -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.set_indexing_parameters", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.set_indexing_parameters", "type": "function", "doc": "<p>Set the parameters that change the subset that is returned at <code>__getitem__</code></p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>unlabeled : bool\n    a pseudolabeling parameter; return only unlabeled samples if <code>True</code>, only labeled if <code>False</code> and\n    all if <code>None</code>\ntag : int\n    if not <code>None</code>, only samples with this meta tag will be returned</p>\n", "signature": "(self, unlabeled: bool, tag: int) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_intervals", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_intervals", "type": "function", "doc": "<p>Get a list of intervals covered by the dataset in the original coordinates</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>intervals : dict\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and\n    values are lists of the intervals in <code>[start, end]</code> format</p>\n", "signature": "(self) -> Tuple[dict, Optional[list]]", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_unannotated_intervals", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_unannotated_intervals", "type": "function", "doc": "<p>Get a list of intervals in the original coordinates where there is no annotation</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>intervals : dict\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and\n    values are lists of the intervals in <code>[start, end]</code> format</p>\n", "signature": "(self, first_intervals=None) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_annotated_intervals", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_annotated_intervals", "type": "function", "doc": "<p>Get a list of intervals in the original coordinates where there is no annotation</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>intervals : dict\n    a nested dictionary where first-level keys are video ids, second-level keys are clip ids and\n    values are lists of the intervals in <code>[start, end]</code> format</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_ids", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_ids", "type": "function", "doc": "<p>Get a dictionary of all clip ids in the dataset</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ids : dict\n    a dictionary where keys are video ids and values are lists of clip ids</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_len", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_len", "type": "function", "doc": "<p>Get the length of a specific clip</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_id : str\n    the video id\nclip_id : str\n    the clip id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>length : int\n    the length</p>\n", "signature": "(self, video_id: str, clip_id: str) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_confusion_matrix", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_confusion_matrix", "type": "function", "doc": "<p>Get a confusion matrix</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>prediction : torch.Tensor\n    a tensor of predicted class probabilities of shape <code>(#samples, #classes, #frames)</code>\nconfusion_type : {\"recall\", \"precision\"}\n    for datasets with non-exclusive annotation, if <code>type</code> is <code>\"recall\"</code>, only false positives are taken\n    into account, and if <code>type</code> is <code>\"precision\"</code>, only false negatives</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>confusion_matrix : np.ndarray\n    a confusion matrix of shape <code>(#classes, #classes)</code> where <code>A[i, j] = F_ij/N_i</code>, <code>F_ij</code> is the number of\n    frames that have the i-th label in the ground truth and a false positive j-th label in the prediction,\n    <code>N_i</code> is the number of frames that have the i-th label in the ground truth\nclasses : list\n    a list of classes</p>\n", "signature": "(\n    self,\n    prediction: torch.Tensor,\n    confusion_type: str = 'recall'\n) -> Tuple[numpy.ndarray, list]", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.annotation_class", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.annotation_class", "type": "function", "doc": "<p>Get the type of annotation ('exclusive_classification', 'nonexclusive_classification', more coming soon)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>annotation_class : str\n    the type of annotation</p>\n", "signature": "(self) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.set_normalization_stats", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.set_normalization_stats", "type": "function", "doc": "<p>Set the stats to normalize data at runtime</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>stats : dict\n    a nested dictionary where first-level keys are feature key names, second-level keys are 'mean' and 'std'\n    and values are the statistics in <code>torch</code> tensors of shape <code>(#features, 1)</code></p>\n", "signature": "(self, stats: Dict) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_min_max_frames", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_min_max_frames", "type": "function", "doc": "<p></p>\n", "signature": "(self, video_id) -> Tuple[Dict, Dict]", "funcdef": "def"}, {"fullname": "dlc2action.data.dataset.BehaviorDataset.get_normalization_stats", "modulename": "dlc2action.data.dataset", "qualname": "BehaviorDataset.get_normalization_stats", "type": "function", "doc": "<p>Get mean and standard deviation for each key</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>stats : dict\n    a nested dictionary where first-level keys are feature key names, second-level keys are 'mean' and 'std'\n    and values are the statistics in <code>torch</code> tensors of shape <code>(#features, 1)</code></p>\n", "signature": "(self, skip_keys=None) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store", "modulename": "dlc2action.data.input_store", "type": "module", "doc": "<p>Specific realisations of <code>dlc2action.data.base_store.InputStore</code> are defined here</p>\n"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore", "type": "class", "doc": "<p>A generalized realization of a PoseInputStore</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>data_path\n\u251c\u2500\u2500 video1DLC1000.pickle\n\u251c\u2500\u2500 video2DLC400.pickle\n\u251c\u2500\u2500 video1_features.pt\n\u2514\u2500\u2500 video2_features.pt\n</code></pre>\n\n<p>Here <code>data_suffix</code> is <code>{'DLC1000.pickle', 'DLC400.pickle'}</code> and <code>feature_suffix</code> (optional) is <code>'_features.pt'</code>.</p>\n", "bases": "dlc2action.data.base_store.PoseInputStore"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.__init__", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects\ndata_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)\ndata_suffix : str | set, optional\n    the suffix or the set of suffices such that the pose files are named {video_id}{data_suffix}\n    (not passed if creating from key objects or if irrelevant for the dataset)\ndata_prefix : str | set, optional\n    the prefix or the set of prefixes such that the pose files for different video views of the same\n    clip are named {prefix}{sep}{video_id}{data_suffix} (not passed if creating from key objects\n    or if irrelevant for the dataset)\nfeature_suffix : str | set, optional\n    the suffix or the set of suffices such that the additional feature files are named\n    {video_id}{feature_suffix} (and placed at the data_path folder)\nconvert_int_indices : bool, default True\n    if <code>True</code>, convert any integer key <code>i</code> in feature files to <code>'ind{i}'</code>\nfeature_save_path : str, optional\n    the path to the folder where pre-processed files are stored (not passed if creating from key objects)\ncanvas_shape : List, default [1, 1]\n    the canvas size where the pose is defined\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nfeature_extraction : str, default 'kinematic'\n    the feature extraction method (see options.feature_extractors for available options)\nignored_clips : list, optional\n    list of strings of clip ids to ignore\nignored_bodyparts : list, optional\n    list of strings of bodypart names to ignore\ndefault_agent_name : str, default 'ind0'\n    the agent name used as default in the pose files for a single agent\nkey_objects : tuple, optional\n    a tuple of key objects\nlikelihood_threshold : float, default 0\n    coordinate values with likelihoods less than this value will be set to 'unknown'\nnum_cpus : int, optional\n    the number of cpus to use in data processing\nframe_limit : int, default 1\n    clips shorter than this number of frames will be ignored\nfeature_extraction_pars : dict, optional\n    parameters of the feature extractor</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    data_path: Union[Set, str] = None,\n    file_paths: Set = None,\n    data_suffix: Union[Set, str] = None,\n    data_prefix: Union[Set, str] = None,\n    feature_suffix: str = None,\n    convert_int_indices: bool = True,\n    feature_save_path: str = None,\n    canvas_shape: List = None,\n    len_segment: int = 128,\n    overlap: int = 0,\n    feature_extraction: str = 'kinematic',\n    ignored_clips: List = None,\n    ignored_bodyparts: List = None,\n    default_agent_name: str = 'ind0',\n    key_objects: Tuple = None,\n    likelihood_threshold: float = 0,\n    num_cpus: int = None,\n    frame_limit: int = 1,\n    normalize: bool = False,\n    feature_extraction_pars: Dict = None,\n    centered: bool = False,\n    transpose_features: bool = False,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.data_suffix", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.data_suffix", "type": "variable", "doc": "<p></p>\n", "default_value": " = None"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_folder", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_folder", "type": "function", "doc": "<p>Get the input folder that the file with this video id was read from</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_id : str\n    the video id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>folder : str\n    the path to the directory that contains the input file associated with the video id</p>\n", "signature": "(self, video_id: str) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.remove", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.remove", "type": "function", "doc": "<p>Remove the samples corresponding to indices</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>indices : int\n    a list of integer indices to remove</p>\n", "signature": "(self, indices: List) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.key_objects", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.key_objects", "type": "function", "doc": "<p>Return a tuple of the key objects necessary to re-create the Store</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>key_objects : tuple\n    a tuple of key objects</p>\n", "signature": "(self) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.load_from_key_objects", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.load_from_key_objects", "type": "function", "doc": "<p>Load the information from a tuple of key objects</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>key_objects : tuple\n    a tuple of key objects</p>\n", "signature": "(self, key_objects: Tuple) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.to_ram", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.to_ram", "type": "function", "doc": "<p>Transfer the data samples to RAM if they were previously stored as file paths</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_original_coordinates", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_original_coordinates", "type": "function", "doc": "<p>Return the original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>np.ndarray\n    an array that contains the coordinates of the data samples in original input data (video id, clip id,\n    start frame)</p>\n", "signature": "(self) -> numpy.ndarray", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.create_subsample", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.create_subsample", "type": "function", "doc": "<p>Create a new store that contains a subsample of the data</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>indices : list\n    the indices to be included in the subsample\nssl_indices : list, optional\n    the indices to be included in the subsample without the annotation data</p>\n", "signature": "(self, indices: List, ssl_indices: List = None)", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_video_id", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_video_id", "type": "function", "doc": "<p>Get the video id from an element of original coordinates</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>coords : tuple\n    an element of the original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_id: str\n    the id of the video that the coordinates point to</p>\n", "signature": "(self, coords: Tuple) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_clip_id", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_clip_id", "type": "function", "doc": "<p>Get the clip id from an element of original coordinates</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>coords : tuple\n    an element of the original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>clip_id : str\n    the id of the clip that the coordinates point to</p>\n", "signature": "(self, coords: Tuple) -> str", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_clip_length", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_clip_length", "type": "function", "doc": "<p>Get the clip length from the id</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_id : str\n    the video id\nclip_id : str\n    the clip id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>clip_length : int\n    the length of the clip</p>\n", "signature": "(self, video_id: str, clip_id: str) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_clip_start_end", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_clip_start_end", "type": "function", "doc": "<p>Get the clip start and end frames from an element of original coordinates</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>coords : tuple\n    an element of original coordinates array</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>start : int\n    the start frame of the clip that the coordinates point to\nend : int\n    the end frame of the clip that the coordinates point to</p>\n", "signature": "(self, coords: Tuple) -> Tuple[int, int]", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_clip_start", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_clip_start", "type": "function", "doc": "<p>Get the clip start frame from the video id and the clip id</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_name : str\n    the video id\nclip_id : str\n    the clip id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>clip_start : int\n    the start frame of the clip</p>\n", "signature": "(self, video_name: str, clip_id: str) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_visibility", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_visibility", "type": "function", "doc": "<p>Get the fraction of the frames in that have a visibility score better than a hard_threshold</p>\n\n<p>For example, in the case of keypoint data the visibility score can be the number of identified keypoints.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_id : str\n    the video id of the frames\nclip_id : str\n    the clip id of the frames\nstart : int\n    the start frame\nend : int\n    the end frame\nscore : float\n    the visibility score hard_threshold</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>frac_visible: float\n    the fraction of frames with visibility above the hard_threshold</p>\n", "signature": "(\n    self,\n    video_id: str,\n    clip_id: str,\n    start: int,\n    end: int,\n    score: int\n) -> float", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_annotation_objects", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_annotation_objects", "type": "function", "doc": "<p>Get a dictionary of objects necessary to create an AnnotationStore</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>annotation_objects : dict\n    a dictionary of objects to be passed to the AnnotationStore constructor where the keys are the names of\n    the objects</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_file_ids", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_suffix : set | str, optional\n    the suffix (or a set of suffixes) of the input data files\ndata_path : set | str, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\ndata_prefix : set | str, optional\n    the prefix or the set of prefixes such that the pose files for different video views of the same\n    clip are named {prefix}{sep}{video_id}{data_suffix} (not passed if creating from key objects\n    or if irrelevant for the dataset)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\nfeature_suffix : str | set, optional\n    the suffix or the set of suffices such that the additional feature files are named\n    {video_id}{feature_suffix} (and placed at the <code>data_path</code> folder or at <code>file_paths</code>)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(\n    cls,\n    data_suffix: Union[Set, str] = None,\n    data_path: Union[Set, str] = None,\n    data_prefix: Union[Set, str] = None,\n    file_paths: Set = None,\n    feature_suffix: Set = None,\n    *args,\n    **kwargs\n) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_bodyparts", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_bodyparts", "type": "function", "doc": "<p>Get a list of bodypart names</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_dict : dict\n    the data dictionary (passed to feature extractor)\nclip_id : str\n    the clip id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>bodyparts : list\n    a list of string or integer body part names</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_coords", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_coords", "type": "function", "doc": "<p>Get the coordinates array of a specific bodypart in a specific clip</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_dict : dict\n    the data dictionary (passed to feature extractor)\nclip_id : str\n    the clip id\nbodypart : str\n    the name of the body part</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>coords : np.ndarray\n    the coordinates array of shape (#timesteps, #coordinates)</p>\n", "signature": "(self, data_dict: Dict, clip_id: str, bodypart: str) -> numpy.ndarray", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_n_frames", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_n_frames", "type": "function", "doc": "<p>Get the length of the clip</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_dict : dict\n    the data dictionary (passed to feature extractor)\nclip_id : str\n    the clip id</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>n_frames : int\n    the length of the clip</p>\n", "signature": "(self, data_dict: Dict, clip_id: str) -> int", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_likelihood", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_likelihood", "type": "function", "doc": "<p>Get the likelihood values</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_dict : dict\n    the data dictionary\nclip_id : str\n    the clip id\nbodypart : str\n    the name of the body part</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>likelihoods: np.ndarrray | None\n    <code>None</code> if the dataset doesn't have likelihoods or an array of shape (#timestamps)</p>\n", "signature": "(\n    self,\n    data_dict: Dict,\n    clip_id: str,\n    bodypart: str\n) -> Optional[numpy.ndarray]", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_indices", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_indices", "type": "function", "doc": "<p>Get a list of indices of samples that have a specific meta tag</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tag : int\n    the meta tag for the subsample (<code>None</code> for the whole dataset)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>indices : list\n    a list of indices that meet the criteria</p>\n", "signature": "(self, tag: int) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_tags", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_tags", "type": "function", "doc": "<p>Get a list of all meta tags</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tags: List\n    a list of unique meta tag values</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.GeneralInputStore.get_tag", "modulename": "dlc2action.data.input_store", "qualname": "GeneralInputStore.get_tag", "type": "function", "doc": "<p>Return a tag object corresponding to an index</p>\n\n<p>Tags can carry meta information (like annotator id) and are accepted by models that require\nthat information. When a tag is <code>None</code>, it is not passed to the model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>idx : int\n    the index</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tag : int\n    the tag object</p>\n", "signature": "(self, idx: int) -> Optional[int]", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.FileInputStore", "modulename": "dlc2action.data.input_store", "qualname": "FileInputStore", "type": "class", "doc": "<p>An implementation of <code>dlc2action.data.InputStore</code> for datasets where each input data file corresponds to one video</p>\n", "bases": "GeneralInputStore"}, {"fullname": "dlc2action.data.input_store.SequenceInputStore", "modulename": "dlc2action.data.input_store", "qualname": "SequenceInputStore", "type": "class", "doc": "<p>An implementation of <code>dlc2action.data.InputStore</code> for datasets where input data files correspond to multiple videos</p>\n", "bases": "GeneralInputStore"}, {"fullname": "dlc2action.data.input_store.SequenceInputStore.get_file_ids", "modulename": "dlc2action.data.input_store", "qualname": "SequenceInputStore.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filenames : set, optional\n    a set of string filenames to search for (only basenames, not the whole paths)\ndata_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(\n    cls,\n    filenames: Set = None,\n    data_path: Union[str, Set] = None,\n    file_paths: Set = None,\n    *args,\n    **kwargs\n) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.DLCTrackStore", "modulename": "dlc2action.data.input_store", "qualname": "DLCTrackStore", "type": "class", "doc": "<p>DLC track data</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>data_path\n\u251c\u2500\u2500 video1DLC1000.pickle\n\u251c\u2500\u2500 video2DLC400.pickle\n\u251c\u2500\u2500 video1_features.pt\n\u2514\u2500\u2500 video2_features.pt\n</code></pre>\n\n<p>Here <code>data_suffix</code> is <code>{'DLC1000.pickle', 'DLC400.pickle'}</code> and <code>feature_suffix</code> (optional) is <code>'_features.pt'</code>.</p>\n\n<p>The feature files should to be dictionaries where keys are clip IDs (e.g. animal names) and values are\nfeature values (arrays of shape <code>(#frames, #features)</code>). If the arrays are shaped as <code>(#features, #frames)</code>,\nset <code>transpose_features</code> to <code>True</code>.</p>\n\n<p>The files can be saved with <code>numpy.save()</code> (with <code>.npy</code> extension), <code>torch.save()</code> (with <code>.pt</code> extension) or\nwith <code>pickle.dump()</code> (with <code>.pickle</code> or <code>.pkl</code> extension).</p>\n", "bases": "FileInputStore"}, {"fullname": "dlc2action.data.input_store.DLCTrackletStore", "modulename": "dlc2action.data.input_store", "qualname": "DLCTrackletStore", "type": "class", "doc": "<p>DLC tracklet data</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>data_path\n\u251c\u2500\u2500 video1DLC1000.pickle\n\u251c\u2500\u2500 video2DLC400.pickle\n\u251c\u2500\u2500 video1_features.pt\n\u2514\u2500\u2500 video2_features.pt\n</code></pre>\n\n<p>Here <code>data_suffix</code> is <code>{'DLC1000.pickle', 'DLC400.pickle'}</code> and <code>feature_suffix</code> (optional) is <code>'_features.pt'</code>.</p>\n\n<p>The feature files should to be dictionaries where keys are clip IDs (e.g. animal names) and values are\nfeature values (arrays of shape <code>(#frames, #features)</code>). If the arrays are shaped as <code>(#features, #frames)</code>,\nset <code>transpose_features</code> to <code>True</code>.</p>\n\n<p>The files can be saved with <code>numpy.save()</code> (with <code>.npy</code> extension), <code>torch.save()</code> (with <code>.pt</code> extension) or\nwith <code>pickle.dump()</code> (with <code>.pickle</code> or <code>.pkl</code> extension).</p>\n", "bases": "FileInputStore"}, {"fullname": "dlc2action.data.input_store.PKUMMDInputStore", "modulename": "dlc2action.data.input_store", "qualname": "PKUMMDInputStore", "type": "class", "doc": "<p>PKU-MMD data</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>data_path\n\u251c\u2500\u2500 0073-R.txt\n...\n\u2514\u2500\u2500 0274-L.txt\n</code></pre>\n", "bases": "FileInputStore"}, {"fullname": "dlc2action.data.input_store.PKUMMDInputStore.__init__", "modulename": "dlc2action.data.input_store", "qualname": "PKUMMDInputStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\ndata_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)\nfeature_save_path : str, optional\n    the path to the folder where pre-processed files are stored (not passed if creating from key objects)\nfeature_extraction : str, default 'kinematic'\n    the feature extraction method (run options.feature_extractors to see available options)\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\ninteractive : bool, default False\n    if True, distances between two agents are included; if False, only the first agent features are computed\nkey_objects : tuple, optional\n    a tuple of key objects\nnum_cpus : int, optional\n    the number of cpus to use in data processing\nfeature_extraction_pars : dict, optional\n    parameters of the feature extractor</p>\n", "signature": "(\n    self,\n    video_order: str = None,\n    data_path: Union[str, Set] = None,\n    file_paths: Set = None,\n    feature_save_path: str = None,\n    feature_extraction: str = 'kinematic',\n    len_segment: int = 128,\n    overlap: int = 0,\n    key_objects: Tuple = None,\n    num_cpus: int = None,\n    interactive: bool = False,\n    feature_extraction_pars: Dict = None,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.PKUMMDInputStore.data_suffix", "modulename": "dlc2action.data.input_store", "qualname": "PKUMMDInputStore.data_suffix", "type": "variable", "doc": "<p></p>\n", "default_value": " = '.txt'"}, {"fullname": "dlc2action.data.input_store.CalMS21InputStore", "modulename": "dlc2action.data.input_store", "qualname": "CalMS21InputStore", "type": "class", "doc": "<p>CalMS21 data</p>\n\n<p>Use the <code>'random:test_from_name:{name}'</code> and <code>'val-from-name:{val_name}:test-from-name:{test_name}'</code>\npartitioning methods with <code>'train'</code>, <code>'test'</code> and <code>'unlabeled'</code> names to separate into train, test and validation\nsubsets according to the original files. For example, with <code>'val-from-name:test:test-from-name:unlabeled'</code>\nthe data from the test file will go into validation and the unlabeled files will be the test.</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>data_path\n\u251c\u2500\u2500 calms21_task1_train.npy\n\u251c\u2500\u2500 calms21_task1_test.npy\n\u251c\u2500\u2500 calms21_task1_test_features.npy\n\u251c\u2500\u2500 calms21_task1_test_features.npy\n\u251c\u2500\u2500 calms21_unlabeled_videos_part1.npy\n\u251c\u2500\u2500 calms21_unlabeled_videos_part1.npy\n\u251c\u2500\u2500 calms21_unlabeled_videos_part2.npy\n\u2514\u2500\u2500 calms21_unlabeled_videos_part3.npy\n</code></pre>\n", "bases": "SequenceInputStore"}, {"fullname": "dlc2action.data.input_store.CalMS21InputStore.__init__", "modulename": "dlc2action.data.input_store", "qualname": "CalMS21InputStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects)\ndata_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)\ntask_n : [1, 2]\n    the number of the task\ninclude_task1 : bool, default True\n    include task 1 data to training set\nfeature_save_path : str, optional\n    the path to the folder where pre-processed files are stored (not passed if creating from key objects)\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nfeature_extraction : str, default 'kinematic'\n    the feature extraction method (see options.feature_extractors for available options)\nignored_bodyparts : list, optional\n    list of strings of bodypart names to ignore\nkey_objects : tuple, optional\n    a tuple of key objects\ntreba_files : bool, default False\n    if <code>True</code>, TREBA feature files will be loaded\nnum_cpus : int, optional\n    the number of cpus to use in data processing\nfeature_extraction_pars : dict, optional\n    parameters of the feature extractor</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    data_path: Union[Set, str] = None,\n    file_paths: Set = None,\n    task_n: int = 1,\n    include_task1: bool = True,\n    feature_save_path: str = None,\n    len_segment: int = 128,\n    overlap: int = 0,\n    feature_extraction: str = 'kinematic',\n    key_objects: Dict = None,\n    treba_files: bool = False,\n    num_cpus: int = None,\n    feature_extraction_pars: Dict = None,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.CalMS21InputStore.get_file_ids", "modulename": "dlc2action.data.input_store", "qualname": "CalMS21InputStore.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>task_n : {1, 2, 3}\n    the index of the CalMS21 challenge task\ninclude_task1 : bool, default False\n    if <code>True</code>, the training file of the task 1 will be loaded\ntreba_files : bool, default False\n    if <code>True</code>, the TREBA feature files will be loaded\nfilenames : set, optional\n    a set of string filenames to search for (only basenames, not the whole paths)\ndata_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(\n    cls,\n    task_n: int = 1,\n    include_task1: bool = False,\n    treba_files: bool = False,\n    data_path: Union[str, Set] = None,\n    file_paths=None,\n    *args,\n    **kwargs\n) -> Iterable", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.Numpy3DInputStore", "modulename": "dlc2action.data.input_store", "qualname": "Numpy3DInputStore", "type": "class", "doc": "<p>3D data</p>\n\n<p>Assumes the data files to be <code>numpy</code> arrays saved in <code>.npy</code> format with shape <code>(#frames, #keypoints, 3)</code>.</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>data_path\n\u251c\u2500\u2500 video1_suffix1.npy\n\u251c\u2500\u2500 video2_suffix2.npy\n\u251c\u2500\u2500 video1_features.pt\n\u2514\u2500\u2500 video2_features.pt\n</code></pre>\n\n<p>Here <code>data_suffix</code> is <code>{'_suffix1.npy', '_suffix1.npy'}</code> and <code>feature_suffix</code> (optional) is <code>'_features.pt'</code>.</p>\n\n<p>The feature files should to be dictionaries where keys are clip IDs (e.g. animal names) and values are\nfeature values (arrays of shape <code>(#frames, #features)</code>). If the arrays are shaped as <code>(#features, #frames)</code>,\nset <code>transpose_features</code> to <code>True</code>.</p>\n\n<p>The files can be saved with <code>numpy.save()</code> (with <code>.npy</code> extension), <code>torch.save()</code> (with <code>.pt</code> extension) or\nwith <code>pickle.dump()</code> (with <code>.pickle</code> or <code>.pkl</code> extension).</p>\n", "bases": "FileInputStore"}, {"fullname": "dlc2action.data.input_store.Numpy3DInputStore.__init__", "modulename": "dlc2action.data.input_store", "qualname": "Numpy3DInputStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects\ndata_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)\ndata_suffix : str | set, optional\n    the suffix or the set of suffices such that the pose files are named {video_id}{data_suffix}\n    (not passed if creating from key objects or if irrelevant for the dataset)\ndata_prefix : str | set, optional\n    the prefix or the set of prefixes such that the pose files for different video views of the same\n    clip are named {prefix}{sep}{video_id}{data_suffix} (not passed if creating from key objects\n    or if irrelevant for the dataset)\nfeature_suffix : str | set, optional\n    the suffix or the set of suffices such that the additional feature files are named\n    {video_id}{feature_suffix} (and placed at the data_path folder)\nconvert_int_indices : bool, default True\n    if <code>True</code>, convert any integer key <code>i</code> in feature files to <code>'ind{i}'</code>\nfeature_save_path : str, optional\n    the path to the folder where pre-processed files are stored (not passed if creating from key objects)\ncanvas_shape : List, default [1, 1]\n    the canvas size where the pose is defined\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nfeature_extraction : str, default 'kinematic'\n    the feature extraction method (see options.feature_extractors for available options)\nignored_clips : list, optional\n    list of strings of clip ids to ignore\nignored_bodyparts : list, optional\n    list of strings of bodypart names to ignore\ndefault_agent_name : str, default 'ind0'\n    the agent name used as default in the pose files for a single agent\nkey_objects : tuple, optional\n    a tuple of key objects\nlikelihood_threshold : float, default 0\n    coordinate values with likelihoods less than this value will be set to 'unknown'\nnum_cpus : int, optional\n    the number of cpus to use in data processing\nframe_limit : int, default 1\n    clips shorter than this number of frames will be ignored\nfeature_extraction_pars : dict, optional\n    parameters of the feature extractor</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    data_path: Union[Set, str] = None,\n    file_paths: Set = None,\n    data_suffix: Union[Set, str] = None,\n    data_prefix: Union[Set, str] = None,\n    feature_suffix: Union[Set, str] = None,\n    convert_int_indices: bool = True,\n    feature_save_path: str = None,\n    canvas_shape: List = None,\n    len_segment: int = 128,\n    overlap: int = 0,\n    feature_extraction: str = 'kinematic',\n    ignored_clips: List = None,\n    ignored_bodyparts: List = None,\n    default_agent_name: str = 'ind0',\n    key_objects: Dict = None,\n    likelihood_threshold: float = 0,\n    num_cpus: int = None,\n    frame_limit: int = 1,\n    feature_extraction_pars: Dict = None,\n    centered: bool = False,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.LoadedFeaturesInputStore", "modulename": "dlc2action.data.input_store", "qualname": "LoadedFeaturesInputStore", "type": "class", "doc": "<p>Non-pose feature files</p>\n\n<p>The feature files should to be dictionaries where keys are clip IDs (e.g. animal names) and values are\nfeature values (arrays of shape <code>(#frames, #features)</code>). If the arrays are shaped as <code>(#features, #frames)</code>,\nset <code>transpose_features</code> to <code>True</code>.</p>\n\n<p>The files can be saved with <code>numpy.save()</code> (with <code>.npy</code> extension), <code>torch.save()</code> (with <code>.pt</code> extension) or\nwith <code>pickle.dump()</code> (with <code>.pickle</code> or <code>.pkl</code> extension).</p>\n\n<p>Assumes the following file structure:</p>\n\n<pre><code>data_path\n\u251c\u2500\u2500 video1_features.pt\n\u2514\u2500\u2500 video2_features.pt\n</code></pre>\n\n<p>Here <code>feature_suffix</code> (optional) is <code>'_features.pt'</code>.</p>\n", "bases": "GeneralInputStore"}, {"fullname": "dlc2action.data.input_store.LoadedFeaturesInputStore.__init__", "modulename": "dlc2action.data.input_store", "qualname": "LoadedFeaturesInputStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects\ndata_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)\nfeature_suffix : str | set, optional\n    the suffix or the set of suffices such that the additional feature files are named\n    {video_id}{feature_suffix} (and placed at the data_path folder)\nfeature_save_path : str, optional\n    the path to the folder where pre-processed files are stored (not passed if creating from key objects)\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nignored_clips : list, optional\n    list of strings of clip ids to ignore\ndefault_agent_name : str, default 'ind0'\n    the agent name used as default in the pose files for a single agent\nkey_objects : tuple, optional\n    a tuple of key objects\nnum_cpus : int, optional\n    the number of cpus to use in data processing\nframe_limit : int, default 1\n    clips shorter than this number of frames will be ignored\nfeature_extraction_pars : dict, optional\n    parameters of the feature extractor</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    data_path: Union[Set, str] = None,\n    file_paths: Set = None,\n    feature_suffix: Union[Set, str] = None,\n    convert_int_indices: bool = True,\n    feature_save_path: str = None,\n    len_segment: int = 128,\n    overlap: int = 0,\n    ignored_clips: List = None,\n    key_objects: Dict = None,\n    num_cpus: int = None,\n    frame_limit: int = 1,\n    transpose_features: bool = False,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.LoadedFeaturesInputStore.get_visibility", "modulename": "dlc2action.data.input_store", "qualname": "LoadedFeaturesInputStore.get_visibility", "type": "function", "doc": "<p>Get the fraction of the frames in that have a visibility score better than a hard_threshold</p>\n\n<p>For example, in the case of keypoint data the visibility score can be the number of identified keypoints.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_id : str\n    the video id of the frames\nclip_id : str\n    the clip id of the frames\nstart : int\n    the start frame\nend : int\n    the end frame\nscore : float\n    the visibility score hard_threshold</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>frac_visible: float\n    the fraction of frames with visibility above the hard_threshold</p>\n", "signature": "(\n    self,\n    video_id: str,\n    clip_id: str,\n    start: int,\n    end: int,\n    score: int\n) -> float", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.LoadedFeaturesInputStore.get_file_ids", "modulename": "dlc2action.data.input_store", "qualname": "LoadedFeaturesInputStore.get_file_ids", "type": "function", "doc": "<p>Process data parameters and return a list of ids  of the videos that should\nbe processed by the __init__ function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_suffix : set | str, optional\n    the suffix (or a set of suffixes) of the input data files\ndata_path : set | str, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\ndata_prefix : set | str, optional\n    the prefix or the set of prefixes such that the pose files for different video views of the same\n    clip are named {prefix}{sep}{video_id}{data_suffix} (not passed if creating from key objects\n    or if irrelevant for the dataset)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\nfeature_suffix : str | set, optional\n    the suffix or the set of suffices such that the additional feature files are named\n    {video_id}{feature_suffix} (and placed at the <code>data_path</code> folder or at <code>file_paths</code>)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>video_ids : list\n    a list of video file ids</p>\n", "signature": "(\n    cls,\n    data_path: Union[Set, str] = None,\n    file_paths: Set = None,\n    feature_suffix: Set = None,\n    *args,\n    **kwargs\n) -> List", "funcdef": "def"}, {"fullname": "dlc2action.data.input_store.SIMBAInputStore", "modulename": "dlc2action.data.input_store", "qualname": "SIMBAInputStore", "type": "class", "doc": "<p>SIMBA paper format data</p>\n\n<p>Assumes the following file structure</p>\n\n<p><code>\n data_path\n \u251c\u2500\u2500 Video1.csv\n ...\n \u2514\u2500\u2500 Video9.csv\n</code>\n Here <code>data_suffix</code> is <code>.csv</code>.</p>\n", "bases": "FileInputStore"}, {"fullname": "dlc2action.data.input_store.SIMBAInputStore.__init__", "modulename": "dlc2action.data.input_store", "qualname": "SIMBAInputStore.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>video_order : list, optional\n    a list of video ids that should be processed in the same order (not passed if creating from key objects\ndata_path : str | set, optional\n    the path to the folder where the pose and feature files are stored or a set of such paths\n    (not passed if creating from key objects or from <code>file_paths</code>)\nfile_paths : set, optional\n    a set of string paths to the pose and feature files\n    (not passed if creating from key objects or from <code>data_path</code>)\ndata_suffix : str | set, optional\n    the suffix or the set of suffices such that the pose files are named {video_id}{data_suffix}\n    (not passed if creating from key objects or if irrelevant for the dataset)\ndata_prefix : str | set, optional\n    the prefix or the set of prefixes such that the pose files for different video views of the same\n    clip are named {prefix}{sep}{video_id}{data_suffix} (not passed if creating from key objects\n    or if irrelevant for the dataset)\nfeature_suffix : str | set, optional\n    the suffix or the set of suffices such that the additional feature files are named\n    {video_id}{feature_suffix} (and placed at the data_path folder)\nfeature_save_path : str, optional\n    the path to the folder where pre-processed files are stored (not passed if creating from key objects)\ncanvas_shape : List, default [1, 1]\n    the canvas size where the pose is defined\nlen_segment : int, default 128\n    the length of the segments in which the data should be cut (in frames)\noverlap : int, default 0\n    the length of the overlap between neighboring segments (in frames)\nfeature_extraction : str, default 'kinematic'\n    the feature extraction method (see options.feature_extractors for available options)\nignored_clips : list, optional\n    list of strings of clip ids to ignore\nignored_bodyparts : list, optional\n    list of strings of bodypart names to ignore\nkey_objects : tuple, optional\n    a tuple of key objects\nlikelihood_threshold : float, default 0\n    coordinate values with likelihoods less than this value will be set to 'unknown'\nnum_cpus : int, optional\n    the number of cpus to use in data processing\nfeature_extraction_pars : dict, optional\n    parameters of the feature extractor</p>\n", "signature": "(\n    self,\n    video_order: List = None,\n    data_path: Union[Set, str] = None,\n    file_paths: Set = None,\n    data_prefix: Union[Set, str] = None,\n    feature_suffix: str = None,\n    feature_save_path: str = None,\n    canvas_shape: List = None,\n    len_segment: int = 128,\n    overlap: int = 0,\n    feature_extraction: str = 'kinematic',\n    ignored_clips: List = None,\n    ignored_bodyparts: List = None,\n    key_objects: Tuple = None,\n    likelihood_threshold: float = 0,\n    num_cpus: int = None,\n    normalize: bool = False,\n    feature_extraction_pars: Dict = None,\n    centered: bool = False,\n    data_suffix: str = None,\n    use_features: bool = False,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.feature_extraction", "modulename": "dlc2action.feature_extraction", "type": "module", "doc": "<h2 id=\"feature-extraction\">Feature extraction</h2>\n\n<p>Feature extractors generate feature dictionaries that are then passed to SSL transformations\n(see <code>dlc2action.ssl</code>) and finally to\ntransformers that perform augmentations and merge all features into a tensor (see <code>dlc2action.transformer</code>).\nThe keys of the dictionaries are the feature names (<code>'coords'</code>, <code>'speeds'</code> and so on) and the values are the\nfeature tensors. It is generally assumed that the tensors have shape <code>(F, ..., L)</code> where <code>F</code> is the variable\nnumber of features (per frame, keypoint, pixel...) and <code>L</code> is the length of the segment in frames. The <code>F</code>\nvalue can be different for every tensor in the dictionary and the rest of the shape should be constant.</p>\n"}, {"fullname": "dlc2action.feature_extraction.FeatureExtractor", "modulename": "dlc2action.feature_extraction", "qualname": "FeatureExtractor", "type": "class", "doc": "<p>The base class for feature extractors</p>\n\n<p>The <code>extract_features</code> method receives a data dictionary as input.\nWe do not assume a specific\nstructure in the values and all necessary information (coordinates of a bodypart, number\nof frames, list of bodyparts) is inferred using input store methods. Therefore, each child class\nof <code>FeatureExtractor</code> is written for a specific subclass of <code>dlc2action.data.base_Store.InputStore</code>\nwith the data inference\nfunctions defined (i.e. <code>dlc2action.data.base_store.PoseInputStore</code>).</p>\n", "bases": "abc.ABC"}, {"fullname": "dlc2action.feature_extraction.FeatureExtractor.__init__", "modulename": "dlc2action.feature_extraction", "qualname": "FeatureExtractor.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignored_clips : list\n    a list of string names of clip ids to ignore</p>\n", "signature": "(self, ignored_clips: List = None, **kwargs)", "funcdef": "def"}, {"fullname": "dlc2action.feature_extraction.FeatureExtractor.input_store_class", "modulename": "dlc2action.feature_extraction", "qualname": "FeatureExtractor.input_store_class", "type": "variable", "doc": "<p>The <code>dlc2action.data.base_Store.InputStore</code> child class paired with this feature extractor</p>\n", "default_value": " = None"}, {"fullname": "dlc2action.feature_extraction.FeatureExtractor.extract_features", "modulename": "dlc2action.feature_extraction", "qualname": "FeatureExtractor.extract_features", "type": "function", "doc": "<p>Extract features from a data dictionary</p>\n\n<p>An input store will call this method while pre-computing a dataset. The data dictionary has to relate to one\nvideo id and have clip ids as keys. Read the documentation at <code>dlc2action.data</code> to find out more about video\nand clip ids. We do not assume a specific\nstructure in the values, so all necessary information (coordinates of a bodypart, number\nof frames, list of bodyparts) is inferred using input store methods.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_dict : dict\n    the data dictionary\nvideo_id : str\n    the id of the video associated with the data dictionary\none_clip : bool, default False\n    if <code>True</code>, all features will be concatenated and assigned to one clip named <code>'all'</code></p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>features : dict\n    a features dictionary where the keys are the feature names (e.g. 'coords', 'distances') and the\n    values are numpy arrays of shape <code>(#features, ..., #frames)</code></p>\n", "signature": "(self, data_dict: Dict, video_id: str, one_clip: bool = False) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.feature_extraction.PoseFeatureExtractor", "modulename": "dlc2action.feature_extraction", "qualname": "PoseFeatureExtractor", "type": "class", "doc": "<p>The base class for pose feature extractors</p>\n\n<p>Pose feature extractors work with <code>dlc2action.data.base_store.InputStore</code> instances\nthat inherit from <code>dlc2action.data.base_store.PoseInputStore</code>.</p>\n", "bases": "FeatureExtractor"}, {"fullname": "dlc2action.feature_extraction.PoseFeatureExtractor.__init__", "modulename": "dlc2action.feature_extraction", "qualname": "PoseFeatureExtractor.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_store : PoseInputStore\n    the input store object</p>\n", "signature": "(\n    self,\n    input_store: dlc2action.data.base_store.PoseInputStore,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.feature_extraction.PoseFeatureExtractor.input_store_class", "modulename": "dlc2action.feature_extraction", "qualname": "PoseFeatureExtractor.input_store_class", "type": "class", "doc": "<p>The <code>dlc2action.data.base_Store.InputStore</code> child class paired with this feature extractor</p>\n", "bases": "dlc2action.data.base_store.InputStore"}, {"fullname": "dlc2action.feature_extraction.KinematicExtractor", "modulename": "dlc2action.feature_extraction", "qualname": "KinematicExtractor", "type": "class", "doc": "<p>A feature extractor for basic kinematic features: speeds, accelerations, distances.</p>\n\n<p>The available keys are:\n    - coords: the allocentric bodypart coordinates,\n    - coord_diff: the egocentric bodypart coordinates,\n    - center: the body center (mean of bodyparts) coordinates,\n    - intra_distance: distances between bodyparts (pairs set in <code>distance_pairs</code> or all combinations by default),\n    - inter_distance: computed in interactive mode (for pairs of animals); distances from each bodypart of each animal to the centroid between them,\n    - speed_direction: unit vector of speed approximation for each bodypart,\n    - speed_value: l2 norm of the speed approximation vector for each bodypart,\n    - acc_joints: l2 norm of the acceleration approximation vector for each bodypart,\n    - angle_speeds: vector of angle speed approximation for each bodypart,\n    - angles: cosines of angles set in <code>angle_pairs</code>,\n    - areas: areas of polygons set in <code>area_vertices</code>,\n    - zone_bools: binary identifier of zone visitation, defined in <code>zone_bools</code>,\n    - zone_distances: distance to zone boundary, defined in <code>zone_distances'</code>,\n    - likelihood: pose estimation likelihood (if known).</p>\n\n<p>The default set is <code>{coord_diff, center, intra_distance, inter_distance, speed_direction, speed_value, acc_joints, angle_speeds}</code></p>\n", "bases": "PoseFeatureExtractor"}, {"fullname": "dlc2action.feature_extraction.KinematicExtractor.__init__", "modulename": "dlc2action.feature_extraction", "qualname": "KinematicExtractor.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_store : PoseInputStore\n    the input store object\nkeys : list, optional\n    a list of names of the features to extract\nignored_clips : list, optional\n    a list of clip ids to ignore\ninteractive : bool, default False\n    if <code>True</code>, features for pairs of clips will be computed\naveraging_window : int, default 1\n    if &gt;1, features are averaged with a moving window of this size (in frames)\ndistance_pairs : list, optional\n    a list of bodypart name tuples (e.g. <code>[(\"tail\", \"nose\")]</code>) to compute distances for when <code>\"intra_distance\"</code>\n    is in <code>keys</code> (by default all distances are computed)\nangle_pairs : list, optional\n    a list of bodypart name tuples (e.g. <code>[(\"ear1\", \"nose\", \"ear2\")]</code>) for the angle between <code>\"ear1\"--\"nose\"</code> and\n    <code>\"nose\"--\"ear2\"</code> lines) to compute angle cosines for when <code>\"angles\"</code> is in <code>keys</code> (by default no angles are computed)\nneighboring_frames : int, default 0\n    if &gt;0, this number of neighboring frames is aggregated in the center frame features (generally not recommended)\narea_vertices : list, optional\n    a list of bodypart name tuples of any length &gt;= 3 (e.g. <code>[(\"ear1\", \"nose\", \"ear2\", \"spine1\")]</code>) that define polygons\n    to compute areas for when <code>\"areas\"</code> is in <code>keys</code> (by default no areas are computed)\nzone_vertices : dict, optional\n    a dictionary of bodypart name tuples of any length &gt;= 3 that define zones for <code>\"zone_bools\"</code>and <code>\"zone_distances\"</code>\n    featyres; keys should be zone names and values should be tuples that define the polygons (e.g.\n    <code>{\"main_area\": (\"x_min\", \"x_max\", \"y_max\", \"y_min\"))}</code>)\nzone_bools : list, optional\n    a list of zone and bodypart name tuples to compute binary identifiers for (1 if an animal is within the polygon or\n    0 if it's outside) (e.g. <code>[(\"main_area\", \"nose\")]</code>); the zones should be defined in the <code>zone_vertices</code> parameter;\n    this is only computed if <code>\"zone_bools\"</code> is in <code>keys</code>\nzone_distances : list, optional\n    a list of zone and bodypart name tuples to compute distances for (distance from the bodypart to the closest of the\n    boundaries) (e.g. <code>[(\"main_area\", \"nose\")]</code>); the zones should be defined in the <code>zone_vertices</code> parameter;\n    this is only computed if <code>\"zone_distances\"</code> is in <code>keys</code></p>\n", "signature": "(\n    self,\n    input_store: dlc2action.data.base_store.PoseInputStore,\n    keys: List = None,\n    ignored_clips: List = None,\n    interactive: bool = False,\n    averaging_window: int = 1,\n    distance_pairs: List = None,\n    angle_pairs: List = None,\n    neighboring_frames: int = 0,\n    area_vertices: List = None,\n    zone_vertices: Dict = None,\n    zone_bools: List = None,\n    zone_distances: List = None,\n    *args,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.feature_extraction.KinematicExtractor.extract_features", "modulename": "dlc2action.feature_extraction", "qualname": "KinematicExtractor.extract_features", "type": "function", "doc": "<p>Extract features from a data dictionary</p>\n\n<p>An input store will call this method while pre-computing a dataset. We do not assume a specific\nstructure in the data dictionary, so all necessary information (coordinates of a bodypart, number\nof frames, list of bodyparts) is inferred using input store methods.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_dict : dict\n    the data dictionary\nvideo_id : str\n    the id of the video associated with the data dictionary\nprefix : str, optional\n    a prefix for the feature names\none_clip : bool, default False\n    if <code>True</code>, all features will be concatenated and assigned to one clip named <code>'all'</code></p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>features : dict\n    a features dictionary where the keys are the feature names (e.g. 'coords', 'distances') and the\n    values are numpy arrays of shape <code>(#features, #frames)</code></p>\n", "signature": "(\n    self,\n    data_dict: Dict,\n    video_id: str,\n    prefix: str = None,\n    one_clip: bool = False\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.loss", "modulename": "dlc2action.loss", "type": "module", "doc": "<h2 id=\"losses\">Losses</h2>\n\n<p>There is no dedicated loss class in <code>dlc2action</code>. Instead we use regular <code>torch.nn.Module</code> instances that take\nprediction\nand target as input and return loss value as output.</p>\n"}, {"fullname": "dlc2action.loss.contrastive", "modulename": "dlc2action.loss.contrastive", "type": "module", "doc": "<p>Losses used by contrastive SSL constructors (see <code>dlc2action.ssl.contrastive</code>)</p>\n"}, {"fullname": "dlc2action.loss.contrastive_frame", "modulename": "dlc2action.loss.contrastive_frame", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dlc2action.loss.ms_tcn", "modulename": "dlc2action.loss.ms_tcn", "type": "module", "doc": "<p>Loss for the MS-TCN models</p>\n\n<p>Adapted from https://github.com/sj-li/MS-TCN2</p>\n"}, {"fullname": "dlc2action.loss.ms_tcn.MS_TCN_Loss", "modulename": "dlc2action.loss.ms_tcn", "qualname": "MS_TCN_Loss", "type": "class", "doc": "<p>The MS-TCN loss\nCrossentropy + consistency loss (MSE over predicted probabilities)</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "dlc2action.loss.ms_tcn.MS_TCN_Loss.__init__", "modulename": "dlc2action.loss.ms_tcn", "qualname": "MS_TCN_Loss.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_classes : int\n    number of classes\nweights : iterable, optional\n    class-wise cross-entropy weights\nexclusive : bool, default True\n    True if single-label classification is used\nignore_index : int, default -100\n    the elements where target is equal to ignore_index will be ignored by cross-entropy\nfocal : bool, default False\n    if True, instead of regular cross-entropy the focal loss will be used\ngamma : float, default 1\n    the gamma parameter of the focal loss\nalpha : float, default 0.15\n    the weight of the consistency loss\nhard_negative_weight : float, default 1\n    the weight assigned to the hard negative frames</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    weights: collections.abc.Iterable = None,\n    exclusive: bool = True,\n    ignore_index: int = -100,\n    focal: bool = False,\n    gamma: float = 1,\n    alpha: float = 0.15,\n    hard_negative_weight: float = 1\n)", "funcdef": "def"}, {"fullname": "dlc2action.loss.ms_tcn.MS_TCN_Loss.consistency_loss", "modulename": "dlc2action.loss.ms_tcn", "qualname": "MS_TCN_Loss.consistency_loss", "type": "function", "doc": "<p>Apply consistency loss</p>\n", "signature": "(self, p: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.loss.ms_tcn.MS_TCN_Loss.forward", "modulename": "dlc2action.loss.ms_tcn", "qualname": "MS_TCN_Loss.forward", "type": "function", "doc": "<p>Compute the loss</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predictions : torch.Tensor\n    a tensor of shape (#batch, #classes, #frames)\ntarget : torch.Tensor\n    a tensor of shape (#batch, #classes, #frames) or (#batch, #frames)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss : float\n    the loss value</p>\n", "signature": "(self, predictions: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.loss.ms_tcn.BoundaryRegressionLoss", "modulename": "dlc2action.loss.ms_tcn", "qualname": "BoundaryRegressionLoss", "type": "class", "doc": "<p>Boundary Regression Loss\n    bce: Binary Cross Entropy Loss for Boundary Prediction\n    mse: Mean Squared Error</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "dlc2action.loss.ms_tcn.BoundaryRegressionLoss.__init__", "modulename": "dlc2action.loss.ms_tcn", "qualname": "BoundaryRegressionLoss.__init__", "type": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "(\n    self,\n    bce: bool = True,\n    focal: bool = False,\n    mse: bool = False,\n    weight: Optional[torch.Tensor] = None,\n    pos_weight: Optional[float] = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.loss.ms_tcn.BoundaryRegressionLoss.forward", "modulename": "dlc2action.loss.ms_tcn", "qualname": "BoundaryRegressionLoss.forward", "type": "function", "doc": "<p>Args:\n    preds: torch.float (N, 1, T).\n    gts: torch. (N, 1, T).\n    masks: torch.bool (N, 1, T).</p>\n", "signature": "(self, preds: torch.Tensor, gts: torch.Tensor, masks: torch.Tensor)", "funcdef": "def"}, {"fullname": "dlc2action.loss.ms_tcn.FocalLoss", "modulename": "dlc2action.loss.ms_tcn", "qualname": "FocalLoss", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "dlc2action.loss.ms_tcn.FocalLoss.__init__", "modulename": "dlc2action.loss.ms_tcn", "qualname": "FocalLoss.__init__", "type": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "(\n    self,\n    weight: Optional[torch.Tensor] = None,\n    size_average: bool = True,\n    batch_average: bool = True,\n    ignore_index: int = 255,\n    gamma: float = 2.0,\n    alpha: float = 0.25\n)", "funcdef": "def"}, {"fullname": "dlc2action.loss.ms_tcn.FocalLoss.forward", "modulename": "dlc2action.loss.ms_tcn", "qualname": "FocalLoss.forward", "type": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "(self, logit: torch.Tensor, target: torch.Tensor) -> torch.Tensor", "funcdef": "def"}, {"fullname": "dlc2action.loss.ms_tcn.ASRFLoss", "modulename": "dlc2action.loss.ms_tcn", "qualname": "ASRFLoss", "type": "class", "doc": "<p>The MS-TCN loss\nCrossentropy + consistency loss (MSE over predicted probabilities)</p>\n", "bases": "MS_TCN_Loss"}, {"fullname": "dlc2action.loss.ms_tcn.ASRFLoss.__init__", "modulename": "dlc2action.loss.ms_tcn", "qualname": "ASRFLoss.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_classes : int\n    number of classes\nweights : iterable, optional\n    class-wise cross-entropy weights\nexclusive : bool, default True\n    True if single-label classification is used\nignore_index : int, default -100\n    the elements where target is equal to ignore_index will be ignored by cross-entropy\nfocal : bool, default False\n    if True, instead of regular cross-entropy the focal loss will be used\ngamma : float, default 1\n    the gamma parameter of the focal loss\nalpha : float, default 0.15\n    the weight of the consistency loss\nhard_negative_weight : float, default 1\n    the weight assigned to the hard negative frames</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    weights: collections.abc.Iterable = None,\n    boundary_pos_weight: float = None,\n    boundary_weight: float = 1,\n    exclusive: bool = True,\n    ignore_index: int = -100,\n    focal: bool = False,\n    gamma: float = 1,\n    alpha: float = 0.15,\n    hard_negative_weight: float = 1\n)", "funcdef": "def"}, {"fullname": "dlc2action.loss.ms_tcn.ASRFLoss.forward", "modulename": "dlc2action.loss.ms_tcn", "qualname": "ASRFLoss.forward", "type": "function", "doc": "<p>Compute the loss</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predictions : torch.Tensor\n    a tensor of shape (#batch, #classes, #frames)\ntarget : torch.Tensor\n    a tensor of shape (#batch, #classes, #frames) or (#batch, #frames)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss : float\n    the loss value</p>\n", "signature": "(self, predictions: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.loss.mse", "modulename": "dlc2action.loss.mse", "type": "module", "doc": "<p>The mean squared error loss</p>\n"}, {"fullname": "dlc2action.loss.tcc", "modulename": "dlc2action.loss.tcc", "type": "module", "doc": "<p>TCC loss</p>\n\n<p>Adapted from from https://github.com/June01/tcc_Temporal_Cycle_Consistency_Loss.pytorch</p>\n"}, {"fullname": "dlc2action.loss.tcc.gen_cycles", "modulename": "dlc2action.loss.tcc", "qualname": "gen_cycles", "type": "function", "doc": "<p>Generates cycles for alignment.\nGenerates a batch of indices to cycle over. For example setting num_cycles=2,\nbatch_size=5, cycle_length=3 might return something like this:\ncycles = [[0, 3, 4, 0], [1, 2, 0, 3]]. This means we have 2 cycles for which\nthe loss will be calculated. The first cycle starts at sequence 0 of the\nbatch, then we find a matching step in sequence 3 of that batch, then we\nfind matching step in sequence 4 and finally come back to sequence 0,\ncompleting a cycle.\nArgs:\nnum_cycles: Integer, Number of cycles that will be matched in one pass.\nbatch_size: Integer, Number of sequences in one batch.\ncycle_length: Integer, Length of the cycles. If we are matching between\n  2 sequences (cycle_length=2), we get cycles that look like [0,1,0].\n  This means that we go from sequence 0 to sequence 1 then back to sequence</p>\n\n<ol>\n<li>A cycle length of 3 might look like [0, 1, 2, 0].\nReturns:\ncycles: Tensor, Batch indices denoting cycles that will be used for\ncalculating the alignment loss.</li>\n</ol>\n", "signature": "(num_cycles, batch_size, cycle_length=2)", "funcdef": "def"}, {"fullname": "dlc2action.loss.tcc.compute_stochastic_alignment_loss", "modulename": "dlc2action.loss.tcc", "qualname": "compute_stochastic_alignment_loss", "type": "function", "doc": "<p></p>\n", "signature": "(\n    embs,\n    steps,\n    seq_lens,\n    num_steps,\n    batch_size,\n    loss_type,\n    similarity_type,\n    num_cycles,\n    cycle_length,\n    temperature,\n    label_smoothing,\n    variance_lambda,\n    huber_delta,\n    normalize_indices,\n    real_lens\n)", "funcdef": "def"}, {"fullname": "dlc2action.loss.tcc.compute_alignment_loss", "modulename": "dlc2action.loss.tcc", "qualname": "compute_alignment_loss", "type": "function", "doc": "<p></p>\n", "signature": "(\n    embs,\n    real_lens,\n    steps=None,\n    seq_lens=None,\n    normalize_embeddings=False,\n    loss_type='classification',\n    similarity_type='l2',\n    num_cycles=20,\n    cycle_length=2,\n    temperature=0.1,\n    label_smoothing=0.1,\n    variance_lambda=0.001,\n    huber_delta=0.1,\n    normalize_indices=True\n)", "funcdef": "def"}, {"fullname": "dlc2action.loss.tcc.regression_loss", "modulename": "dlc2action.loss.tcc", "qualname": "regression_loss", "type": "function", "doc": "<p>Loss function based on regressing to the correct indices.\nIn the paper, this is called Cycle-back Regression. There are 3 variants\nof this loss:\ni) regression_mse: MSE of the predicted indices and ground truth indices.\nii) regression_mse_var: MSE of the predicted indices that takes into account\nthe variance of the similarities. This is important when the rate at which\nsequences go through different phases changes a lot. The variance scaling\nallows dynamic weighting of the MSE loss based on the similarities.\niii) regression_huber: Huber loss between the predicted indices and ground\ntruth indices.\nArgs:\n  logits: Tensor, Pre-softmax similarity scores after cycling back to the\n    starting sequence.\n  labels: Tensor, One hot labels containing the ground truth. The index where\n    the cycle started is 1.\n  num_steps: Integer, Number of steps in the sequence embeddings.\n  steps: Tensor, step indices/frame indices of the embeddings of the shape\n    [N, T] where N is the batch size, T is the number of the timesteps.\n  seq_lens: Tensor, Lengths of the sequences from which the sampling was done.\n    This can provide additional temporal information to the alignment loss.\n  loss_type: String, This specifies the kind of regression loss function.\n    Currently supported loss functions: regression_mse, regression_mse_var,\n    regression_huber.\n  normalize_indices: Boolean, If True, normalizes indices by sequence lengths.\n    Useful for ensuring numerical instabilities don't arise as sequence\n    indices can be large numbers.\n  variance_lambda: Float, Weight of the variance of the similarity\n    predictions while cycling back. If this is high then the low variance\n    similarities are preferred by the loss while making this term low results\n    in high variance of the similarities (more uniform/random matching).\nReturns:\n   loss: Tensor, A scalar loss calculated using a variant of regression.</p>\n", "signature": "(\n    logits,\n    labels,\n    num_steps,\n    steps,\n    seq_lens,\n    loss_type,\n    normalize_indices,\n    variance_lambda\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric", "modulename": "dlc2action.metric", "type": "module", "doc": "<h2 id=\"metrics\">Metrics</h2>\n\n<p>Metrics that can be updated with batch data and computed at the end of an epoch.</p>\n"}, {"fullname": "dlc2action.metric.base_metric", "modulename": "dlc2action.metric.base_metric", "type": "module", "doc": "<p>Abstract parent class for all metrics</p>\n"}, {"fullname": "dlc2action.metric.base_metric.Metric", "modulename": "dlc2action.metric.base_metric", "qualname": "Metric", "type": "class", "doc": "<p>Base class for all metric</p>\n\n<p>Metrics are reset at the beginning of each epoch, updated with batch data and then calculated at the end of the epoch.\nIf needs_raw_data is True for a metric class, it should expect to receive raw model output as the predicted vector;\notherwise it should be the final class prediction</p>\n", "bases": "abc.ABC"}, {"fullname": "dlc2action.metric.base_metric.Metric.needs_raw_data", "modulename": "dlc2action.metric.base_metric", "qualname": "Metric.needs_raw_data", "type": "variable", "doc": "<p>If <code>True</code>, <code>dlc2action.task.universal_task.Task</code> will pass raw data to the metric (only primary predict \nfunction applied).\nOtherwise it will pass a prediction for the classes.</p>\n", "default_value": " = False"}, {"fullname": "dlc2action.metric.base_metric.Metric.update", "modulename": "dlc2action.metric.base_metric", "qualname": "Metric.update", "type": "function", "doc": "<p>Update the intrinsic parameters (with a batch)</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predicted : torch.Tensor\n    the main prediction tensor generated by the model\nssl_predicted : torch.Tensor\n    the SSL prediction tensor generated by the model\ntarget : torch.Tensor\n    the corresponding main target tensor\nssl_target : torch.Tensor\n    the corresponding SSL target tensor\ntags : torch.Tensor\n    the tensor of meta tags (or <code>None</code>, if tags are not given)</p>\n", "signature": "(\n    self,\n    predicted: torch.Tensor,\n    target: torch.Tensor,\n    tags: torch.Tensor\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.metric.base_metric.Metric.reset", "modulename": "dlc2action.metric.base_metric", "qualname": "Metric.reset", "type": "function", "doc": "<p>Reset the intrinsic parameters (at the beginning of an epoch)</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.metric.base_metric.Metric.calculate", "modulename": "dlc2action.metric.base_metric", "qualname": "Metric.calculate", "type": "function", "doc": "<p>Calculate the metric (at the end of an epoch)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : float | dict\n    either the single value of the metric or a dictionary where the keys are class indices and the values\n    are class metric values</p>\n", "signature": "(self) -> Union[float, Dict]", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics", "modulename": "dlc2action.metric.metrics", "type": "module", "doc": "<p>Implementations of <code>dlc2action.metric.base_metric.Metric</code></p>\n"}, {"fullname": "dlc2action.metric.metrics.PR_AUC", "modulename": "dlc2action.metric.metrics", "qualname": "PR_AUC", "type": "class", "doc": "<p>Area under precision-recall curve (not advised for training)</p>\n", "bases": "_ClassificationMetric"}, {"fullname": "dlc2action.metric.metrics.PR_AUC.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "PR_AUC.__init__", "type": "function", "doc": "<p>Initialize the metric</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignore_index : int, default -100\n    the class index that indicates ignored samples\naverage: {'macro', 'micro', 'none'}\n    method for averaging across classes\nnum_classes : int, optional\n    number of classes (not necessary if main_class is not None)\nignored_classes : set, optional\n    a set of class ids to ignore in calculation\nexclusive: bool, default True\n    set to False for multi-label classification tasks\ntag_average: {'micro', 'macro', 'none'}\n    method for averaging across meta tags (if given)\nthreshold_step : float, default 0.1\n    the decision threshold step</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    average: str = 'macro',\n    ignored_classes: Set = None,\n    exclusive: bool = False,\n    tag_average: str = 'micro',\n    threshold_step: float = 0.1\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.Precision", "modulename": "dlc2action.metric.metrics", "qualname": "Precision", "type": "class", "doc": "<p>Precision</p>\n", "bases": "_ClassificationMetric"}, {"fullname": "dlc2action.metric.metrics.Precision.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "Precision.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignore_index : int, default -100\n    the class index that indicates ignored samples\naverage: {'macro', 'micro', 'none'}\n    method for averaging across classes\nnum_classes : int, optional\n    number of classes (not necessary if main_class is not None)\nignored_classes : set, optional\n    a set of class ids to ignore in calculation\nexclusive: bool, default True\n    set to False for multi-label classification tasks\ntag_average: {'micro', 'macro', 'none'}\n    method for averaging across meta tags (if given)\nthreshold_value : float | list, optional\n    the decision threshold value (cannot be defined for exclusive classification, and 0.5 be default\n    for non-exclusive); if <code>threshold_value</code> is a list, every value should correspond to the class\n    under the same index</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    average: str = 'macro',\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    tag_average: str = 'micro',\n    threshold_value: Union[float, List] = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SegmentalPrecision", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalPrecision", "type": "class", "doc": "<p>Segmental precision (not advised for training)</p>\n", "bases": "_ClassificationMetric"}, {"fullname": "dlc2action.metric.metrics.SegmentalPrecision.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalPrecision.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignore_index : int, default -100\n    the class index that indicates ignored samples\naverage: {'macro', 'micro', 'none'}\n    method for averaging across classes\nnum_classes : int, optional\n    number of classes (not necessary if main_class is not None)\nignored_classes : set, optional\n    a set of class ids to ignore in calculation\nexclusive: bool, default True\n    set to False for multi-label classification tasks\niou_threshold : float, default 0.5\n    if segmental is true, intervals with IoU larger than this threshold are considered correct\ntag_average: {'micro', 'macro', 'none'}\n    method for averaging across meta tags (if given)\nthreshold_value : float | list, optional\n    the decision threshold value (cannot be defined for exclusive classification, and 0.5 be default\n    for non-exclusive); if <code>threshold_value</code> is a list, every value should correspond to the class\n    under the same index</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    average: str = 'macro',\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    iou_threshold: float = 0.5,\n    tag_average: str = 'micro',\n    threshold_value: Union[float, List] = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SegmentalPrecision.segmental", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalPrecision.segmental", "type": "variable", "doc": "<p>If <code>True</code>, the metric will be calculated over segments; otherwise over frames.</p>\n", "default_value": " = True"}, {"fullname": "dlc2action.metric.metrics.Recall", "modulename": "dlc2action.metric.metrics", "qualname": "Recall", "type": "class", "doc": "<p>Recall</p>\n", "bases": "_ClassificationMetric"}, {"fullname": "dlc2action.metric.metrics.Recall.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "Recall.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignore_index : int, default -100\n    the class index that indicates ignored samples\naverage: {'macro', 'micro', 'none'}\n    method for averaging across classes\nnum_classes : int, optional\n    number of classes (not necessary if main_class is not None)\nignored_classes : set, optional\n    a set of class ids to ignore in calculation\nexclusive: bool, default True\n    set to False for multi-label classification tasks\ntag_average: {'micro', 'macro', 'none'}\n    method for averaging across meta tags (if given)\nthreshold_value : float | list, optional\n    the decision threshold value (cannot be defined for exclusive classification, and 0.5 be default\n    for non-exclusive); if <code>threshold_value</code> is a list, every value should correspond to the class\n    under the same index</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    average: str = 'macro',\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    tag_average: str = 'micro',\n    threshold_value: Union[float, List] = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SegmentalRecall", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalRecall", "type": "class", "doc": "<p>Segmental recall (not advised for training)</p>\n", "bases": "_ClassificationMetric"}, {"fullname": "dlc2action.metric.metrics.SegmentalRecall.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalRecall.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignore_index : int, default -100\n    the class index that indicates ignored samples\naverage: {'macro', 'micro', 'none'}\n    method for averaging across classes\nnum_classes : int, optional\n    number of classes (not necessary if main_class is not None)\nignored_classes : set, optional\n    a set of class ids to ignore in calculation\nexclusive: bool, default True\n    set to False for multi-label classification tasks\niou_threshold : float, default 0.5\n    if segmental is true, intervals with IoU larger than this threshold are considered correct\ntag_average: {'micro', 'macro', 'none'}\n    method for averaging across meta tags (if given)\nthreshold_value : float | list, optional\n    the decision threshold value (cannot be defined for exclusive classification, and 0.5 be default\n    for non-exclusive); if <code>threshold_value</code> is a list, every value should correspond to the class\n    under the same index</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    average: str = 'macro',\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    iou_threshold: float = 0.5,\n    tag_average: str = 'micro',\n    threshold_value: Union[float, List] = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SegmentalRecall.segmental", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalRecall.segmental", "type": "variable", "doc": "<p>If <code>True</code>, the metric will be calculated over segments; otherwise over frames.</p>\n", "default_value": " = True"}, {"fullname": "dlc2action.metric.metrics.F1", "modulename": "dlc2action.metric.metrics", "qualname": "F1", "type": "class", "doc": "<p>F1 score</p>\n", "bases": "_ClassificationMetric"}, {"fullname": "dlc2action.metric.metrics.F1.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "F1.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignore_index : int, default -100\n    the class index that indicates ignored samples\naverage: {'macro', 'micro', 'none'}\n    method for averaging across classes\nnum_classes : int, optional\n    number of classes (not necessary if main_class is not None)\nignored_classes : set, optional\n    a set of class ids to ignore in calculation\nexclusive: bool, default True\n    set to False for multi-label classification tasks\ntag_average: {'micro', 'macro', 'none'}\n    method for averaging across meta tags (if given)\nthreshold_value : float | list, optional\n    the decision threshold value (cannot be defined for exclusive classification, and 0.5 be default\n    for non-exclusive); if <code>threshold_value</code> is a list, every value should correspond to the class\n    under the same index</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    average: str = 'macro',\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    tag_average: str = 'micro',\n    threshold_value: Union[float, List] = None,\n    integration_interval: int = 0\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SegmentalF1", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalF1", "type": "class", "doc": "<p>Segmental F1 score (not advised for training)</p>\n", "bases": "_ClassificationMetric"}, {"fullname": "dlc2action.metric.metrics.SegmentalF1.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalF1.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignore_index : int, default -100\n    the class index that indicates ignored samples\naverage: {'macro', 'micro', 'none'}\n    method for averaging across classes\nnum_classes : int, optional\n    number of classes (not necessary if main_class is not None)\nignored_classes : set, optional\n    a set of class ids to ignore in calculation\nexclusive: bool, default True\n    set to False for multi-label classification tasks\niou_threshold : float, default 0.5\n    if segmental is true, intervals with IoU larger than this threshold are considered correct\ntag_average: {'micro', 'macro', 'none'}\n    method for averaging across meta tags (if given)\nthreshold_value : float | list, optional\n    the decision threshold value (cannot be defined for exclusive classification, and 0.5 be default\n    for non-exclusive); if <code>threshold_value</code> is a list, every value should correspond to the class\n    under the same index</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    average: str = 'macro',\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    iou_threshold: float = 0.5,\n    tag_average: str = 'micro',\n    threshold_value: Union[float, List] = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SegmentalF1.segmental", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalF1.segmental", "type": "variable", "doc": "<p>If <code>True</code>, the metric will be calculated over segments; otherwise over frames.</p>\n", "default_value": " = True"}, {"fullname": "dlc2action.metric.metrics.Fbeta", "modulename": "dlc2action.metric.metrics", "qualname": "Fbeta", "type": "class", "doc": "<p>F-beta score</p>\n", "bases": "_ClassificationMetric"}, {"fullname": "dlc2action.metric.metrics.Fbeta.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "Fbeta.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>beta : float, default 1\n    the beta parameter\nignore_index : int, default -100\n    the class index that indicates ignored samples\naverage: {'macro', 'micro', 'none'}\n    method for averaging across classes\nnum_classes : int, optional\n    number of classes (not necessary if main_class is not None)\nignored_classes : set, optional\n    a set of class ids to ignore in calculation\nexclusive: bool, default True\n    set to False for multi-label classification tasks\ntag_average: {'micro', 'macro', 'none'}\n    method for averaging across meta tags (if given)\nthreshold_value : float | list, optional\n    the decision threshold value (cannot be defined for exclusive classification, and 0.5 be default\n    for non-exclusive); if <code>threshold_value</code> is a list, every value should correspond to the class\n    under the same index</p>\n", "signature": "(\n    self,\n    beta: float = 1,\n    ignore_index: int = -100,\n    average: str = 'macro',\n    num_classes: int = None,\n    ignored_classes: Set = None,\n    tag_average: str = 'micro',\n    exclusive: bool = True,\n    threshold_value: float = 0.5\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SegmentalFbeta", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalFbeta", "type": "class", "doc": "<p>Segmental F-beta score (not advised for training)</p>\n", "bases": "_ClassificationMetric"}, {"fullname": "dlc2action.metric.metrics.SegmentalFbeta.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalFbeta.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>beta : float, default 1\n    the beta parameter\nignore_index : int, default -100\n    the class index that indicates ignored samples\naverage: {'macro', 'micro', 'none'}\n    method for averaging across classes\nnum_classes : int, optional\n    number of classes (not necessary if main_class is not None)\nignored_classes : set, optional\n    a set of class ids to ignore in calculation\nexclusive: bool, default True\n    set to False for multi-label classification tasks\niou_threshold : float, default 0.5\n    if segmental is true, intervals with IoU larger than this threshold are considered correct\ntag_average: {'micro', 'macro', 'none'}\n    method for averaging across meta tags (if given)\nthreshold_value : float | list, optional\n    the decision threshold value (cannot be defined for exclusive classification, and 0.5 be default\n    for non-exclusive); if <code>threshold_value</code> is a list, every value should correspond to the class\n    under the same index</p>\n", "signature": "(\n    self,\n    beta: float = 1,\n    ignore_index: int = -100,\n    average: str = 'macro',\n    num_classes: int = None,\n    ignored_classes: Set = None,\n    iou_threshold: float = 0.5,\n    tag_average: str = 'micro',\n    exclusive: bool = True,\n    threshold_value: float = 0.5\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SegmentalFbeta.segmental", "modulename": "dlc2action.metric.metrics", "qualname": "SegmentalFbeta.segmental", "type": "variable", "doc": "<p>If <code>True</code>, the metric will be calculated over segments; otherwise over frames.</p>\n", "default_value": " = True"}, {"fullname": "dlc2action.metric.metrics.SemiSegmentalRecall", "modulename": "dlc2action.metric.metrics", "qualname": "SemiSegmentalRecall", "type": "class", "doc": "<p>Semi-segmental recall (not advised for training)</p>\n\n<p>A metric in-between segmental and frame-wise recall.</p>\n\n<p>This metric follows the following algorithm:\n1) smooth over too-short intervals, both in ground truth and in prediction (first remove\n    groups of zeros shorter than <code>smooth_interval</code> and then do the same with groups of ones),\n2) add <code>delta</code> frames to each ground truth interval at both ends and count the number of predicted\n    positive frames at the resulting intervals (intersection),\n3) calculate the threshold for each interval as\n    <code>t = sigmoid(4 * (a * x + b)) * (iou_threshold_long - iou_threshold_short))</code>, where\n    <code>a = 2 / (long_length - short_length)</code>, <code>b = 1 - a * long_length</code>, <code>x</code> is the length of the interval\n    before <code>delta</code> was added,\n4) for each interval, if intersection is higher than <code>t * x</code>, the interval is labeled as true positive (<code>TP</code>),\n    and otherwise as false negative (<code>FN</code>),\n5) the final metric value is computed as <code>TP / (TP + FN)</code>.</p>\n", "bases": "_SemiSegmentalMetric"}, {"fullname": "dlc2action.metric.metrics.SemiSegmentalRecall.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "SemiSegmentalRecall.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_classes : int\n    the number of classes in the dataset\nignore_index : int, default -100\n    the ground truth label to ignore\nignored_classes : set, optional\n    the class indices to ignore in computation\nexclusive : bool, default True\n    <code>False</code> for multi-label classification tasks\naverage : {\"macro\", \"micro\", \"none\"}\n    the method to average the results over classes\ntag_average : {\"macro\", \"micro\", \"none\"}\n    the method to average the results over meta tags (if given)\ndelta : int, default 0\n    the number of frames to add to each ground truth interval before computing the intersection,\n    see description of the class for details\nsmooth_interval : int, default 0\n    intervals shorter than this number of frames will be ignored (both in prediction and in ground truth,\n    see description of the class for details\niou_threshold_long : float, default 0.5\n    the intersection threshold for segments longer than <code>long_length</code> frames (between 0 and 1),\n    see description of the class for details\niou_threshold_short : float, default 0.5\n    the intersection threshold for segments shorter than <code>short_length</code> frames (between 0 and 1),\n    see description of the class for details\nshort_length : int, default 30\n    the threshold number of frames for short intervals that will have an intersection threshold of\n    <code>iou_threshold_short</code>, see description of the class for details\nlong_length : int, default 300\n    the threshold number of frames for long intervals that will have an intersection threshold of\n    <code>iou_threshold_long</code>, see description of the class for details</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    average: str = 'macro',\n    tag_average: str = 'micro',\n    delta: int = 0,\n    smooth_interval: int = 0,\n    iou_threshold_long: float = 0.5,\n    iou_threshold_short: float = 0.5,\n    short_length: int = 30,\n    long_length: int = 300,\n    threshold_value: Union[float, List] = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SemiSegmentalPrecision", "modulename": "dlc2action.metric.metrics", "qualname": "SemiSegmentalPrecision", "type": "class", "doc": "<p>Semi-segmental precision (not advised for training)</p>\n\n<p>A metric in-between segmental and frame-wise precision.</p>\n\n<p>This metric follows the following algorithm:\n1) smooth over too-short intervals, both in ground truth and in prediction (first remove\n    groups of zeros shorter than <code>smooth_interval</code> and then do the same with groups of ones),\n2) add <code>delta</code> frames to each predicted interval at both ends and count the number of ground truth\n    positive frames at the resulting intervals (intersection),\n3) calculate the threshold for each interval as\n    <code>t = sigmoid(4 * (a * x + b)) * (iou_threshold_long - iou_threshold_short))</code>, where\n    <code>a = 2 / (long_length - short_length)</code>, <code>b = 1 - a * long_length</code>, <code>x</code> is the length of the interval\n    before <code>delta</code> was added,\n4) for each interval, if intersection is higher than <code>t * x</code>, the interval is labeled as true positive (<code>TP</code>),\n    and otherwise as false positive (<code>FP</code>),\n5) the final metric value is computed as <code>TP / (TP + FP)</code>.</p>\n", "bases": "_SemiSegmentalMetric"}, {"fullname": "dlc2action.metric.metrics.SemiSegmentalPrecision.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "SemiSegmentalPrecision.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_classes : int\n    the number of classes in the dataset\nignore_index : int, default -100\n    the ground truth label to ignore\nignored_classes : set, optional\n    the class indices to ignore in computation\nexclusive : bool, default True\n    <code>False</code> for multi-label classification tasks\naverage : {\"macro\", \"micro\", \"none\"}\n    the method to average the results over classes\ntag_average : {\"macro\", \"micro\", \"none\"}\n    the method to average the results over meta tags (if given)\ndelta : int, default 0\n    the number of frames to add to each ground truth interval before computing the intersection,\n    see description of the class for details\nsmooth_interval : int, default 0\n    intervals shorter than this number of frames will be ignored (both in prediction and in ground truth,\n    see description of the class for details\niou_threshold_long : float, default 0.5\n    the intersection threshold for segments longer than <code>long_length</code> frames (between 0 and 1),\n    see description of the class for details\niou_threshold_short : float, default 0.5\n    the intersection threshold for segments shorter than <code>short_length</code> frames (between 0 and 1),\n    see description of the class for details\nshort_length : int, default 30\n    the threshold number of frames for short intervals that will have an intersection threshold of\n    <code>iou_threshold_short</code>, see description of the class for details\nlong_length : int, default 300\n    the threshold number of frames for long intervals that will have an intersection threshold of\n    <code>iou_threshold_long</code>, see description of the class for details</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    average: str = 'macro',\n    tag_average: str = 'micro',\n    delta: int = 0,\n    smooth_interval: int = 0,\n    iou_threshold_long: float = 0.5,\n    iou_threshold_short: float = 0.5,\n    short_length: int = 30,\n    long_length: int = 300,\n    threshold_value: Union[float, List] = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SemiSegmentalF1", "modulename": "dlc2action.metric.metrics", "qualname": "SemiSegmentalF1", "type": "class", "doc": "<p>The F1 score for semi-segmental recall and precision (not advised for training)</p>\n", "bases": "_SemiSegmentalMetric"}, {"fullname": "dlc2action.metric.metrics.SemiSegmentalF1.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "SemiSegmentalF1.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_classes : int\n    the number of classes in the dataset\nignore_index : int, default -100\n    the ground truth label to ignore\nignored_classes : set, optional\n    the class indices to ignore in computation\nexclusive : bool, default True\n    <code>False</code> for multi-label classification tasks\naverage : {\"macro\", \"micro\", \"none\"}\n    the method to average the results over classes\ntag_average : {\"macro\", \"micro\", \"none\"}\n    the method to average the results over meta tags (if given)\ndelta : int, default 0\n    the number of frames to add to each ground truth interval before computing the intersection,\n    see description of the class for details\nsmooth_interval : int, default 0\n    intervals shorter than this number of frames will be ignored (both in prediction and in ground truth,\n    see description of the class for details\niou_threshold_long : float, default 0.5\n    the intersection threshold for segments longer than <code>long_length</code> frames (between 0 and 1),\n    see description of the class for details\niou_threshold_short : float, default 0.5\n    the intersection threshold for segments shorter than <code>short_length</code> frames (between 0 and 1),\n    see description of the class for details\nshort_length : int, default 30\n    the threshold number of frames for short intervals that will have an intersection threshold of\n    <code>iou_threshold_short</code>, see description of the class for details\nlong_length : int, default 300\n    the threshold number of frames for long intervals that will have an intersection threshold of\n    <code>iou_threshold_long</code>, see description of the class for details</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    average: str = 'macro',\n    tag_average: str = 'micro',\n    delta: int = 0,\n    smooth_interval: int = 0,\n    iou_threshold_long: float = 0.5,\n    iou_threshold_short: float = 0.5,\n    short_length: int = 30,\n    long_length: int = 300,\n    threshold_value: Union[float, List] = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.SemiSegmentalPR_AUC", "modulename": "dlc2action.metric.metrics", "qualname": "SemiSegmentalPR_AUC", "type": "class", "doc": "<p>The area under the precision-recall curve for semi-segmental metrics (not advised for training)</p>\n", "bases": "_SemiSegmentalMetric"}, {"fullname": "dlc2action.metric.metrics.SemiSegmentalPR_AUC.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "SemiSegmentalPR_AUC.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_classes : int\n    the number of classes in the dataset\nignore_index : int, default -100\n    the ground truth label to ignore\nignored_classes : set, optional\n    the class indices to ignore in computation\nexclusive : bool, default True\n    <code>False</code> for multi-label classification tasks\naverage : {\"macro\", \"micro\", \"none\"}\n    the method to average the results over classes\ntag_average : {\"macro\", \"micro\", \"none\"}\n    the method to average the results over meta tags (if given)\ndelta : int, default 0\n    the number of frames to add to each ground truth interval before computing the intersection,\n    see description of the class for details\nsmooth_interval : int, default 0\n    intervals shorter than this number of frames will be ignored (both in prediction and in ground truth,\n    see description of the class for details\niou_threshold_long : float, default 0.5\n    the intersection threshold for segments longer than <code>long_length</code> frames (between 0 and 1),\n    see description of the class for details\niou_threshold_short : float, default 0.5\n    the intersection threshold for segments shorter than <code>short_length</code> frames (between 0 and 1),\n    see description of the class for details\nshort_length : int, default 30\n    the threshold number of frames for short intervals that will have an intersection threshold of\n    <code>iou_threshold_short</code>, see description of the class for details\nlong_length : int, default 300\n    the threshold number of frames for long intervals that will have an intersection threshold of\n    <code>iou_threshold_long</code>, see description of the class for details</p>\n", "signature": "(\n    self,\n    num_classes: int,\n    ignore_index: int = -100,\n    ignored_classes: Set = None,\n    exclusive: bool = True,\n    average: str = 'macro',\n    tag_average: str = 'micro',\n    delta: int = 0,\n    smooth_interval: int = 0,\n    iou_threshold_long: float = 0.5,\n    iou_threshold_short: float = 0.5,\n    short_length: int = 30,\n    long_length: int = 300,\n    threshold_step: float = 0.1\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.Accuracy", "modulename": "dlc2action.metric.metrics", "qualname": "Accuracy", "type": "class", "doc": "<p>Accuracy</p>\n", "bases": "dlc2action.metric.base_metric.Metric"}, {"fullname": "dlc2action.metric.metrics.Accuracy.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "Accuracy.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignore_index: int\n    the class index that indicates ignored sample</p>\n", "signature": "(self, ignore_index=-100)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.Accuracy.reset", "modulename": "dlc2action.metric.metrics", "qualname": "Accuracy.reset", "type": "function", "doc": "<p>Reset the intrinsic parameters (at the beginning of an epoch)</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.Accuracy.calculate", "modulename": "dlc2action.metric.metrics", "qualname": "Accuracy.calculate", "type": "function", "doc": "<p>Calculate the metric value</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>metric : float\n    metric value</p>\n", "signature": "(self) -> float", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.Accuracy.update", "modulename": "dlc2action.metric.metrics", "qualname": "Accuracy.update", "type": "function", "doc": "<p>Update the intrinsic parameters (with a batch)</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predicted : torch.Tensor\n    the main prediction tensor generated by the model\nssl_predicted : torch.Tensor\n    the SSL prediction tensor generated by the model\ntarget : torch.Tensor\n    the corresponding main target tensor\nssl_target : torch.Tensor\n    the corresponding SSL target tensor\ntags : torch.Tensor\n    the tensor of meta tags (or <code>None</code>, if tags are not given)</p>\n", "signature": "(\n    self,\n    predicted: torch.Tensor,\n    target: torch.Tensor,\n    tags: torch.Tensor = None\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.Count", "modulename": "dlc2action.metric.metrics", "qualname": "Count", "type": "class", "doc": "<p>Fraction of samples labeled by the model as a class</p>\n", "bases": "dlc2action.metric.base_metric.Metric"}, {"fullname": "dlc2action.metric.metrics.Count.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "Count.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>classes : set\n    the set of classes to count\nexclusive: bool, default True\n    set to False for multi-label classification tasks</p>\n", "signature": "(self, classes: Set, exclusive: bool = True)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.Count.reset", "modulename": "dlc2action.metric.metrics", "qualname": "Count.reset", "type": "function", "doc": "<p>Reset the intrinsic parameters (at the beginning of an epoch)</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.Count.update", "modulename": "dlc2action.metric.metrics", "qualname": "Count.update", "type": "function", "doc": "<p>Update the intrinsic parameters (with a batch)</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predicted : torch.Tensor\n    the main prediction tensor generated by the model\nssl_predicted : torch.Tensor\n    the SSL prediction tensor generated by the model\ntarget : torch.Tensor\n    the corresponding main target tensor\nssl_target : torch.Tensor\n    the corresponding SSL target tensor\ntags : torch.Tensor\n    the tensor of meta tags (or <code>None</code>, if tags are not given)</p>\n", "signature": "(\n    self,\n    predicted: torch.Tensor,\n    target: torch.Tensor,\n    tags: torch.Tensor\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.Count.calculate", "modulename": "dlc2action.metric.metrics", "qualname": "Count.calculate", "type": "function", "doc": "<p>Calculate the metric (at the end of an epoch)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : dict\n    a dictionary where the keys are class indices and the values are class metric values</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.EditDistance", "modulename": "dlc2action.metric.metrics", "qualname": "EditDistance", "type": "class", "doc": "<p>Edit distance (not advised for training)</p>\n\n<p>Normalized by the length of the sequences</p>\n", "bases": "dlc2action.metric.base_metric.Metric"}, {"fullname": "dlc2action.metric.metrics.EditDistance.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "EditDistance.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ignore_index : int, default -100\n    the class index that indicates samples that should be ignored\nexclusive : bool, default True\n    set to False for multi-label classification tasks</p>\n", "signature": "(self, ignore_index: int = -100, exclusive: bool = True)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.EditDistance.reset", "modulename": "dlc2action.metric.metrics", "qualname": "EditDistance.reset", "type": "function", "doc": "<p>Reset the intrinsic parameters (at the beginning of an epoch)</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.EditDistance.update", "modulename": "dlc2action.metric.metrics", "qualname": "EditDistance.update", "type": "function", "doc": "<p>Update the intrinsic parameters (with a batch)</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predicted : torch.Tensor\n    the main prediction tensor generated by the model\nssl_predicted : torch.Tensor\n    the SSL prediction tensor generated by the model\ntarget : torch.Tensor\n    the corresponding main target tensor\nssl_target : torch.Tensor\n    the corresponding SSL target tensor\ntags : torch.Tensor\n    the tensor of meta tags (or <code>None</code>, if tags are not given)</p>\n", "signature": "(\n    self,\n    predicted: torch.Tensor,\n    target: torch.Tensor,\n    tags: torch.Tensor\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.EditDistance.calculate", "modulename": "dlc2action.metric.metrics", "qualname": "EditDistance.calculate", "type": "function", "doc": "<p>Calculate the metric (at the end of an epoch)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : float\n    the metric value</p>\n", "signature": "(self) -> float", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.PKU_mAP", "modulename": "dlc2action.metric.metrics", "qualname": "PKU_mAP", "type": "class", "doc": "<p>Mean average precision (segmental) (not advised for training)</p>\n", "bases": "dlc2action.metric.base_metric.Metric"}, {"fullname": "dlc2action.metric.metrics.PKU_mAP.__init__", "modulename": "dlc2action.metric.metrics", "qualname": "PKU_mAP.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    average,\n    exclusive,\n    num_classes,\n    iou_threshold=0.5,\n    threshold_value=0.5,\n    ignored_classes=None\n)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.PKU_mAP.needs_raw_data", "modulename": "dlc2action.metric.metrics", "qualname": "PKU_mAP.needs_raw_data", "type": "variable", "doc": "<p>If <code>True</code>, <code>dlc2action.task.universal_task.Task</code> will pass raw data to the metric (only primary predict \nfunction applied).\nOtherwise it will pass a prediction for the classes.</p>\n", "default_value": " = True"}, {"fullname": "dlc2action.metric.metrics.PKU_mAP.match", "modulename": "dlc2action.metric.metrics", "qualname": "PKU_mAP.match", "type": "function", "doc": "<p></p>\n", "signature": "(self, lst, ratio, ground)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.PKU_mAP.reset", "modulename": "dlc2action.metric.metrics", "qualname": "PKU_mAP.reset", "type": "function", "doc": "<p>Reset the intrinsic parameters (at the beginning of an epoch)</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.PKU_mAP.calc_pr", "modulename": "dlc2action.metric.metrics", "qualname": "PKU_mAP.calc_pr", "type": "function", "doc": "<p></p>\n", "signature": "(self, positive, proposal, ground)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.PKU_mAP.calculate", "modulename": "dlc2action.metric.metrics", "qualname": "PKU_mAP.calculate", "type": "function", "doc": "<p>Calculate the metric (at the end of an epoch)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : float | dict\n    either the single value of the metric or a dictionary where the keys are class indices and the values\n    are class metric values</p>\n", "signature": "(self) -> Union[float, Dict]", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.PKU_mAP.ap", "modulename": "dlc2action.metric.metrics", "qualname": "PKU_mAP.ap", "type": "function", "doc": "<p></p>\n", "signature": "(self, cos_map, count_map, positive, confidence)", "funcdef": "def"}, {"fullname": "dlc2action.metric.metrics.PKU_mAP.update", "modulename": "dlc2action.metric.metrics", "qualname": "PKU_mAP.update", "type": "function", "doc": "<p>Update the intrinsic parameters (with a batch)</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predicted : torch.Tensor\n    the main prediction tensor generated by the model\nssl_predicted : torch.Tensor\n    the SSL prediction tensor generated by the model\ntarget : torch.Tensor\n    the corresponding main target tensor\nssl_target : torch.Tensor\n    the corresponding SSL target tensor\ntags : torch.Tensor\n    the tensor of meta tags (or <code>None</code>, if tags are not given)</p>\n", "signature": "(\n    self,\n    predicted: torch.Tensor,\n    target: torch.Tensor,\n    tags: torch.Tensor\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.model", "modulename": "dlc2action.model", "type": "module", "doc": "<h2 id=\"models\">Models</h2>\n\n<p>The <code>dlc2action.model.base_model.Model</code> abstract class inherits from <code>torch.nn.Module</code> but additionally\nhandles automatic integration\nof SSL modules (see <code>dlc2action.ssl</code>) and enforces consistent input and output formats.</p>\n"}, {"fullname": "dlc2action.model.asformer", "modulename": "dlc2action.model.asformer", "type": "module", "doc": "<p>ASFormer</p>\n\n<p>Adapted from https://github.com/ChinaYi/ASFormer</p>\n"}, {"fullname": "dlc2action.model.asformer.ASFormer", "modulename": "dlc2action.model.asformer", "qualname": "ASFormer", "type": "class", "doc": "<p>An implementation of ASFormer</p>\n", "bases": "dlc2action.model.base_model.Model"}, {"fullname": "dlc2action.model.asformer.ASFormer.__init__", "modulename": "dlc2action.model.asformer", "qualname": "ASFormer.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ssl_constructors : list, optional\n    a list of SSL constructors that build the necessary SSL modules\nssl_modules : list, optional\n    a list of torch.nn.Module instances that will serve as SSL modules\nssl_types : list, optional\n    a list of string SSL types\nstate_dict_path : str, optional\n    path to the model state dictionary to load\nstrict : bool, default False\n    when True, the state dictionary will only be loaded if the current and the loaded architecture are the same;\n    otherwise missing or extra keys, as well as shaoe inconsistencies, are ignored</p>\n", "signature": "(\n    self,\n    num_decoders,\n    num_layers,\n    r1,\n    r2,\n    num_f_maps,\n    input_dim,\n    num_classes,\n    channel_masking_rate,\n    state_dict_path=None,\n    ssl_constructors=None,\n    ssl_types=None,\n    ssl_modules=None\n)", "funcdef": "def"}, {"fullname": "dlc2action.model.asformer.ASFormer.features_shape", "modulename": "dlc2action.model.asformer", "qualname": "ASFormer.features_shape", "type": "function", "doc": "<p>Get the shape of feature extractor output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>feature_shape : torch.Size\n    shape of feature extractor output</p>\n", "signature": "(self) -> torch.Size", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model", "modulename": "dlc2action.model.base_model", "type": "module", "doc": "<p>Abstract parent class for models used in <code>dlc2action.task.universal_task.Task</code></p>\n"}, {"fullname": "dlc2action.model.base_model.Model", "modulename": "dlc2action.model.base_model", "qualname": "Model", "type": "class", "doc": "<p>Base class for all models</p>\n\n<p>Manages interaction of base model and SSL modules + ensures consistent input and output format</p>\n", "bases": "torch.nn.modules.module.Module, abc.ABC"}, {"fullname": "dlc2action.model.base_model.Model.__init__", "modulename": "dlc2action.model.base_model", "qualname": "Model.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ssl_constructors : list, optional\n    a list of SSL constructors that build the necessary SSL modules\nssl_modules : list, optional\n    a list of torch.nn.Module instances that will serve as SSL modules\nssl_types : list, optional\n    a list of string SSL types\nstate_dict_path : str, optional\n    path to the model state dictionary to load\nstrict : bool, default False\n    when True, the state dictionary will only be loaded if the current and the loaded architecture are the same;\n    otherwise missing or extra keys, as well as shaoe inconsistencies, are ignored</p>\n", "signature": "(\n    self,\n    ssl_constructors: List = None,\n    ssl_modules: List = None,\n    ssl_types: List = None,\n    state_dict_path: str = None,\n    strict: bool = False,\n    prompt_function: Callable = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.process_labels", "modulename": "dlc2action.model.base_model", "qualname": "Model.process_labels", "type": "variable", "doc": "<p></p>\n", "default_value": " = False"}, {"fullname": "dlc2action.model.base_model.Model.freeze_feature_extractor", "modulename": "dlc2action.model.base_model", "qualname": "Model.freeze_feature_extractor", "type": "function", "doc": "<p>Freeze the parameters of the feature extraction module</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.unfreeze_feature_extractor", "modulename": "dlc2action.model.base_model", "qualname": "Model.unfreeze_feature_extractor", "type": "function", "doc": "<p>Unfreeze the parameters of the feature extraction module</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.load_state_dict", "modulename": "dlc2action.model.base_model", "qualname": "Model.load_state_dict", "type": "function", "doc": "<p>Load a model state dictionary</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>state_dict : str\n    the path to the saved state dictionary\nstrict : bool, default True\n    when True, the state dictionary will only be loaded if the current and the loaded architecture are the same;\n    otherwise missing or extra keys, as well as shaoe inconsistencies, are ignored</p>\n", "signature": "(self, state_dict: str, strict: bool = True) -> None", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.ssl_off", "modulename": "dlc2action.model.base_model", "qualname": "Model.ssl_off", "type": "function", "doc": "<p>Turn SSL off (SSL output will not be computed by the forward function)</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.ssl_on", "modulename": "dlc2action.model.base_model", "qualname": "Model.ssl_on", "type": "function", "doc": "<p>Turn SSL on (SSL output will be computed by the forward function)</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.main_task_on", "modulename": "dlc2action.model.base_model", "qualname": "Model.main_task_on", "type": "function", "doc": "<p>Turn main task training on</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.main_task_off", "modulename": "dlc2action.model.base_model", "qualname": "Model.main_task_off", "type": "function", "doc": "<p>Turn main task training on</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.set_ssl", "modulename": "dlc2action.model.base_model", "qualname": "Model.set_ssl", "type": "function", "doc": "<p>Set the SSL modules</p>\n", "signature": "(\n    self,\n    ssl_constructors: List = None,\n    ssl_types: List = None,\n    ssl_modules: List = None\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.features_shape", "modulename": "dlc2action.model.base_model", "qualname": "Model.features_shape", "type": "function", "doc": "<p>Get the shape of feature extractor output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>feature_shape : torch.Size\n    shape of feature extractor output</p>\n", "signature": "(self) -> torch.Size", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.extract_features", "modulename": "dlc2action.model.base_model", "qualname": "Model.extract_features", "type": "function", "doc": "<p>Apply the feature extraction modules consecutively</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>x : torch.Tensor\n    the input tensor\nstart : int, default 0\n    the index of the feature extraction module to start with</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>output : torch.Tensor\n    the output tensor</p>\n", "signature": "(self, x, start=0)", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.Model.forward", "modulename": "dlc2action.model.base_model", "qualname": "Model.forward", "type": "function", "doc": "<p>Generate a prediction for x</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>x : torch.Tensor\n    the main input\nssl_xs : list\n    a list of SSL input tensors\ntag : any, optional\n    a meta information tag</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>prediction : torch.Tensor\n    prediction for the main input\nssl_out : list\n    a list of SSL prediction tensors</p>\n", "signature": "(\n    self,\n    x: torch.Tensor,\n    ssl_xs: list,\n    tag: torch.Tensor = None\n) -> Tuple[torch.Tensor, list]", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.LoadedModel", "modulename": "dlc2action.model.base_model", "qualname": "LoadedModel", "type": "class", "doc": "<p>A class to generate a Model instance from a torch.nn.Module</p>\n", "bases": "Model"}, {"fullname": "dlc2action.model.base_model.LoadedModel.__init__", "modulename": "dlc2action.model.base_model", "qualname": "LoadedModel.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>model : torch.nn.Module\n    a model with a forward function that takes a single tensor as input and returns a single tensor as output</p>\n", "signature": "(self, model: torch.nn.modules.module.Module, **kwargs)", "funcdef": "def"}, {"fullname": "dlc2action.model.base_model.LoadedModel.ssl_types", "modulename": "dlc2action.model.base_model", "qualname": "LoadedModel.ssl_types", "type": "variable", "doc": "<p></p>\n", "default_value": " = ['none']"}, {"fullname": "dlc2action.model.base_model.LoadedModel.ssl_on", "modulename": "dlc2action.model.base_model", "qualname": "LoadedModel.ssl_on", "type": "function", "doc": "<p>Turn SSL on (SSL output will be computed by the forward function)</p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.model.c2f_tcn", "modulename": "dlc2action.model.c2f_tcn", "type": "module", "doc": "<p>C2F-TCN</p>\n\n<p>Adapted from https://github.com/dipika-singhania/C2F-TCN</p>\n"}, {"fullname": "dlc2action.model.c2f_tcn.C2F_TCN", "modulename": "dlc2action.model.c2f_tcn", "qualname": "C2F_TCN", "type": "class", "doc": "<p>An implementation of C2F-TCN</p>\n\n<p>Requires the <code>\"general/len_segment\"</code> parameter to be at least 512</p>\n", "bases": "dlc2action.model.base_model.Model"}, {"fullname": "dlc2action.model.c2f_tcn.C2F_TCN.__init__", "modulename": "dlc2action.model.c2f_tcn", "qualname": "C2F_TCN.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ssl_constructors : list, optional\n    a list of SSL constructors that build the necessary SSL modules\nssl_modules : list, optional\n    a list of torch.nn.Module instances that will serve as SSL modules\nssl_types : list, optional\n    a list of string SSL types\nstate_dict_path : str, optional\n    path to the model state dictionary to load\nstrict : bool, default False\n    when True, the state dictionary will only be loaded if the current and the loaded architecture are the same;\n    otherwise missing or extra keys, as well as shaoe inconsistencies, are ignored</p>\n", "signature": "(\n    self,\n    num_classes,\n    input_dims,\n    num_f_maps=128,\n    feature_dim=None,\n    state_dict_path=None,\n    ssl_constructors=None,\n    ssl_types=None,\n    ssl_modules=None\n)", "funcdef": "def"}, {"fullname": "dlc2action.model.c2f_tcn.C2F_TCN.features_shape", "modulename": "dlc2action.model.c2f_tcn", "qualname": "C2F_TCN.features_shape", "type": "function", "doc": "<p>Get the shape of feature extractor output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>feature_shape : torch.Size\n    shape of feature extractor output</p>\n", "signature": "(self) -> Optional[torch.Size]", "funcdef": "def"}, {"fullname": "dlc2action.model.edtcn", "modulename": "dlc2action.model.edtcn", "type": "module", "doc": "<p>EDTCN</p>\n\n<p>Adapted from https://github.com/yiskw713/asrf/blob/main/libs/models/tcn.py</p>\n"}, {"fullname": "dlc2action.model.edtcn.EDTCN", "modulename": "dlc2action.model.edtcn", "qualname": "EDTCN", "type": "class", "doc": "<p>An implementation of EDTCN (Endoder-Decoder TCN)</p>\n", "bases": "dlc2action.model.base_model.Model"}, {"fullname": "dlc2action.model.edtcn.EDTCN.__init__", "modulename": "dlc2action.model.edtcn", "qualname": "EDTCN.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ssl_constructors : list, optional\n    a list of SSL constructors that build the necessary SSL modules\nssl_modules : list, optional\n    a list of torch.nn.Module instances that will serve as SSL modules\nssl_types : list, optional\n    a list of string SSL types\nstate_dict_path : str, optional\n    path to the model state dictionary to load\nstrict : bool, default False\n    when True, the state dictionary will only be loaded if the current and the loaded architecture are the same;\n    otherwise missing or extra keys, as well as shaoe inconsistencies, are ignored</p>\n", "signature": "(\n    self,\n    num_classes,\n    input_dims,\n    kernel_size,\n    mid_channels,\n    feature_dim=None,\n    state_dict_path=None,\n    ssl_constructors=None,\n    ssl_types=None,\n    ssl_modules=None\n)", "funcdef": "def"}, {"fullname": "dlc2action.model.edtcn.EDTCN.features_shape", "modulename": "dlc2action.model.edtcn", "qualname": "EDTCN.features_shape", "type": "function", "doc": "<p>Get the shape of feature extractor output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>feature_shape : torch.Size\n    shape of feature extractor output</p>\n", "signature": "(self) -> torch.Size", "funcdef": "def"}, {"fullname": "dlc2action.model.mlp", "modulename": "dlc2action.model.mlp", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dlc2action.model.mlp.MLP", "modulename": "dlc2action.model.mlp", "qualname": "MLP", "type": "class", "doc": "<p>A Multi-Layer Perceptron</p>\n", "bases": "dlc2action.model.base_model.Model"}, {"fullname": "dlc2action.model.mlp.MLP.__init__", "modulename": "dlc2action.model.mlp", "qualname": "MLP.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ssl_constructors : list, optional\n    a list of SSL constructors that build the necessary SSL modules\nssl_modules : list, optional\n    a list of torch.nn.Module instances that will serve as SSL modules\nssl_types : list, optional\n    a list of string SSL types\nstate_dict_path : str, optional\n    path to the model state dictionary to load\nstrict : bool, default False\n    when True, the state dictionary will only be loaded if the current and the loaded architecture are the same;\n    otherwise missing or extra keys, as well as shaoe inconsistencies, are ignored</p>\n", "signature": "(\n    self,\n    f_maps_list,\n    input_dims,\n    num_classes,\n    dropout_rates=None,\n    state_dict_path=None,\n    ssl_constructors=None,\n    ssl_types=None,\n    ssl_modules=None\n)", "funcdef": "def"}, {"fullname": "dlc2action.model.mlp.MLP.features_shape", "modulename": "dlc2action.model.mlp", "qualname": "MLP.features_shape", "type": "function", "doc": "<p>Get the shape of feature extractor output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>feature_shape : torch.Size\n    shape of feature extractor output</p>\n", "signature": "(self) -> torch.Size", "funcdef": "def"}, {"fullname": "dlc2action.model.transformer", "modulename": "dlc2action.model.transformer", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dlc2action.model.transformer.Transformer", "modulename": "dlc2action.model.transformer", "qualname": "Transformer", "type": "class", "doc": "<p>A modification of Transformer-Encoder with additional max-pooling and upsampling</p>\n\n<p>Set <code>num_pool</code> to 0 to get a standart transformer-encoder.</p>\n", "bases": "dlc2action.model.base_model.Model"}, {"fullname": "dlc2action.model.transformer.Transformer.__init__", "modulename": "dlc2action.model.transformer", "qualname": "Transformer.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>ssl_constructors : list, optional\n    a list of SSL constructors that build the necessary SSL modules\nssl_modules : list, optional\n    a list of torch.nn.Module instances that will serve as SSL modules\nssl_types : list, optional\n    a list of string SSL types\nstate_dict_path : str, optional\n    path to the model state dictionary to load\nstrict : bool, default False\n    when True, the state dictionary will only be loaded if the current and the loaded architecture are the same;\n    otherwise missing or extra keys, as well as shaoe inconsistencies, are ignored</p>\n", "signature": "(\n    self,\n    N,\n    heads,\n    num_f_maps,\n    input_dim,\n    num_classes,\n    num_pool,\n    add_batchnorm=False,\n    feature_dim=None,\n    state_dict_path=None,\n    ssl_constructors=None,\n    ssl_types=None,\n    ssl_modules=None\n)", "funcdef": "def"}, {"fullname": "dlc2action.model.transformer.Transformer.features_shape", "modulename": "dlc2action.model.transformer", "qualname": "Transformer.features_shape", "type": "function", "doc": "<p>Get the shape of feature extractor output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>feature_shape : torch.Size\n    shape of feature extractor output</p>\n", "signature": "(self) -> torch.Size", "funcdef": "def"}, {"fullname": "dlc2action.options", "modulename": "dlc2action.options", "type": "module", "doc": "<p>Here all option dictionaries are stored</p>\n"}, {"fullname": "dlc2action.parameters", "modulename": "dlc2action.parameters", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dlc2action.project", "modulename": "dlc2action.project", "type": "module", "doc": "<h2 id=\"project-interface\">Project interface</h2>\n\n<p>The most convenient way to use <code>dlc2action</code> is through the high-level project interface. It is defined in the\n<code>project</code> module and its main functions are managing configuration files and keeping track of experiments.\nWhen you create a <code>project.Project</code> instance with a previously unused name, it generates a new project folder with results,\nhistory and configuration files.</p>\n\n<pre><code>.\nproject_name\n\u251c\u2500\u2500 config\n\u251c\u2500\u2500 meta\n\u251c\u2500\u2500 saved_datasets\n\u2514\u2500\u2500 results\n    \u251c\u2500\u2500 logs\n    \u2502   \u2514\u2500\u2500 episode.txt\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 episode\n    \u2502       \u251c\u2500\u2500 epoch25.pt\n    \u2502       \u2514\u2500\u2500 epoch50.pt\n    \u251c\u2500\u2500 searches\n    \u2502   \u2514\u2500\u2500 search\n    \u2502       \u251c\u2500\u2500 search_param_importances.html_docs\n    \u2502       \u2514\u2500\u2500 search_contour.html_docs\n    \u251c\u2500\u2500 splits\n    \u2502       \u251c\u2500\u2500 time_25.0%validation_10.0%test.txt\n    \u2502       \u2514\u2500\u2500 random_20.0%validation_10.0%test.txt\n    \u251c\u2500\u2500 suggestions\n    \u2502       \u2514\u2500\u2500 active_learning\n    \u2502           \u251c\u2500\u2500 video1_suggestion.pickle\n    \u2502           \u251c\u2500\u2500 video2_suggestion.pickle\n    \u2502           \u2514\u2500\u2500 al_points.pickle\n    \u2514\u2500\u2500 predictions\n            \u251c\u2500\u2500 episode_epoch25.pickle\n            \u2514\u2500\u2500 episode_epoch50_newdata.pickle\n</code></pre>\n\n<p>Here is an explanation of this structure.</p>\n\n<p>The <strong>config</strong> folder contains .yaml configuration files. Project instances can read them into a parameter dictionary\nand update. Those readers understand several blanks for certain parameters that can be inferred from the data on\nruntime:</p>\n\n<ul>\n<li><code>'dataset_features'</code> will be replaced with the shape of features per frame in the data,</li>\n<li><code>'dataset_classes'</code> will be replaced with the number of classes,</li>\n<li><code>'dataset_inverse_weights'</code> at losses.yaml will be replaced with a list of float values that are inversely</li>\n<li><code>'dataset_len_segment'</code> will be replaced with the length of segment in the data,</li>\n<li><code>'model_features'</code> will be replaced with the shape of features per frame in the model feature extraction\noutput (the input to SSL modules).\nproportional to the number of frames labeled with the corresponding classes.</li>\n</ul>\n\n<p>Pickled history files go in the <strong>meta</strong> folder. They are all pandas dataframes that store the relevant task\nparameters, a summary of experiment results (where applicable) and some meta information, like additional\nparameters or the time when the record was added. There are separate files for the history of training episodes,\nhyperparameter searches, predictions, saved datasets and active learning file generations. The classes that handle\nthose files are defined at the <code>meta</code> module.</p>\n\n<p>When a dataset is generated (the features are extracted and cut), it is saved in the <strong>saved_datasets</strong> folder. Every\ntime you create a new task, Project will check the saved dataset records and load pre-computed features if they\nexist. You can always safely clean the datasets to save space with the remove_datasets() function.</p>\n\n<p>Everything else is stored in the <em>results</em> folder. The text training log files go into the <strong>logs</strong> subfolder. Model\ncheckpoints (with <code>'model_state_dict'</code>, <code>'optimizer_state_dict'</code> and <code>'epoch'</code> keys) are saved in the <strong>models</strong>\nsubfolder. The main results of hyperparameter searches (best parameters and best values) are kept in the meta files\nbut they also generate html_docs plots that can be accessed in the <strong>searches</strong> subfolder. Split text files can be found\nin the <strong>splits</strong> subfolder. They are also checked every time you create a task and if a split with the same\nparameters already exists it will be loaded. Active learning files are saved in the <strong>suggestions</strong> subfolder.\nSuggestions for each video are named <em>{video_id}_suggestion.pickle</em> and the active learning file is always\n<em>al_points.pickle</em>. Finally, prediction files (pickled dictionaries) are stored in the <strong>predictions</strong> subfolder.</p>\n"}, {"fullname": "dlc2action.project.meta", "modulename": "dlc2action.project.meta", "type": "module", "doc": "<p>Handling meta (history) files</p>\n"}, {"fullname": "dlc2action.project.meta.Run", "modulename": "dlc2action.project.meta", "qualname": "Run", "type": "class", "doc": "<p>A class that manages operations with a single episode record</p>\n"}, {"fullname": "dlc2action.project.meta.Run.__init__", "modulename": "dlc2action.project.meta", "qualname": "Run.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode\nmeta_path : str, optional\n    the path to the pickled SavedRuns dataframe\nparams : dict, optional\n    alternative to meta_path: pre-loaded pandas Series of episode parameters</p>\n", "signature": "(\n    self,\n    episode_name: str,\n    project_path: str,\n    meta_path: str = None,\n    params: Dict = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.training_time", "modulename": "dlc2action.project.meta", "qualname": "Run.training_time", "type": "function", "doc": "<p>Get the training time in seconds</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>training_time : int\n    the training time in seconds</p>\n", "signature": "(self) -> int", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.model_file", "modulename": "dlc2action.project.meta", "qualname": "Run.model_file", "type": "function", "doc": "<p>Get a checkpoint file path</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>project_path : str\n    the current project folder path\nload_epoch : int, optional\n    the epoch to load (the closest checkpoint will be chosen; if not given will be set to last)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>checkpoint_path : str\n    the path to the checkpoint</p>\n", "signature": "(self, load_epoch: int = None) -> str", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.dataset_name", "modulename": "dlc2action.project.meta", "qualname": "Run.dataset_name", "type": "function", "doc": "<p>Get the dataset name</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dataset_name : str\n    the name of the dataset record</p>\n", "signature": "(self) -> str", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.split_file", "modulename": "dlc2action.project.meta", "qualname": "Run.split_file", "type": "function", "doc": "<p>Get the split file</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>split_path : str\n    the path to the split file</p>\n", "signature": "(self) -> str", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.log_file", "modulename": "dlc2action.project.meta", "qualname": "Run.log_file", "type": "function", "doc": "<p>Get the log file</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>log_path : str\n    the path to the log file</p>\n", "signature": "(self) -> str", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.split_info", "modulename": "dlc2action.project.meta", "qualname": "Run.split_info", "type": "function", "doc": "<p>Get the train/test/val split information</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>split_info : dict\n    a dictionary with [val_frac, test_frac, partition_method] keys and corresponding values</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.same_split_info", "modulename": "dlc2action.project.meta", "qualname": "Run.same_split_info", "type": "function", "doc": "<p>Check whether this episode has the same split information</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>split_info : dict\n    a dictionary with [val_frac, test_frac, partition_method] keys and corresponding values from another episode</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : bool\n    if True, this episode has the same split information</p>\n", "signature": "(self, split_info: Dict) -> bool", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.get_metrics", "modulename": "dlc2action.project.meta", "qualname": "Run.get_metrics", "type": "function", "doc": "<p>Get a list of metric names in the episode log</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>metrics : List\n    a list of string metric names</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.get_metric_log", "modulename": "dlc2action.project.meta", "qualname": "Run.get_metric_log", "type": "function", "doc": "<p>Get the metric log</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>mode : {'train', 'val'}\n    the mode to get the log from\nmetric_name : str\n    the metric to get the log for (has to be one of the metric computed for this episode during training)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>log : np.ndarray\n    the log of metric values (empty if the metric was not computed during training)</p>\n", "signature": "(self, mode: str, metric_name: str) -> numpy.ndarray", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.get_epoch_list", "modulename": "dlc2action.project.meta", "qualname": "Run.get_epoch_list", "type": "function", "doc": "<p>Get a list of epoch indices</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>epoch_list : list\n    a list of int epoch indices</p>\n", "signature": "(self, mode) -> List", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.unfinished", "modulename": "dlc2action.project.meta", "qualname": "Run.unfinished", "type": "function", "doc": "<p>Check whether this episode was interrupted</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : bool\n    True if the number of epochs in the log file is smaller than in the parameters</p>\n", "signature": "(self) -> bool", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.get_class_ind", "modulename": "dlc2action.project.meta", "qualname": "Run.get_class_ind", "type": "function", "doc": "<p>Get the integer label from a class name</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>class_name : str\n    the name of the class</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>class_ind : int\n    the integer label</p>\n", "signature": "(self, class_name: str) -> int", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.get_behaviors_dict", "modulename": "dlc2action.project.meta", "qualname": "Run.get_behaviors_dict", "type": "function", "doc": "<p>Get behaviors dictionary in the episode</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>behaviors_dict : dict\n    a dictionary with class indices as keys and labels as values</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Run.get_num_classes", "modulename": "dlc2action.project.meta", "qualname": "Run.get_num_classes", "type": "function", "doc": "<p>Get number of classes in episode</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>num_classes : int\n    the number of classes</p>\n", "signature": "(self) -> int", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.DecisionThresholds", "modulename": "dlc2action.project.meta", "qualname": "DecisionThresholds", "type": "class", "doc": "<p>A class that saves and looks up tuned decision thresholds</p>\n"}, {"fullname": "dlc2action.project.meta.DecisionThresholds.__init__", "modulename": "dlc2action.project.meta", "qualname": "DecisionThresholds.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>path : str\n    the path to the pickled SavedRuns dataframe</p>\n", "signature": "(self, path: str)", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.DecisionThresholds.save_thresholds", "modulename": "dlc2action.project.meta", "qualname": "DecisionThresholds.save_thresholds", "type": "function", "doc": "<p>Add a new record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode\nepoch : int\n    the epoch index\nmetric_name : str\n    the name of the metric the thresholds were tuned on\nmetric_parameters : dict\n    the metric parameter dictionary\nthresholds : list\n    a list of float decision thresholds</p>\n", "signature": "(\n    self,\n    episode_names: List,\n    epochs: List,\n    metric_name: str,\n    metric_parameters: Dict,\n    thresholds: List\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.DecisionThresholds.find_thresholds", "modulename": "dlc2action.project.meta", "qualname": "DecisionThresholds.find_thresholds", "type": "function", "doc": "<p>Find a record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode\nepoch : int\n    the epoch index\nmetric_name : str\n    the name of the metric the thresholds were tuned on\nmetric_parameters : dict\n    the metric parameter dictionary</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>thresholds : list\n    a list of float decision thresholds</p>\n", "signature": "(\n    self,\n    episode_names: List,\n    epochs: List,\n    metric_name: str,\n    metric_parameters: Dict\n) -> Optional[List]", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns", "type": "class", "doc": "<p>A class that manages operations with all episode (or prediction) records</p>\n"}, {"fullname": "dlc2action.project.meta.SavedRuns.__init__", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>path : str\n    the path to the pickled SavedRuns dataframe</p>\n", "signature": "(self, path: str, project_path: str)", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.update", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.update", "type": "function", "doc": "<p>Update with new data</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data : pd.DataFrame\n    the new dataframe\ndata_path : str\n    the new data path\nannotation_path : str\n    the new annotation path\nname_map : dict, optional\n    the name change dictionary; keys are old episode names and values are new episode names\nforce : bool, default False\n    replace existing episodes if <code>True</code></p>\n", "signature": "(\n    self,\n    data: pandas.core.frame.DataFrame,\n    data_path: str,\n    annotation_path: str,\n    name_map: Dict = None,\n    force: bool = False\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.get_subset", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.get_subset", "type": "function", "doc": "<p>Get a subset of the raw metadata</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : list\n    a list of the episodes to include</p>\n", "signature": "(self, episode_names: List) -> pandas.core.frame.DataFrame", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.get_saved_data_path", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.get_saved_data_path", "type": "function", "doc": "<p>Get the <code>saved_data_path</code> parameter for the episode</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>saved_data_path : str\n    the saved data path</p>\n", "signature": "(self, episode_name: str) -> str", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.check_name_validity", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.check_name_validity", "type": "function", "doc": "<p>Check if an episode name already exists</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name to check</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : bool\n    True if the name can be used</p>\n", "signature": "(self, episode_name: str) -> bool", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.update_episode_metrics", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.update_episode_metrics", "type": "function", "doc": "<p>Update meta data with evaluation results</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode to update\nmetrics : dict\n    a dictionary of the metrics</p>\n", "signature": "(self, episode_name: str, metrics: Dict) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.save_episode", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.save_episode", "type": "function", "doc": "<p>Save a new run record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode\nparameters : dict\n    the parameters to save\nbehaviors_dict : dict\n    the dictionary of behaviors (keys are indices, values are names)\nsuppress_validation : bool, optional False\n    if True, existing episode with the same name will be overwritten\ntraining_time : str, optional\n    the training time in '%H:%M:%S' format</p>\n", "signature": "(\n    self,\n    episode_name: str,\n    parameters: Dict,\n    behaviors_dict: Dict,\n    suppress_validation: bool = False,\n    training_time: str = None\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.load_parameters", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.load_parameters", "type": "function", "doc": "<p>Load the task parameters from a record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode to load</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>parameters : dict\n    the loaded task parameters</p>\n", "signature": "(self, episode_name: str) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.get_active_datasets", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.get_active_datasets", "type": "function", "doc": "<p>Get a list of names of datasets that are used by unfinished episodes</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>active_datasets : list\n    a list of dataset names used by unfinished episodes</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.list_episodes", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.list_episodes", "type": "function", "doc": "<p>Get a filtered pandas dataframe with episode metadata</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : List\n    a list of strings of episode names\nvalue_filter : str\n    a string of filters to apply of this general structure:\n    'group_name1/par_name1::(&lt;&gt;=)value1,group_name2/par_name2::(&lt;&gt;=)value2', e.g.\n    'data/overlap::=50,results/recall::&gt;0.5,data/feature_extraction::=kinematic'\ndisplay_parameters : List\n    list of parameters to display (e.g. ['data/overlap', 'results/recall'])</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>pandas.DataFrame\n    the filtered dataframe</p>\n", "signature": "(\n    self,\n    episode_names: List = None,\n    value_filter: str = '',\n    display_parameters: List = None\n) -> pandas.core.frame.DataFrame", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.rename_episode", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.rename_episode", "type": "function", "doc": "<p></p>\n", "signature": "(self, episode_name, new_episode_name)", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.remove_episode", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.remove_episode", "type": "function", "doc": "<p>Remove all model, logs and metafile records related to an episode</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode to remove</p>\n", "signature": "(self, episode_name: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.unfinished_episodes", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.unfinished_episodes", "type": "function", "doc": "<p>Get a list of unfinished episodes (currently running or interrupted)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>interrupted_episodes: List\n    a list of string names of unfinished episodes in the records</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.update_episode_results", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.update_episode_results", "type": "function", "doc": "<p>Add results to an episode record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode to update\nlogs : dict\n    a log dictionary from task.train()\ntraining_time : str\n    the training time</p>\n", "signature": "(self, episode_name: str, logs: Tuple, training_time: str = None) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedRuns.get_runs", "modulename": "dlc2action.project.meta", "qualname": "SavedRuns.get_runs", "type": "function", "doc": "<p>Get a list of runs with this episode name (episodes like <code>episode_name::0</code>)</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>runs_list : List\n    a list of string run names</p>\n", "signature": "(self, episode_name: str) -> List", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Searches", "modulename": "dlc2action.project.meta", "qualname": "Searches", "type": "class", "doc": "<p>A class that manages operations with search records</p>\n", "bases": "SavedRuns"}, {"fullname": "dlc2action.project.meta.Searches.save_search", "modulename": "dlc2action.project.meta", "qualname": "Searches.save_search", "type": "function", "doc": "<p>Save a new search record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>search_name : str\n    the name of the search to save\nparameters : dict\n    the task parameters to save\nn_trials : int\n    the number of trials in the search\nbest_params : dict\n    the best parameters dictionary\nbest_value : float\n    the best valie\nmetric : str\n    the name of the objective metric\nsearch_space : dict\n    a dictionary representing the search space; of this general structure:\n    {'group/param_name': ('float/int/float_log/int_log', start, end),\n    'group/param_name': ('categorical', [choices])}, e.g.\n    {'data/overlap': ('int', 5, 100), 'training/lr': ('float_log', 1e-4, 1e-2),\n    'data/feature_extraction': ('categorical', ['kinematic', 'bones'])}</p>\n", "signature": "(\n    self,\n    search_name: str,\n    parameters: Dict,\n    n_trials: int,\n    best_params: Dict,\n    best_value: float,\n    metric: str,\n    search_space: Dict\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Searches.get_best_params_raw", "modulename": "dlc2action.project.meta", "qualname": "Searches.get_best_params_raw", "type": "function", "doc": "<p>Get the raw dictionary of best parameters found by a search</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>search_name : str\n    the name of the search</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>best_params : dict\n    a dictionary of the best parameters where the keys are in '{group}/{name}' format</p>\n", "signature": "(self, search_name: str) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Searches.get_best_params", "modulename": "dlc2action.project.meta", "qualname": "Searches.get_best_params", "type": "function", "doc": "<p>Get the best parameters from a search</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>search_name : str\n    the name of the search\nload_parameters : List, optional\n    a list of string names of the parameters to load (if not provided all parameters are loaded)\nround_to_binary : List, optional\n    a list of string names of the loaded parameters that should be rounded to the nearest power of two</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>best_params : dict\n    a dictionary of the best parameters</p>\n", "signature": "(\n    self,\n    search_name: str,\n    load_parameters: List = None,\n    round_to_binary: List = None\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.Suggestions", "modulename": "dlc2action.project.meta", "qualname": "Suggestions", "type": "class", "doc": "<p>A class that manages operations with all episode (or prediction) records</p>\n", "bases": "SavedRuns"}, {"fullname": "dlc2action.project.meta.Suggestions.save_suggestion", "modulename": "dlc2action.project.meta", "qualname": "Suggestions.save_suggestion", "type": "function", "doc": "<p></p>\n", "signature": "(self, episode_name: str, parameters: Dict, meta_parameters)", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedStores", "modulename": "dlc2action.project.meta", "qualname": "SavedStores", "type": "class", "doc": "<p>A class that manages operation with saved dataset records</p>\n"}, {"fullname": "dlc2action.project.meta.SavedStores.__init__", "modulename": "dlc2action.project.meta", "qualname": "SavedStores.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>path : str\n    the path to the pickled SavedRuns dataframe</p>\n", "signature": "(self, path)", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedStores.clear", "modulename": "dlc2action.project.meta", "qualname": "SavedStores.clear", "type": "function", "doc": "<p>Remove all datasets</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedStores.dataset_names", "modulename": "dlc2action.project.meta", "qualname": "SavedStores.dataset_names", "type": "function", "doc": "<p>Get a list of dataset names</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dataset_names : List\n    a list of string dataset names</p>\n", "signature": "(self) -> List", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedStores.remove", "modulename": "dlc2action.project.meta", "qualname": "SavedStores.remove", "type": "function", "doc": "<p>Remove some datasets</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>names : List\n    a list of string names of the datasets to delete</p>\n", "signature": "(self, names: List) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedStores.remove_dataset", "modulename": "dlc2action.project.meta", "qualname": "SavedStores.remove_dataset", "type": "function", "doc": "<p>Remove a dataset record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset_name : str\n    the name of the dataset to remove</p>\n", "signature": "(self, dataset_name: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedStores.find_name", "modulename": "dlc2action.project.meta", "qualname": "SavedStores.find_name", "type": "function", "doc": "<p>Find a record that satisfies the parameters (if it exists)</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters : dict\n    a dictionary of data parameters</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>name : str\n    the name of a record that has the same parameters (None if it does not exist; the earliest if there are\n    several)</p>\n", "signature": "(self, parameters: Dict) -> str", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedStores.save_store", "modulename": "dlc2action.project.meta", "qualname": "SavedStores.save_store", "type": "function", "doc": "<p>Save a new saved dataset record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the dataset\nparameters : dict\n    a dictionary of data parameters</p>\n", "signature": "(self, episode_name: str, parameters: Dict) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.meta.SavedStores.check_name_validity", "modulename": "dlc2action.project.meta", "qualname": "SavedStores.check_name_validity", "type": "function", "doc": "<p>Check if a store name already exists</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name to check</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : bool\n    True if the name can be used</p>\n", "signature": "(self, store_name: str) -> bool", "funcdef": "def"}, {"fullname": "dlc2action.project.project", "modulename": "dlc2action.project.project", "type": "module", "doc": "<p>Project interface</p>\n"}, {"fullname": "dlc2action.project.project.Project", "modulename": "dlc2action.project.project", "qualname": "Project", "type": "class", "doc": "<p>A class to create and maintain the project files + keep track of experiments</p>\n"}, {"fullname": "dlc2action.project.project.Project.__init__", "modulename": "dlc2action.project.project", "qualname": "Project.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>name : str\n    name of the project\ndata_type : str, optional\n    data type (run Project.data_types() to see available options; has to be provided if the project is being\n    created)\nannotation_type : str, default 'none'\n    annotation type (run Project.annotation_types() to see available options)\nprojects_path : str, optional\n    path to the projects folder (is filled with ~/DLC2Action by default)\ndata_path : str, optional\n    path to the folder containing input files for the project (has to be provided if the project is being\n    created)\nannotation_path : str, optional\n    path to the folder containing annotation files for the project\ncopy : bool, default False\n    if True, the files from annotation_path and data_path will be copied to the projects folder;\n    otherwise they will be moved</p>\n", "signature": "(\n    self,\n    name: str,\n    data_type: str = None,\n    annotation_type: str = 'none',\n    projects_path: str = None,\n    data_path: Union[str, List] = None,\n    annotation_path: Union[str, List] = None,\n    copy: bool = False\n)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.run_episode", "modulename": "dlc2action.project.project", "qualname": "Project.run_episode", "type": "function", "doc": "<p>Run an episode</p>\n\n<p>The task parameters are read from the config files and then updated with the\nparameters_update dictionary. The model can be either initialized from scratch or loaded from one of the\nprevious experiments. All parameters and results are saved in the meta files and can be accessed with the\nlist_episodes() function. The train/test/validation split is saved and loaded from a file whenever the\nsame split parameters are used. The pre-computed datasets are also saved and loaded whenever the same\ndata parameters are used.</p>\n\n<p>You can use the autostop parameters to finish training when the parameters are not improving. It will be\nstopped if the average value of <code>autostop_metric</code> over the last <code>autostop_interval</code> epochs is smaller than\nthe average over the previous <code>autostop_interval</code> epochs + <code>autostop_threshold</code>. For example, if the\ncurrent epoch is 120 and <code>autostop_interval</code> is 50, the averages over epochs 70-120 and 20-70 will be compared.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the episode name\nload_episode : str, optional\n    the (previously run) episode name to load the model from; if the episode has multiple runs,\n    the new episode will have the same number of runs, each starting with one of the pre-trained models\nparameters_update : dict, optional\n    the dictionary used to update the parameters from the config files\ntask : TaskDispatcher, optional\n    a pre-existing <code>TaskDispatcher</code> object (if provided, the method will update it instead of creating\n    a new instance)\nload_epoch : int, optional\n    the epoch to load (if load_episodes is not None); if not provided, the last epoch is used\nload_search : str, optional\n    the hyperparameter search result to load\nload_parameters : list, optional\n    a list of string names of the parameters to load from load_search (if not provided, all parameters\n    are loaded)\nround_to_binary : list, optional\n    a list of string names of the loaded parameters that should be rounded to the nearest power of two\nload_strict : bool, default True\n    if <code>False</code>, matching weights will be loaded from <code>load_episode</code> and differences in parameter name lists and\n    weight shapes will be ignored; otherwise mismatches will prompt a <code>RuntimeError</code>\nn_seeds : int, default 1\n    the number of runs to perform with different random seeds; if <code>n_seeds &gt; 1</code>, the episodes will be named\n    <code>episode_name::seed_index</code>, e.g. <code>test_episode::0</code> and <code>test_episode::1</code>\nforce : bool, default False\n    if <code>True</code> and an episode with name <code>episode_name</code> already exists, it will be overwritten (use with caution!)\nsuppress_name_check : bool, default False\n    if <code>True</code>, episode names with a double colon are allowed (please don't use this option unless you understand\n    why they are usually forbidden)\ndelete_dataset : bool, default False\n    if <code>True</code>, the dataset will be deleted after training\nmask_name : str, optional\n    the name of the real_lens to apply\nautostop_interval : int, default 50\n    the number of epochs to average the autostop metric over\nautostop_threshold : float, default 0.001\n    the autostop difference threshold\nautostop_metric : str, optional\n    the autostop metric (can be any one of the tracked metrics of <code>'loss'</code>)</p>\n", "signature": "(\n    self,\n    episode_name: str,\n    load_episode: str = None,\n    parameters_update: Dict = None,\n    task: dlc2action.task.task_dispatcher.TaskDispatcher = None,\n    load_epoch: int = None,\n    load_search: str = None,\n    load_parameters: list = None,\n    round_to_binary: list = None,\n    load_strict: bool = True,\n    n_seeds: int = 1,\n    force: bool = False,\n    suppress_name_check: bool = False,\n    delete_dataset: bool = False,\n    mask_name: str = None,\n    autostop_metric: str = None,\n    autostop_interval: int = 50,\n    autostop_threshold: float = 0.001,\n    loading_bar: bool = False,\n    trial: Tuple = None\n) -> dlc2action.task.task_dispatcher.TaskDispatcher", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.run_episodes", "modulename": "dlc2action.project.project", "qualname": "Project.run_episodes", "type": "function", "doc": "<p>Run multiple episodes in sequence (and re-use previously loaded information)</p>\n\n<p>For each episode, the task parameters are read from the config files and then updated with the\nparameter_update dictionary. The model can be either initialized from scratch or loaded from one of the\nprevious experiments. All parameters and results are saved in the meta files and can be accessed with the\nlist_episodes() function. The train/test/validation split is saved and loaded from a file whenever the\nsame split parameters are used. The pre-computed datasets are also saved and loaded whenever the same\ndata parameters are used.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : list\n    a list of strings of episode names\nload_episodes : list, optional\n    a list of strings of (previously run) episode names to load the model from; if the episode has multiple runs,\n    the new episode will have the same number of runs, each starting with one of the pre-trained models\nparameters_updates : list, optional\n    a list of dictionaries used to update the parameters from the config\nload_epochs : list, optional\n    a list of integers used to specify the epoch to load (if load_episodes is not None)\nload_searches : list, optional\n    a list of strings of hyperparameter search results to load\nload_parameters : list, optional\n    a list of lists of string names of the parameters to load from the searches\nround_to_binary : list, optional\n    a list of string names of the loaded parameters that should be rounded to the nearest power of two\nload_strict : list, optional\n    a list of boolean values specifying weight loading policy: if <code>False</code>, matching weights will be loaded from\n    the corresponding episode and differences in parameter name lists and\n    weight shapes will be ignored; otherwise mismatches will prompt a <code>RuntimeError</code> (by default <code>True</code> for\n    every episode)\nforce : bool, default False\n    if <code>True</code> and an episode name is already taken, it will be overwritten (use with caution!)\nsuppress_name_check : bool, default False\n    if <code>True</code>, episode names with a double colon are allowed (please don't use this option unless you understand\n    why they are usually forbidden)\ndelete_dataset : bool, default False\n    if <code>True</code>, the dataset will be deleted after training</p>\n", "signature": "(\n    self,\n    episode_names: List,\n    load_episodes: List = None,\n    parameters_updates: List = None,\n    load_epochs: List = None,\n    load_searches: List = None,\n    load_parameters: List = None,\n    round_to_binary: List = None,\n    load_strict: List = None,\n    force: bool = False,\n    suppress_name_check: bool = False,\n    delete_dataset: bool = False\n) -> dlc2action.task.task_dispatcher.TaskDispatcher", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.continue_episode", "modulename": "dlc2action.project.project", "qualname": "Project.continue_episode", "type": "function", "doc": "<p>Load an older episode and continue running from the latest checkpoint</p>\n\n<p>All parameters as well as the model and optimizer state dictionaries are loaded from the episode.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode to continue\nnum_epochs : int, optional\n    the new number of epochs\ntask : TaskDispatcher, optional\n    a pre-existing task; if provided, the method will update the task instead of creating a new one\n    (this might save time, mainly on dataset loading)\nresult_average_interval : int, default 5\n    the metric are averaged over the last result_average_interval to be stored in the episodes meta file\n    and displayed by list_episodes() function (the full log is still always available)\nn_runs : int, default 1\n    the number of runs to perform; if <code>n_runs &gt; 1</code>, the episodes will be named <code>episode_name::run_index</code>, e.g.\n    <code>test_episode::0</code> and <code>test_episode::1</code>\ndelete_dataset : bool, default False\n    if <code>True</code>, pre-computed features will be deleted after the run\ndevice : str, default \"cuda\"\n    the torch device to use</p>\n", "signature": "(\n    self,\n    episode_name: str,\n    num_epochs: int = None,\n    task: dlc2action.task.task_dispatcher.TaskDispatcher = None,\n    result_average_interval: int = 5,\n    n_runs: int = 1,\n    delete_dataset: bool = False,\n    device: str = 'cuda',\n    num_cpus: int = None\n) -> dlc2action.task.task_dispatcher.TaskDispatcher", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.run_default_hyperparameter_search", "modulename": "dlc2action.project.project", "qualname": "Project.run_default_hyperparameter_search", "type": "function", "doc": "<p>Run an optuna hyperparameter search with default parameters for a model</p>\n\n<p>For the vast majority of cases, optimizing the default parameters should be enough.\nCheck out <code>dlc2action.options.model_hyperparameters</code> for the lists of parameters.\nThere are also options to set overlap, test fraction and number of epochs parameters for the search without\nmodifying the project config files. However, if you want something more complex, look into\n<code>Project.run_hyperparameter_search</code>.</p>\n\n<p>The task parameters are read from the config files and updated with the parameters_update dictionary.\nThe model can be either initialized from scratch or loaded from a previously run episode.\nFor each trial, the objective metric is averaged over a few best epochs.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>search_name : str\n    the name of the search to store it in the meta files and load in run_episode\nmodel_name : str\n    the name\nmetric : str\n    the metric to maximize/minimize (see direction); if the metric has an <code>\"average\"</code> parameter and it is set to\n    <code>\"none\"</code> in the config files, it will be reset to <code>\"macro\"</code> for the search\nn_trials : int, default 20\n    the number of optimization trials to run\nbest_n : int, default 1\n    the number of epochs to average the metric; if 0, the last value is taken\nparameters_update : dict, optional\n    the parameters update dictionary\ndirection : {'maximize', 'minimize'}\n    optimization direction\nload_episode : str, optional\n    the name of the episode to load the model from\nload_epoch : int, optional\n    the epoch to load the model from (if not provided, the last checkpoint is used)\nprune : bool, default False\n    if <code>True</code>, experiments where the optimized metric is improving too slowly will be terminated\n    (with optuna HyperBand pruner)\nforce : bool, default False\n    if <code>True</code>, existing searches with the same name will be overwritten\ndelete_dataset : bool, default False\n    if <code>True</code>, pre-computed features will be deleted after each run (if the data parameters change)\ndevice : str, optional\n    cuda:{i} or cpu, if not given it is read from the default parameters</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dict\n    a dictionary of best parameters</p>\n", "signature": "(\n    self,\n    search_name: str,\n    model_name: str,\n    metric: str,\n    best_n: int = 3,\n    direction: str = 'maximize',\n    load_episode: str = None,\n    load_epoch: int = None,\n    load_strict: bool = True,\n    prune: bool = True,\n    force: bool = False,\n    delete_dataset: bool = False,\n    overlap: float = 0,\n    num_epochs: int = 50,\n    test_frac: float = 0,\n    n_trials=150,\n    device: str = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.run_hyperparameter_search", "modulename": "dlc2action.project.project", "qualname": "Project.run_hyperparameter_search", "type": "function", "doc": "<p>Run an optuna hyperparameter search</p>\n\n<p>For a simpler function that fits most use cases, check out <code>Project.run_default_hyperparameter_search()</code>.</p>\n\n<p>The task parameters are read from the config files and updated with the parameters_update dictionary.\nThe model can be either initialized from scratch or loaded from a previously run episode.\nFor each trial, the objective metric is averaged over a few best epochs.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>search_name : str\n    the name of the search to store it in the meta files and load in run_episode\nsearch_space : dict\n    a dictionary representing the search space; of this general structure:\n    {'group/param_name': ('float/int/float_log/int_log', start, end),\n    'group/param_name': ('categorical', [choices])}, e.g.\n    {'data/overlap': ('int', 5, 100), 'training/lr': ('float_log', 1e-4, 1e-2),\n    'data/feature_extraction': ('categorical', ['kinematic', 'bones'])};\nmetric : str\n    the metric to maximize/minimize (see direction)\nn_trials : int, default 20\n    the number of optimization trials to run\nbest_n : int, default 1\n    the number of epochs to average the metric; if 0, the last value is taken\nparameters_update : dict, optional\n    the parameters update dictionary\ndirection : {'maximize', 'minimize'}\n    optimization direction\nload_episode : str, optional\n    the name of the episode to load the model from\nload_epoch : int, optional\n    the epoch to load the model from (if not provided, the last checkpoint is used)\nprune : bool, default False\n    if <code>True</code>, experiments where the optimized metric is improving too slowly will be terminated\n    (with optuna HyperBand pruner)\nforce : bool, default False\n    if <code>True</code>, existing searches with the same name will be overwritten\ndelete_dataset : bool, default False\n    if <code>True</code>, pre-computed features will be deleted after each run (if the data parameters change)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dict\n    a dictionary of best parameters</p>\n", "signature": "(\n    self,\n    search_name: str,\n    search_space: Dict,\n    metric: str,\n    n_trials: int = 20,\n    best_n: int = 1,\n    parameters_update: Dict = None,\n    direction: str = 'maximize',\n    load_episode: str = None,\n    load_epoch: int = None,\n    load_strict: bool = True,\n    prune: bool = False,\n    force: bool = False,\n    delete_dataset: bool = False\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.run_prediction", "modulename": "dlc2action.project.project", "qualname": "Project.run_prediction", "type": "function", "doc": "<p>Load models from previously run episodes to generate a prediction</p>\n\n<p>The probabilities predicted by the models are averaged.\nUnless <code>submission</code> is <code>True</code>, the prediction results are saved as a pickled dictionary in the project_name/results/predictions folder\nunder the {episode_name}_{load_epoch}.pickle name. The file is a nested dictionary where the first-level\nkeys are the video ids, the second-level keys are the clip ids (like individual names) and the values\nare the prediction arrays.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>prediction_name : str\n    the name of the prediction\nepisode_names : list\n    a list of string episode names to load the models from\nload_epochs : list, optional\n    a list of integer epoch indices to load the model from; if None, the last ones are used\nparameters_update : dict, optional\n    a dictionary of parameter updates\naugment_n : int, default 10\n    the number of augmentations to average over\ndata_path : str, optional\n    the data path to run the prediction for\nmode : {'all', 'test', 'val', 'train'}\n    the subset of the data to make the prediction for (forced to 'all' if data_path is not None)\nfile_paths : set, optional\n    a set of string file paths (data with all prefixes + feature files, in any order) to run the prediction\n    for\ndelete_dataset : bool, default False\n    if <code>True</code>, pre-computed features will be deleted\nsubmission : bool, default False\n    if <code>True</code>, a MABe-22 style submission file is generated\nframe_number_map_file : str, optional\n    path to the frame number map file\nforce : bool, default False\n    if <code>True</code>, existing prediction with this name will be overwritten</p>\n", "signature": "(\n    self,\n    prediction_name: str,\n    episode_names: List,\n    load_epochs: List = None,\n    parameters_update: Dict = None,\n    augment_n: int = 10,\n    data_path: str = None,\n    mode: str = 'all',\n    file_paths: Set = None,\n    delete_dataset: bool = False,\n    submission: bool = False,\n    frame_number_map_file: str = None,\n    force: bool = False,\n    embedding: bool = False\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.evaluate_prediction", "modulename": "dlc2action.project.project", "qualname": "Project.evaluate_prediction", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    prediction_name: str,\n    parameters_update: Dict = None,\n    data_path: str = None,\n    file_paths: Set = None,\n    mode: str = None,\n    delete_dataset: bool = False\n) -> Tuple[float, dict]", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.evaluate", "modulename": "dlc2action.project.project", "qualname": "Project.evaluate", "type": "function", "doc": "<p>Load one or several models from previously run episodes to make an evaluation</p>\n\n<p>By default it will run on the test (or validation, if there is no test) subset of the project dataset.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : list\n    a list of string episode names to load the models from\nload_epochs : list, optional\n    a list of integer epoch indices to load the model from; if None, the last ones are used\naugment_n : int, default 0\n    the number of augmentations to average over\ndata_path : str, optional\n    the data path to run the prediction for\nfile_paths : set, optional\n    a set of files to run the prediction for\nmode : {'test', 'val', 'train', 'all'}\n    the subset of the data to make the prediction for (forced to 'all' if data_path is not None;\n    by default 'test' if test subset is not empty and 'val' otherwise)\nparameters_update : dict, optional\n    a dictionary with parameter updates (cannot change model parameters)\ndelete_dataset : bool, default False\n    if <code>True</code>, the dataset will be deleted</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>metric : dict\n    a dictionary of average values of metric functions</p>\n", "signature": "(\n    self,\n    episode_names: List,\n    load_epochs: List = None,\n    augment_n: int = 0,\n    data_path: str = None,\n    file_paths: Set = None,\n    mode: str = None,\n    parameters_update: Dict = None,\n    multiple_episode_policy: str = 'average',\n    delete_dataset: bool = False,\n    skip_updating_meta: bool = True\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.list_episodes", "modulename": "dlc2action.project.project", "qualname": "Project.list_episodes", "type": "function", "doc": "<p>Get a filtered pandas dataframe with episode metadata</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : list\n    a list of strings of episode names\nvalue_filter : str\n    a string of filters to apply; of this general structure:\n    'group_name1/par_name1::(</>/&lt;=/>=/=)value1,group_name2/par_name2::(</>/&lt;=/>=/=)value2', e.g.\n    'data/overlap::=50,results/recall::&gt;0.5,data/feature_extraction::=kinematic,meta/training_time::&gt;=00:00:10'\ndisplay_parameters : list\n    list of parameters to display (e.g. ['data/overlap', 'results/recall'])\nprint_results : bool, default True\n    if True, the result will be printed to standard output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>pd.DataFrame\n    the filtered dataframe</p>\n", "signature": "(\n    self,\n    episode_names: List = None,\n    value_filter: str = '',\n    display_parameters: List = None,\n    print_results: bool = True\n) -> pandas.core.frame.DataFrame", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.list_predictions", "modulename": "dlc2action.project.project", "qualname": "Project.list_predictions", "type": "function", "doc": "<p>Get a filtered pandas dataframe with prediction metadata</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : list\n    a list of strings of episode names\nvalue_filter : str\n    a string of filters to apply; of this general structure:\n    'group_name1/par_name1:(&lt;&gt;=)value1,group_name2/par_name2:(&lt;&gt;=)value2', e.g.\n    'data/overlap:=50,results/recall:&gt;0.5,data/feature_extraction:=kinematic'\ndisplay_parameters : list\n    list of parameters to display (e.g. ['data/overlap', 'results/recall'])\nprint_results : bool, default True\n    if True, the result will be printed to standard output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>pd.DataFrame\n    the filtered dataframe</p>\n", "signature": "(\n    self,\n    episode_names: List = None,\n    value_filter: str = '',\n    display_parameters: List = None,\n    print_results: bool = True\n) -> pandas.core.frame.DataFrame", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.list_searches", "modulename": "dlc2action.project.project", "qualname": "Project.list_searches", "type": "function", "doc": "<p>Get a filtered pandas dataframe with hyperparameter search metadata</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>search_names : list\n    a list of strings of search names\nvalue_filter : str\n    a string of filters to apply; of this general structure:\n    'group_name1/par_name1:(&lt;&gt;=)value1,group_name2/par_name2:(&lt;&gt;=)value2', e.g.\n    'data/overlap:=50,results/recall:&gt;0.5,data/feature_extraction:=kinematic'\ndisplay_parameters : list\n    list of parameters to display (e.g. ['data/overlap', 'results/recall'])\nprint_results : bool, default True\n    if True, the result will be printed to standard output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>pd.DataFrame\n    the filtered dataframe</p>\n", "signature": "(\n    self,\n    search_names: List = None,\n    value_filter: str = '',\n    display_parameters: List = None,\n    print_results: bool = True\n) -> pandas.core.frame.DataFrame", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.get_best_parameters", "modulename": "dlc2action.project.project", "qualname": "Project.get_best_parameters", "type": "function", "doc": "<p></p>\n", "signature": "(self, search_name: str, round_to_binary: List = None)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.list_best_parameters", "modulename": "dlc2action.project.project", "qualname": "Project.list_best_parameters", "type": "function", "doc": "<p>Get the raw dictionary of best parameters found by a search</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>search_name : str\n    the name of the search\nprint_results : bool, default True\n    if True, the result will be printed to standard output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>best_params : dict\n    a dictionary of the best parameters where the keys are in '{group}/{name}' format</p>\n", "signature": "(self, search_name: str, print_results: bool = True) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.plot_episodes", "modulename": "dlc2action.project.project", "qualname": "Project.plot_episodes", "type": "function", "doc": "<p>Plot episode training curves</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : list\n    a list of episode names to plot; to plot to episodes in one line combine them in a list\n    (e.g. ['episode1', ['episode2', 'episode3']] to plot episode2 and episode3 as one experiment)\nmetrics : list\n    a list of metric to plot\nmodes : list, optional\n    a list of modes to plot ('train' and/or 'val'; <code>['val']</code> by default)\ntitle : str, optional\n    title for the plot\nepisode_labels : list, optional\n    a list of strings used to label the curves (has to be the same length as episode_names)\nsave_path : str, optional\n    the path to save the resulting plot\nadd_hlines : list, optional\n    a list of float values (or (value, label) tuples) to mark with horizontal lines\ncolors: list, optional\n    a list of matplotlib colors\nadd_highpoint_hlines : bool, default False\n    if <code>True</code>, horizontal lines will be added at the highest value of each episode</p>\n", "signature": "(\n    self,\n    episode_names: List,\n    metrics: List,\n    modes: List = None,\n    title: str = None,\n    episode_labels: List = None,\n    save_path: str = None,\n    add_hlines: List = None,\n    epoch_limits: List = None,\n    colors: List = None,\n    add_highpoint_hlines: bool = False\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.update_parameters", "modulename": "dlc2action.project.project", "qualname": "Project.update_parameters", "type": "function", "doc": "<p>Update the parameters in the project config files</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters_update : dict, optional\n    a dictionary of parameter updates\nload_search : str, optional\n    the name of hyperparameter search results to load to config\nload_parameters : list, optional\n    a list of lists of string names of the parameters to load from the searches\nround_to_binary : list, optional\n    a list of string names of the loaded parameters that should be rounded to the nearest power of two</p>\n", "signature": "(\n    self,\n    parameters_update: Dict = None,\n    load_search: str = None,\n    load_parameters: List = None,\n    round_to_binary: List = None\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.get_summary", "modulename": "dlc2action.project.project", "qualname": "Project.get_summary", "type": "function", "doc": "<p>Get a summary of episode statistics</p>\n\n<p>If the episode has multiple runs, the statistics will be aggregated over all of them.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode\nmethod : [\"best\", \"last\"]\n    the method for choosing the epochs\naverage : int, default 1\n    the number of epochs to average over (for each run)\nmetrics : list, optional\n    a list of metrics</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>statistics : dict\n    a nested dictionary where first-level keys are metric names and second-level keys are 'mean' for the mean\n    and 'std' for the standard deviation</p>\n", "signature": "(\n    self,\n    episode_names: list,\n    method: str = 'last',\n    average: int = 1,\n    metrics: List = None\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.remove_project", "modulename": "dlc2action.project.project", "qualname": "Project.remove_project", "type": "function", "doc": "<p>Remove all project files and experiment records and results</p>\n", "signature": "(name: str, projects_path: str = None) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.remove_saved_features", "modulename": "dlc2action.project.project", "qualname": "Project.remove_saved_features", "type": "function", "doc": "<p>Remove saved pre-computed dataset files</p>\n\n<p>By default, all pre-computed features will be deleted.\nNo essential information can get lost, storing them only saves time. Be careful with deleting datasets\nwhile training or inference is happening though.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset_names : list, optional\n    a list of dataset names to delete (by default all names are added)\nexceptions : list, optional\n    a list of dataset names to not be deleted\nremove_active : bool, default False\n    if <code>False</code>, datasets used by unfinished episodes will not be deleted</p>\n", "signature": "(\n    self,\n    dataset_names: List = None,\n    exceptions: List = None,\n    remove_active: bool = False\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.remove_extra_checkpoints", "modulename": "dlc2action.project.project", "qualname": "Project.remove_extra_checkpoints", "type": "function", "doc": "<p>Remove intermediate model checkpoint files (only leave the results of the last epoch)</p>\n\n<p>By default, all intermediate checkpoints will be deleted.\nFiles in the model folder that are not associated with any record in the meta files are also deleted.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : list, optional\n    a list of episode names to clean (by default all names are added)\nexceptions : list, optional\n    a list of episode names to not clean</p>\n", "signature": "(self, episode_names: List = None, exceptions: List = None) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.remove_search", "modulename": "dlc2action.project.project", "qualname": "Project.remove_search", "type": "function", "doc": "<p>Remove a hyperparameter search record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>search_name : str\n    the name of the search to remove</p>\n", "signature": "(self, search_name: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.remove_prediction", "modulename": "dlc2action.project.project", "qualname": "Project.remove_prediction", "type": "function", "doc": "<p>Remove a prediction record</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>prediction_name : str\n    the name of the prediction to remove</p>\n", "signature": "(self, prediction_name: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.remove_episode", "modulename": "dlc2action.project.project", "qualname": "Project.remove_episode", "type": "function", "doc": "<p>Remove all model, logs and metafile records related to an episode</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode to remove</p>\n", "signature": "(self, episode_name: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.prune_unfinished", "modulename": "dlc2action.project.project", "qualname": "Project.prune_unfinished", "type": "function", "doc": "<p>Remove all interrupted episodes</p>\n\n<p>Remove all episodes that either don't have a log file or have less epochs in the log file than in\nthe training parameters or have a model folder but not a record. Note that it can remove episodes that are\ncurrently running!</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>exceptions : list\n    the episodes to keep even if they are interrupted</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>pruned : list\n    a list of the episode names that were pruned</p>\n", "signature": "(self, exceptions: List = None) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.prediction_path", "modulename": "dlc2action.project.project", "qualname": "Project.prediction_path", "type": "function", "doc": "<p>Get the path where prediction files are saved</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>prediction_name : str\n    name of the prediction</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>prediction_path : str\n    the file path</p>\n", "signature": "(self, prediction_name: str) -> str", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.print_data_types", "modulename": "dlc2action.project.project", "qualname": "Project.print_data_types", "type": "function", "doc": "<p></p>\n", "signature": "(cls)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.print_annotation_types", "modulename": "dlc2action.project.project", "qualname": "Project.print_annotation_types", "type": "function", "doc": "<p></p>\n", "signature": "(cls)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.data_types", "modulename": "dlc2action.project.project", "qualname": "Project.data_types", "type": "function", "doc": "<p>Get available data types</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>list\n    available data types</p>\n", "signature": "() -> List", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.annotation_types", "modulename": "dlc2action.project.project", "qualname": "Project.annotation_types", "type": "function", "doc": "<p>Get available annotation types</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>list\n    available annotation types</p>\n", "signature": "() -> List", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.set_main_parameters", "modulename": "dlc2action.project.project", "qualname": "Project.set_main_parameters", "type": "function", "doc": "<p>Select the model and the metrics</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model_name : str, optional\n    model name; run <code>project.help(\"model\") to find out more\nmetric_names : list, optional\n    a list of metric function names; run</code>project.help(\"metrics\") to find out more</p>\n", "signature": "(self, model_name: str = None, metric_names: List = None)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.help", "modulename": "dlc2action.project.project", "qualname": "Project.help", "type": "function", "doc": "<p>Get information on available options</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>keyword : str, optional\n    the keyword for options (run without arguments to see which keywords are available)</p>\n", "signature": "(self, keyword: str = None)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.list_blanks", "modulename": "dlc2action.project.project", "qualname": "Project.list_blanks", "type": "function", "doc": "<p>List parameters that need to be filled in</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>blanks : list, optional\n    a list of the parameters to list, if already known</p>\n", "signature": "(self, blanks=None)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.list_basic_parameters", "modulename": "dlc2action.project.project", "qualname": "Project.list_basic_parameters", "type": "function", "doc": "<p>Get a list of most relevant parameters and code to modify them</p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.count_classes", "modulename": "dlc2action.project.project", "qualname": "Project.count_classes", "type": "function", "doc": "<p>Get a dictionary of class counts in different modes</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>load_episode : str, optional\n    the episode settings to load\nparameters_update : dict, optional\n    a dictionary of parameter updates (only for \"data\" and \"general\" categories)\ndelete_dataset : bool, default False\n    if <code>True</code>, the dataset that is used for computation is then deleted\nbouts : bool, default False\n    if <code>True</code>, instead of frame counts segment counts are returned</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>class_counts : dict\n    a dictionary where first-level keys are \"train\", \"val\" and \"test\", second-level keys are\n    class names and values are class counts (in frames)</p>\n", "signature": "(\n    self,\n    load_episode: str = None,\n    parameters_update: Dict = None,\n    delete_dataset: bool = False,\n    bouts: bool = True\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.plot_class_distribution", "modulename": "dlc2action.project.project", "qualname": "Project.plot_class_distribution", "type": "function", "doc": "<p>Make a class distribution plot</p>\n\n<p>You can either specify the parameters, choose an existing dataset or do neither (in that case a dataset\nis created or laoded for the computation with the default parameters).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters_update : dict, optional\n    a dictionary of parameter updates (only for \"data\" and \"general\" categories)\ndelete_dataset : bool, default False\n    if <code>True</code>, the dataset that is used for computation is then deleted</p>\n", "signature": "(\n    self,\n    parameters_update: Dict = None,\n    frame_cutoff: int = 1,\n    bout_cutoff: int = 1,\n    print_full: bool = False,\n    delete_dataset: bool = False\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.plot_confusion_matrix", "modulename": "dlc2action.project.project", "qualname": "Project.plot_confusion_matrix", "type": "function", "doc": "<p>Make a confusion matrix plot and return the data</p>\n\n<p>If the annotation is non-exclusive, only false positive labels are considered.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode to load\nload_epoch : int, optional\n    the index of the epoch to load (by default the last one is loaded)\nparameters_update : dict, optional\n    a dictionary of parameter updates (only for \"data\" and \"general\" categories)\nmode : {'val', 'all', 'test', 'train'}\n    the subset of the data to make the prediction for (forced to 'all' if data_path is not None)\ntype : {\"recall\", \"precision\"}\n    for datasets with non-exclusive annotation, if <code>type</code> is <code>\"recall\"</code>, only false positives are taken\n    into account, and if <code>type</code> is <code>\"precision\"</code>, only false negatives\ndelete_dataset : bool, default False\n    if <code>True</code>, the dataset that is used for computation is then deleted</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>confusion_matrix : np.ndarray\n    a confusion matrix of shape <code>(#classes, #classes)</code> where <code>A[i, j] = F_ij/N_i</code>, <code>F_ij</code> is the number of\n    frames that have the i-th label in the ground truth and a false positive j-th label in the prediction,\n    <code>N_i</code> is the number of frames that have the i-th label in the ground truth\nclasses : list\n    a list of labels</p>\n", "signature": "(\n    self,\n    episode_name: str,\n    load_epoch: int = None,\n    parameters_update: Dict = None,\n    type: str = 'recall',\n    mode: str = 'val',\n    delete_dataset: bool = False\n) -> Tuple[numpy.ndarray, collections.abc.Iterable]", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.plot_predictions", "modulename": "dlc2action.project.project", "qualname": "Project.plot_predictions", "type": "function", "doc": "<p>Visualize random predictions</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode to load\nload_epoch : int, optional\n    the epoch to load (by default last)\nparameters_update : dict, optional\n    parameter update dictionary\nadd_legend : bool, default True\n    if True, legend will be added to the plot\nground_truth : bool, default True\n    if True, ground truth will be added to the plot\ncolormap : str, default 'Accent'\n    the <code>matplotlib</code> colormap to use\nhide_axes : bool, default True\n    if <code>True</code>, the axes will be hidden on the plot\nmin_classes : int, default 1\n    the minimum number of classes in a displayed interval\nwidth : float, default 10\n    the width of the plot\nwhole_video : bool, default False\n    if <code>True</code>, whole videos are plotted instead of segments\ntransparent : bool, default False\n    if <code>True</code>, the background on the plot is transparent\ndrop_classes : set, optional\n    a set of class names to not be displayed\nsearch_classes : set, optional\n    if given, only intervals where at least one of the classes is in ground truth will be shown\nnum_plots : int, default 1\n    the number of plots to make\ndelete_dataset : bool, default False\n    if <code>True</code>, the dataset will be deleted after computation\nsmooth_interval_prediction : int, default 0\n    if &gt;0, predictions shorter than this number of frames are removed (filled with prediction for the previous frame)\ndata_path : str, optional\n    the data path to run the prediction for\nmode : {'all', 'test', 'val', 'train'}\n    the subset of the data to make the prediction for (forced to 'all' if data_path is not None)\nfile_paths : set, optional\n    a set of string file paths (data with all prefixes + feature files, in any order) to run the prediction\n    for\nbehavior_name : str, optional\n    for non-exclusive classificaton datasets, choose which behavior to visualize (by default first in list)</p>\n", "signature": "(\n    self,\n    episode_name: str,\n    load_epoch: int = None,\n    parameters_update: Dict = None,\n    add_legend: bool = True,\n    ground_truth: bool = True,\n    colormap: str = 'viridis',\n    hide_axes: bool = False,\n    min_classes: int = 1,\n    width: float = 10,\n    whole_video: bool = False,\n    transparent: bool = False,\n    drop_classes: Set = None,\n    search_classes: Set = None,\n    num_plots: int = 1,\n    delete_dataset: bool = False,\n    smooth_interval_prediction: int = 0,\n    data_path: str = None,\n    file_paths: Set = None,\n    mode: str = 'val',\n    behavior_name: str = None\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.create_metadata_backup", "modulename": "dlc2action.project.project", "qualname": "Project.create_metadata_backup", "type": "function", "doc": "<p>Create a copy of the meta files</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.load_metadata_backup", "modulename": "dlc2action.project.project", "qualname": "Project.load_metadata_backup", "type": "function", "doc": "<p>Load from previously created meta data backup (in case of corruption)</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.get_behavior_dictionary", "modulename": "dlc2action.project.project", "qualname": "Project.get_behavior_dictionary", "type": "function", "doc": "<p>Get the behavior dictionary for an episode</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the name of the episode</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>behaviors_dictionary : dict\n    a dictionary where keys are label indices and values are label names</p>\n", "signature": "(self, episode_name: str) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.import_episodes", "modulename": "dlc2action.project.project", "qualname": "Project.import_episodes", "type": "function", "doc": "<p>Import episodes exported with <code>Project.export_episodes</code></p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episodes_directory : str\n    the path to the exported episodes directory\nname_map : dict\n    a name change dictionary for the episodes: keys are old names, values are new names</p>\n", "signature": "(\n    self,\n    episodes_directory: str,\n    name_map: Dict = None,\n    repeat_policy: str = 'error'\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.export_episodes", "modulename": "dlc2action.project.project", "qualname": "Project.export_episodes", "type": "function", "doc": "<p>Save selected episodes as a file that can be imported into another project with <code>Project.import_episodes</code></p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : list\n    a list of string episode names\noutput_directory : str\n    the path to the directory where the episodes will be saved\nname : str, optional\n    the name of the episodes directory (by default <code>exported_episodes</code>)</p>\n", "signature": "(\n    self,\n    episode_names: List,\n    output_directory: str,\n    name: str = None\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.get_results_table", "modulename": "dlc2action.project.project", "qualname": "Project.get_results_table", "type": "function", "doc": "<p>Genererate a <code>pandas</code> dataframe with a summary of episode results</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_names : list\n    a list of names of episodes to include\nmetrics : list, optional\n    a list of metric names to include\ninclude_std : bool, default False\n    if <code>True</code>, for episodes with multiple runs the mean and standard deviation will be displayed;\n    otherwise only mean\nclasses : list, optional\n    a list of names of classes to include (by default all are included)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>results : pd.DataFrame\n    a table with the results</p>\n", "signature": "(\n    self,\n    episode_names: List,\n    metrics: List = None,\n    include_std: bool = False,\n    classes: List = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.episode_exists", "modulename": "dlc2action.project.project", "qualname": "Project.episode_exists", "type": "function", "doc": "<p>Check if an episode already exists</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>episode_name : str\n    the episode name</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>exists : bool\n    <code>True</code> if the episode exists</p>\n", "signature": "(self, episode_name: str) -> bool", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.search_exists", "modulename": "dlc2action.project.project", "qualname": "Project.search_exists", "type": "function", "doc": "<p>Check if a search already exists</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>search_name : str\n    the search name</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>exists : bool\n    <code>True</code> if the search exists</p>\n", "signature": "(self, search_name: str) -> bool", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.prediction_exists", "modulename": "dlc2action.project.project", "qualname": "Project.prediction_exists", "type": "function", "doc": "<p>Check if a prediction already exists</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>prediction_name : str\n    the prediction name</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>exists : bool\n    <code>True</code> if the prediction exists</p>\n", "signature": "(self, prediction_name: str) -> bool", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.project_name_available", "modulename": "dlc2action.project.project", "qualname": "Project.project_name_available", "type": "function", "doc": "<p></p>\n", "signature": "(projects_path: str, project_name: str)", "funcdef": "def"}, {"fullname": "dlc2action.project.project.Project.rename_episode", "modulename": "dlc2action.project.project", "qualname": "Project.rename_episode", "type": "function", "doc": "<p></p>\n", "signature": "(self, episode_name: str, new_episode_name: str)", "funcdef": "def"}, {"fullname": "dlc2action.ssl", "modulename": "dlc2action.ssl", "type": "module", "doc": "<h2 id=\"self-supervised-tasks\">Self-supervised tasks</h2>\n\n<p>Self-supervised learning tasks in <code>dlc2action</code> are implemented in the form of the <code>base_ssl.SSLConstructor</code>\nabstract class.\nIn order to create a new task you need to define four things: a data <em>transformation</em>, a network <em>module</em>, a <em>loss\nfunction</em> and the <em>type</em> of the task that defines interaction with the base network.</p>\n\n<p>The <em>transformation</em> is applied to model input data at runtime to generate the SSL input and target from input data.\nIt receives the target data as a feature dictionary, so complex generalised transformations can be defined easily.\nThe SSL input and target should also be returned in similarly formatted dictionaries so that augmentations can be\napplied to them correctly. If you are using the <code>dlc2action.task.task_dispatcher.TaskDispatcher</code>\n(or <code>dlc2action.project.project.Project</code>) interface, they will be regarded\ndifferently according to the type of the task:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: if the input generated by transformation is None an error will be raised; if the generated target\nis None it will be replaced by the unmodified input data,</li>\n<li><code>'ssl_target'</code>: the input generated by transformation will be disregarded; if the generated target is None it will\nbe replaced by the unmodified input data,</li>\n<li><code>'contrastive'</code>, <code>'contrastive_2layers'</code>: if the transformation returns None for the input, the input will be created by the transformer as\nan augmentation of the input data; if the target is <code>None</code> it will stay <code>None</code>,</li>\n</ul>\n\n<p>You can also set these rules manually with the <code>keep_target_none</code> and <code>generate_ssl_input</code> parameters of the\ntransformers.</p>\n\n<p>The SSL <em>module</em> is stacked with the base network feature extraction module as described above. The <em>loss function</em>\ntakes SSL output and SSL target as input and returns a loss value (all operations must be differentiable by <code>torch</code>).\nThe available types are described at <code>dlc2action.ssl.base_ssl.SSLConstructor</code>.</p>\n"}, {"fullname": "dlc2action.ssl.base_ssl", "modulename": "dlc2action.ssl.base_ssl", "type": "module", "doc": "<p>Abstract class for defining SSL tasks</p>\n"}, {"fullname": "dlc2action.ssl.base_ssl.SSLConstructor", "modulename": "dlc2action.ssl.base_ssl", "qualname": "SSLConstructor", "type": "class", "doc": "<p>A base class for all SSL constructors</p>\n\n<p>An SSL method is defined by three things: a <em>transformation</em> that maps a sample into SSL input and output,\na neural net <em>module</em> that takes features as input and predicts SSL target, a <em>type</em> and a <em>loss function</em>.</p>\n", "bases": "abc.ABC"}, {"fullname": "dlc2action.ssl.base_ssl.SSLConstructor.type", "modulename": "dlc2action.ssl.base_ssl", "qualname": "SSLConstructor.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'none'"}, {"fullname": "dlc2action.ssl.base_ssl.SSLConstructor.transformation", "modulename": "dlc2action.ssl.base_ssl", "qualname": "SSLConstructor.transformation", "type": "function", "doc": "<p>Transform a sample feature dictionary into SSL input and target</p>\n\n<p>Either input, target or both can be left as <code>None</code>. Transformers can be configured to replace <code>None</code> SSL targets\nwith the input sample at runtime and/or to replace <code>None SSL</code> inputs with a new augmentation of the input sample.\nIf the keys of the feature dictionaries are recognized by the transformer, they will be augmented before\nall features are stacked together.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>sample_data : dict\n    a feature dictionary</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ssl_input : dict | torch.float('nan')\n    a feature dictionary of SSL inputs\nssl_target : dict | torch.float('nan')\n    a feature dictionary of SSL targets</p>\n", "signature": "(self, sample_data: Dict) -> Tuple[Dict, Dict]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.base_ssl.SSLConstructor.loss", "modulename": "dlc2action.ssl.base_ssl", "qualname": "SSLConstructor.loss", "type": "function", "doc": "<p>Calculate the SSL loss</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predicted : torch.Tensor\n    output of the SSL module\ntarget : torch.Tensor\n    augmented and stacked SSL_target</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss : float\n    the loss value</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.base_ssl.SSLConstructor.construct_module", "modulename": "dlc2action.ssl.base_ssl", "qualname": "SSLConstructor.construct_module", "type": "function", "doc": "<p>Construct the SSL module</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ssl_module : torch.nn.Module\n    a neural net module that takes features extracted by a model's feature extractor as input and\n    returns SSL output</p>\n", "signature": "(self) -> torch.nn.modules.module.Module", "funcdef": "def"}, {"fullname": "dlc2action.ssl.base_ssl.EmptySSL", "modulename": "dlc2action.ssl.base_ssl", "qualname": "EmptySSL", "type": "class", "doc": "<p>Empty SSL class</p>\n", "bases": "SSLConstructor"}, {"fullname": "dlc2action.ssl.base_ssl.EmptySSL.transformation", "modulename": "dlc2action.ssl.base_ssl", "qualname": "EmptySSL.transformation", "type": "function", "doc": "<p>Empty transformation</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.base_ssl.EmptySSL.loss", "modulename": "dlc2action.ssl.base_ssl", "qualname": "EmptySSL.loss", "type": "function", "doc": "<p>Empty loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.base_ssl.EmptySSL.construct_module", "modulename": "dlc2action.ssl.base_ssl", "qualname": "EmptySSL.construct_module", "type": "function", "doc": "<p>Empty module</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive", "modulename": "dlc2action.ssl.contrastive", "type": "module", "doc": "<p>Implementations of <code>dlc2action.ssl.base_ssl.SSLConstructor</code> of the <code>'contrastive'</code> type</p>\n"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveSSL", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveSSL", "type": "class", "doc": "<p>A contrastive SSL class with an NT-Xent loss</p>\n\n<p>The SSL input and target are left empty (the SSL input is generated as an augmentation of the\ninput sample at runtime).</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveSSL.__init__", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveSSL.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_f_maps : torch.Size\n    shape of feature extractor output\nlen_segment : int\n    length of segment in the base feature extractor output\nssl_features : int, default 128\n    the final number of features per clip\ntau : float, default 1\n    the tau parameter of NT-Xent loss</p>\n", "signature": "(\n    self,\n    num_f_maps: torch.Size,\n    len_segment: int,\n    ssl_features: int = 128,\n    tau: float = 1\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveSSL.type", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'contrastive'"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveSSL.transformation", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveSSL.transformation", "type": "function", "doc": "<p>Empty transformation</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveSSL.loss", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveSSL.loss", "type": "function", "doc": "<p>NT-Xent loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveSSL.construct_module", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveSSL.construct_module", "type": "function", "doc": "<p>Clip-wise feature TCN extractor</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveMaskedSSL", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveMaskedSSL", "type": "class", "doc": "<p>A contrastive masked SSL class with an NT-Xent loss</p>\n\n<p>A few frames in the middle of each segment are masked and then the output of the second layer of\nfeature extraction for the segment is used to predict the output of the first layer for the missing frames.\nThe SSL input and target are left empty (the SSL input is generated as an augmentation of the\ninput sample at runtime).</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveMaskedSSL.__init__", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveMaskedSSL.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_f_maps : torch.Size\n    shape of feature extractor output\nlen_segment : int\n    length of segment in the base feature extractor output\nssl_features : int, default 128\n    the final number of features per clip\ntau : float, default 1\n    the tau parameter of NT-Xent loss</p>\n", "signature": "(\n    self,\n    num_f_maps: torch.Size,\n    len_segment: int,\n    ssl_features: int = 128,\n    tau: float = 1,\n    num_masked: int = 10\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveMaskedSSL.type", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveMaskedSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'contrastive_2layers'"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveMaskedSSL.transformation", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveMaskedSSL.transformation", "type": "function", "doc": "<p>Empty transformation</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveMaskedSSL.loss", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveMaskedSSL.loss", "type": "function", "doc": "<p>NT-Xent loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveMaskedSSL.construct_module", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveMaskedSSL.construct_module", "type": "function", "doc": "<p>Clip-wise feature TCN extractor</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseSSL", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseSSL", "type": "class", "doc": "<p>A pairwise SSL class with triplet or circle loss</p>\n\n<p>The SSL input and target are left empty (the SSL input is generated as an augmentation of the\ninput sample at runtime).</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseSSL.__init__", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseSSL.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_f_maps : torch.Size\n    shape of feature extractor output\nlen_segment : int\n    length of segment in feature extractor output\nssl_features : int, default 128\n    final number of features per clip\nmargin : float, default 0\n    the margin parameter of triplet or circle loss\ndistance : {'cosine', 'euclidean'}\n    the distance calculation method for triplet or circle loss\nloss : {'triplet', 'circle'}\n    the loss function name\ngamma : float, default 1\n    the gamma parameter of circle loss</p>\n", "signature": "(\n    self,\n    num_f_maps: torch.Size,\n    len_segment: int,\n    ssl_features: int = 128,\n    margin: float = 0,\n    distance: str = 'cosine',\n    loss: str = 'triplet',\n    gamma: float = 1\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseSSL.type", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'contrastive'"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseSSL.transformation", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseSSL.transformation", "type": "function", "doc": "<p>Empty transformation</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseSSL.loss", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseSSL.loss", "type": "function", "doc": "<p>Triplet or circle loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseSSL.construct_module", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseSSL.construct_module", "type": "function", "doc": "<p>Clip-wise feature TCN extractor</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseMaskedSSL", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseMaskedSSL", "type": "class", "doc": "<p>A pairwise SSL class with triplet or circle loss</p>\n\n<p>The SSL input and target are left empty (the SSL input is generated as an augmentation of the\ninput sample at runtime).</p>\n", "bases": "PairwiseSSL"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseMaskedSSL.__init__", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseMaskedSSL.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_f_maps : torch.Size\n    shape of feature extractor output\nlen_segment : int\n    length of segment in feature extractor output\nssl_features : int, default 128\n    final number of features per clip\nmargin : float, default 0\n    the margin parameter of triplet or circle loss\ndistance : {'cosine', 'euclidean'}\n    the distance calculation method for triplet or circle loss\nloss : {'triplet', 'circle'}\n    the loss function name\ngamma : float, default 1\n    the gamma parameter of circle loss</p>\n", "signature": "(\n    self,\n    num_f_maps: torch.Size,\n    len_segment: int,\n    ssl_features: int = 128,\n    margin: float = 0,\n    distance: str = 'cosine',\n    loss: str = 'triplet',\n    gamma: float = 1,\n    num_masked: int = 10\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseMaskedSSL.type", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseMaskedSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'contrastive_2layers'"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseMaskedSSL.transformation", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseMaskedSSL.transformation", "type": "function", "doc": "<p>Empty transformation</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.PairwiseMaskedSSL.construct_module", "modulename": "dlc2action.ssl.contrastive", "qualname": "PairwiseMaskedSSL.construct_module", "type": "function", "doc": "<p>Clip-wise feature TCN extractor</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveRegressionSSL", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveRegressionSSL", "type": "class", "doc": "<p>A base class for all SSL constructors</p>\n\n<p>An SSL method is defined by three things: a <em>transformation</em> that maps a sample into SSL input and output,\na neural net <em>module</em> that takes features as input and predicts SSL target, a <em>type</em> and a <em>loss function</em>.</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveRegressionSSL.__init__", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveRegressionSSL.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    num_f_maps: torch.Size,\n    num_features: int = 128,\n    num_ssl_layers: int = 1,\n    distance: str = 'cosine',\n    temperature: float = 1,\n    break_factor: int = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveRegressionSSL.type", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveRegressionSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'contrastive'"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveRegressionSSL.loss", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveRegressionSSL.loss", "type": "function", "doc": "<p>NT-Xent loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveRegressionSSL.transformation", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveRegressionSSL.transformation", "type": "function", "doc": "<p>Empty transformation</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.contrastive.ContrastiveRegressionSSL.construct_module", "modulename": "dlc2action.ssl.contrastive", "qualname": "ContrastiveRegressionSSL.construct_module", "type": "function", "doc": "<p>Clip-wise feature TCN extractor</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked", "modulename": "dlc2action.ssl.masked", "type": "module", "doc": "<p>Implementations of <code>dlc2action.ssl.base_ssl.SSLConstructor</code> that predict masked input features</p>\n"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL", "type": "class", "doc": "<p>A base masked features SSL class</p>\n\n<p>Mask some of the input features randomly and predict the initial data.</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor, abc.ABC"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL.__init__", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>frac_masked : float\n    fraction of features to real_lens</p>\n", "signature": "(self, frac_masked: float = 0.2)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL.type", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'ssl_input'"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL.transformation", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL.transformation", "type": "function", "doc": "<p>Mask some of the features randomly</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL.loss", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL.loss", "type": "function", "doc": "<p>MSE loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL.construct_module", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL.construct_module", "type": "function", "doc": "<p>Construct the SSL prediction module using the parameters specified at initialization</p>\n", "signature": "(self) -> torch.nn.modules.module.Module", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL_FC", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL_FC", "type": "class", "doc": "<p>A fully connected masked features SSL class</p>\n\n<p>Mask some of the input features randomly and predict the initial data.</p>\n", "bases": "MaskedFeaturesSSL"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL_FC.__init__", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL_FC.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>dims : torch.Size\n    the shape of features in model input\nnum_f_maps : torch.Size\n    shape of feature extraction output\nfrac_masked : float, default 0.1\n    fraction of features to real_lens\nnum_ssl_layers : int, default 5\n    number of layers in the SSL module\nnum_ssl_f_maps : int, default 16\n    number of feature maps in the SSL module</p>\n", "signature": "(\n    self,\n    dims: torch.Size,\n    num_f_maps: torch.Size,\n    frac_masked: float = 0.2,\n    num_ssl_layers: int = 5,\n    num_ssl_f_maps: int = 16\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL_FC.construct_module", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL_FC.construct_module", "type": "function", "doc": "<p>A fully connected module</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL_TCN", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL_TCN", "type": "class", "doc": "<p>A TCN masked features SSL class</p>\n\n<p>Mask some of the input features randomly and predict the initial data.</p>\n", "bases": "MaskedFeaturesSSL"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL_TCN.__init__", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL_TCN.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>dims : torch.Size\n    the shape of features in model input\nnum_f_maps : torch.Size\n    shape of feature extraction output\nfrac_masked : float, default 0.1\n    fraction of features to real_lens\nnum_ssl_layers : int, default 5\n    number of layers in the SSL module</p>\n", "signature": "(\n    self,\n    dims: Dict,\n    num_f_maps: torch.Size,\n    frac_masked: float = 0.2,\n    num_ssl_layers: int = 5\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFeaturesSSL_TCN.construct_module", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFeaturesSSL_TCN.construct_module", "type": "function", "doc": "<p>A TCN module</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL", "type": "class", "doc": "<p>A base masked joints SSL class</p>\n\n<p>Mask some of the joints randomly and predict the initial data.</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor, abc.ABC"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL.__init__", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>frac_masked : float, default 0.1\n    fraction of features to real_lens</p>\n", "signature": "(self, frac_masked: float = 0.2)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL.type", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'ssl_input'"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL.transformation", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL.transformation", "type": "function", "doc": "<p>Mask joints randomly</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL.loss", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL.loss", "type": "function", "doc": "<p>MSE loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL.construct_module", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL.construct_module", "type": "function", "doc": "<p>Construct the SSL prediction module using the parameters specified at initialization</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL_FC", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL_FC", "type": "class", "doc": "<p>A base masked joints SSL class</p>\n\n<p>Mask some of the joints randomly and predict the initial data.</p>\n", "bases": "MaskedKinematicSSL"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL_FC.__init__", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL_FC.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>dims : torch.Size\n    the number of features in model input\nnum_f_maps : torch.Size\n    shape of feature extraction output\nfrac_masked : float, default 0.1\n    fraction of joints to real_lens\nnum_ssl_layers : int, default 5\n    number of layers in the SSL module\nnum_ssl_f_maps : int, default 16\n    number of feature maps in the SSL module</p>\n", "signature": "(\n    self,\n    dims: torch.Size,\n    num_f_maps: torch.Size,\n    frac_masked: float = 0.2,\n    num_ssl_layers: int = 5,\n    num_ssl_f_maps: int = 16\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL_FC.construct_module", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL_FC.construct_module", "type": "function", "doc": "<p>A fully connected module</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL_TCN", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL_TCN", "type": "class", "doc": "<p>A base masked joints SSL class</p>\n\n<p>Mask some of the joints randomly and predict the initial data.</p>\n", "bases": "MaskedKinematicSSL"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL_TCN.__init__", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL_TCN.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>dims : torch.Size\n    the shape of features in model input\nnum_f_maps : torch.Size\n    shape of feature extraction output\nfrac_masked : float, default 0.1\n    fraction of joints to real_lens\nnum_ssl_layers : int, default 5\n    number of layers in the SSL module</p>\n", "signature": "(\n    self,\n    dims: torch.Size,\n    num_f_maps: torch.Size,\n    frac_masked: float = 0.2,\n    num_ssl_layers: int = 5\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedKinematicSSL_TCN.construct_module", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedKinematicSSL_TCN.construct_module", "type": "function", "doc": "<p>A TCN module</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL", "type": "class", "doc": "<p>Generates the functions necessary to build a masked features SSL: real_lens some of the input features randomly\nand predict the initial data</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor, abc.ABC"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL.__init__", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>frac_masked : float, default 0.1\n    fraction of frames to real_lens</p>\n", "signature": "(self, frac_masked: float = 0.1)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL.type", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'ssl_input'"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL.transformation", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL.transformation", "type": "function", "doc": "<p>Mask some of the frames randomly</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL.loss", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL.loss", "type": "function", "doc": "<p>MSE loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL.construct_module", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL.construct_module", "type": "function", "doc": "<p>Construct the SSL prediction module using the parameters specified at initialization</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL_FC", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL_FC", "type": "class", "doc": "<p>Generates the functions necessary to build a masked features SSL: real_lens some of the input features randomly\nand predict the initial data</p>\n", "bases": "MaskedFramesSSL"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL_FC.__init__", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL_FC.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>dims : torch.Size\n    the shape of features in model input\nnum_f_maps : torch.Size\n    shape of feature extraction output\nfrac_masked : float, default 0.1\n    fraction of frames to real_lens\nnum_ssl_layers : int, default 5\n    number of layers in the SSL module\nnum_ssl_f_maps : int, default 16\n    number of feature maps in the SSL module</p>\n", "signature": "(\n    self,\n    dims: torch.Size,\n    num_f_maps: torch.Size,\n    frac_masked: float = 0.1,\n    num_ssl_layers: int = 3,\n    num_ssl_f_maps: int = 16\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL_FC.construct_module", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL_FC.construct_module", "type": "function", "doc": "<p>A fully connected module</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL_TCN", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL_TCN", "type": "class", "doc": "<p>Generates the functions necessary to build a masked features SSL: real_lens some of the input features randomly\nand predict the initial data</p>\n", "bases": "MaskedFramesSSL"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL_TCN.__init__", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL_TCN.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>dims : torch.Size\n    the number of features in model input\nnum_f_maps : torch.Size\n    shape of feature extraction output\nfrac_masked : float, default 0.1\n    fraction of frames to real_lens\nnum_ssl_layers : int, default 5\n    number of layers in the SSL module</p>\n", "signature": "(\n    self,\n    dims: torch.Size,\n    num_f_maps: torch.Size,\n    frac_masked: float = 0.2,\n    num_ssl_layers: int = 5\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.masked.MaskedFramesSSL_TCN.construct_module", "modulename": "dlc2action.ssl.masked", "qualname": "MaskedFramesSSL_TCN.construct_module", "type": "function", "doc": "<p>A TCN module</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.ssl.modules", "modulename": "dlc2action.ssl.modules", "type": "module", "doc": "<p>Network modules used by implementations of <code>dlc2action.ssl.base_ssl.SSLConstructor</code></p>\n"}, {"fullname": "dlc2action.ssl.segment_order", "modulename": "dlc2action.ssl.segment_order", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dlc2action.ssl.segment_order.ReverseSSL", "modulename": "dlc2action.ssl.segment_order", "qualname": "ReverseSSL", "type": "class", "doc": "<p>A flip detection SSL</p>\n\n<p>Reverse some of the segments and predict the flip with a binary classifier.</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor, abc.ABC"}, {"fullname": "dlc2action.ssl.segment_order.ReverseSSL.__init__", "modulename": "dlc2action.ssl.segment_order", "qualname": "ReverseSSL.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_f_maps : torch.Size\n    the number of input feature maps\nlen_segment : int\n    the length of the input segments</p>\n", "signature": "(self, num_f_maps: torch.Size, len_segment: int)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.segment_order.ReverseSSL.type", "modulename": "dlc2action.ssl.segment_order", "qualname": "ReverseSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'ssl_input'"}, {"fullname": "dlc2action.ssl.segment_order.ReverseSSL.transformation", "modulename": "dlc2action.ssl.segment_order", "qualname": "ReverseSSL.transformation", "type": "function", "doc": "<p>Do the flip</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.segment_order.ReverseSSL.loss", "modulename": "dlc2action.ssl.segment_order", "qualname": "ReverseSSL.loss", "type": "function", "doc": "<p>Cross-entropy loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.segment_order.ReverseSSL.construct_module", "modulename": "dlc2action.ssl.segment_order", "qualname": "ReverseSSL.construct_module", "type": "function", "doc": "<p>Construct the SSL prediction module using the parameters specified at initialization</p>\n", "signature": "(self) -> torch.nn.modules.module.Module", "funcdef": "def"}, {"fullname": "dlc2action.ssl.segment_order.OrderSSL", "modulename": "dlc2action.ssl.segment_order", "qualname": "OrderSSL", "type": "class", "doc": "<p>An order prediction SSL</p>\n\n<p>Cut out segments from the features, permute them and predict the order.</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor, abc.ABC"}, {"fullname": "dlc2action.ssl.segment_order.OrderSSL.__init__", "modulename": "dlc2action.ssl.segment_order", "qualname": "OrderSSL.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>num_f_maps : torch.Size\n    the number of the input feature maps\nlen_segment : int\n    the length of the input segments\nnum_segments : int, default 3\n    the number of segments to permute\nssl_features : int, default 32\n    the number of features per permuted segment\nskip_frames : int, default 10\n    the number of frames to cut from each permuted segment</p>\n", "signature": "(\n    self,\n    num_f_maps: torch.Size,\n    len_segment: int,\n    num_segments: int = 3,\n    ssl_features: int = 32,\n    skip_frames: int = 10\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.segment_order.OrderSSL.type", "modulename": "dlc2action.ssl.segment_order", "qualname": "OrderSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'ssl_target'"}, {"fullname": "dlc2action.ssl.segment_order.OrderSSL.transformation", "modulename": "dlc2action.ssl.segment_order", "qualname": "OrderSSL.transformation", "type": "function", "doc": "<p>Empty transformation</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.segment_order.OrderSSL.loss", "modulename": "dlc2action.ssl.segment_order", "qualname": "OrderSSL.loss", "type": "function", "doc": "<p>Cross-entropy loss</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.segment_order.OrderSSL.construct_module", "modulename": "dlc2action.ssl.segment_order", "qualname": "OrderSSL.construct_module", "type": "function", "doc": "<p>Construct the SSL prediction module using the parameters specified at initialization</p>\n", "signature": "(self) -> torch.nn.modules.module.Module", "funcdef": "def"}, {"fullname": "dlc2action.ssl.tcc", "modulename": "dlc2action.ssl.tcc", "type": "module", "doc": "<p></p>\n"}, {"fullname": "dlc2action.ssl.tcc.TCCSSL", "modulename": "dlc2action.ssl.tcc", "qualname": "TCCSSL", "type": "class", "doc": "<p>A contrastive SSL class with an NT-Xent loss</p>\n\n<p>The SSL input and target are left empty (the SSL input is generated as an augmentation of the\ninput sample at runtime).</p>\n", "bases": "dlc2action.ssl.base_ssl.SSLConstructor"}, {"fullname": "dlc2action.ssl.tcc.TCCSSL.__init__", "modulename": "dlc2action.ssl.tcc", "qualname": "TCCSSL.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    num_f_maps: torch.Size,\n    len_segment: int,\n    projection_head_f_maps: int = None,\n    loss_type: str = 'regression_mse_var',\n    variance_lambda: float = 0.001,\n    normalize_indices: bool = True,\n    normalize_embeddings: bool = False,\n    similarity_type: str = 'l2',\n    num_cycles: int = 20,\n    cycle_length: int = 2,\n    temperature: float = 0.1,\n    label_smoothing: float = 0.1\n)", "funcdef": "def"}, {"fullname": "dlc2action.ssl.tcc.TCCSSL.type", "modulename": "dlc2action.ssl.tcc", "qualname": "TCCSSL.type", "type": "variable", "doc": "<p>The <code>type</code> parameter defines interaction with the model:</p>\n\n<ul>\n<li><code>'ssl_input'</code>: a modification of the input data passes through the base network feature extraction module and the\nSSL module; it is returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'ssl_output'</code>:  the input data passes through the base network feature extraction module and the SSL module; it\nis returned as SSL output and compared to SSL target (or, if it is None, to the input data),</li>\n<li><code>'contrastive'</code>:  the input data and its modification pass through the base network feature extraction module and\nthe SSL module; an (input results, modification results) tuple is returned as SSL output,</li>\n<li><code>'contrastive_2layers'</code>: the input data and its modification pass through the base network feature extraction module; \nthe output of the second feature extraction layer for the modified data goes through an SSL module and then, \noptionally, that result and the first-level unmodified features pass another transformation; \nan (input results, modified results) tuple is returned as SSL output,</li>\n</ul>\n", "default_value": " = 'ssl_target'"}, {"fullname": "dlc2action.ssl.tcc.TCCSSL.transformation", "modulename": "dlc2action.ssl.tcc", "qualname": "TCCSSL.transformation", "type": "function", "doc": "<p>Empty transformation</p>\n", "signature": "(self, sample_data: Dict) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.ssl.tcc.TCCSSL.loss", "modulename": "dlc2action.ssl.tcc", "qualname": "TCCSSL.loss", "type": "function", "doc": "<p>Calculate the SSL loss</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>predicted : torch.Tensor\n    output of the SSL module\ntarget : torch.Tensor\n    augmented and stacked SSL_target</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss : float\n    the loss value</p>\n", "signature": "(self, predicted: torch.Tensor, target: torch.Tensor) -> float", "funcdef": "def"}, {"fullname": "dlc2action.ssl.tcc.TCCSSL.construct_module", "modulename": "dlc2action.ssl.tcc", "qualname": "TCCSSL.construct_module", "type": "function", "doc": "<p>Construct the SSL module</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ssl_module : torch.nn.Module\n    a neural net module that takes features extracted by a model's feature extractor as input and\n    returns SSL output</p>\n", "signature": "(self) -> Optional[torch.nn.modules.module.Module]", "funcdef": "def"}, {"fullname": "dlc2action.task", "modulename": "dlc2action.task", "type": "module", "doc": "<h2 id=\"training-and-inference\">Training and inference</h2>\n\n<p><code>dlc2action.task.universal_task.Task</code> is the class that performs training and inference while\n'dlc2action.task.task_dispatcher.TaskDispatcher<code>creates and updates</code>dlc2action.task.universal_task.Task` instances\nin accordance with task parameter dictionaries.</p>\n"}, {"fullname": "dlc2action.task.task_dispatcher", "modulename": "dlc2action.task.task_dispatcher", "type": "module", "doc": "<p>Class that provides an interface for <code>dlc2action.task.universal_task.Task</code></p>\n"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher", "type": "class", "doc": "<p>A class that manages the interactions between config dictionaries and a Task</p>\n"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.__init__", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters : dict\n    a dictionary of task parameters</p>\n", "signature": "(self, parameters: Dict)", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.complete_function_parameters", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.complete_function_parameters", "type": "function", "doc": "<p>Complete a parameter dictionary with values from other dictionaries if required by a function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters : dict\n    the function parameters dictionary\nfunction : callable\n    the function to be inspected\ngeneral_dicts : list\n    a list of dictionaries where the missing values will be pulled from</p>\n", "signature": "(parameters, function, general_dicts: List) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.complete_dataset_parameters", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.complete_dataset_parameters", "type": "function", "doc": "<p>Complete a parameter dictionary with values from other dictionaries if required by a dataset</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters : dict\n    the function parameters dictionary\ngeneral_dict : dict\n    the dictionary where the missing values will be pulled from\ndata_type : str\n    the input type of the dataset\nannotation_type : str\n    the annotation type of the dataset</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>parameters : dict\n    the updated parameter dictionary</p>\n", "signature": "(\n    parameters: dict,\n    general_dict: dict,\n    data_type: str,\n    annotation_type: str\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.check", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.check", "type": "function", "doc": "<p>Check whether there is a non-<code>None</code> value under the name key in the parameters dictionary</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters : dict\n    the dictionary to check\nname : str\n    the key to check</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : bool\n    True if a non-<code>None</code> value exists</p>\n", "signature": "(parameters: Dict, name: str) -> bool", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.get", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.get", "type": "function", "doc": "<p>Get the value under the name key or the default if it is <code>None</code> or does not exist</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters : dict\n    the dictionary to check\nname : str\n    the key to check\ndefault\n    the default value to return</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>value\n    the resulting value</p>\n", "signature": "(parameters: Dict, name: str, default)", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.make_dataloader", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.make_dataloader", "type": "function", "doc": "<p>Make a torch dataloader from a dataset</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset : dlc2action.data.dataset.BehaviorDataset\n    the dataset\nbatch_size : int\n    the batch size</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dataloader : DataLoader\n    the dataloader (or <code>None</code> if the length of the dataset is 0)</p>\n", "signature": "(\n    dataset: dlc2action.data.dataset.BehaviorDataset,\n    batch_size: int = 32,\n    shuffle: bool = False\n) -> torch.utils.data.dataloader.DataLoader", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.update_task", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.update_task", "type": "function", "doc": "<p>Update the <code>dlc2action.task.universal_task.Task</code> instance given the parameter updates</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters : dict\n    the dictionary of parameter updates</p>\n", "signature": "(self, parameters: Dict) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.train", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.train", "type": "function", "doc": "<p>Train the task and return a log of epoch-average loss and metric</p>\n\n<p>You can use the autostop parameters to finish training when the parameters are not improving. It will be\nstopped if the average value of <code>autostop_metric</code> over the last <code>autostop_interval</code> epochs is smaller than\nthe average over the previous <code>autostop_interval</code> epochs + <code>autostop_threshold</code>. For example, if the\ncurrent epoch is 120 and <code>autostop_interval</code> is 50, the averages over epochs 70-120 and 20-70 will be compared.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>trial : Trial\n    an <code>optuna</code> trial (for hyperparameter searches)\noptimized_metric : str\n    the name of the metric being optimized (for hyperparameter searches)\nto_ram : bool, default False\n    if <code>True</code>, the dataset will be loaded in RAM (this speeds up the calculations but can lead to crashes\n    if the dataset is too large)\nautostop_interval : int, default 50\n    the number of epochs to average the autostop metric over\nautostop_threshold : float, default 0.001\n    the autostop difference threshold\nautostop_metric : str, optional\n    the autostop metric (can be any one of the tracked metrics of <code>'loss'</code>)\nmain_task_on : bool, default True\n    if <code>False</code>, the main task (action segmentation) will not be used in training\nssl_on : bool, default True\n    if <code>False</code>, the SSL task will not be used in training</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss_log: list\n    a list of float loss function values for each epoch\nmetrics_log: dict\n    a dictionary of metric value logs (first-level keys are 'train' and 'val', second-level keys are metric\n    names, values are lists of function values)</p>\n", "signature": "(\n    self,\n    trial: optuna.trial._trial.Trial = None,\n    optimized_metric: str = None,\n    autostop_metric: str = None,\n    autostop_interval: int = 10,\n    autostop_threshold: float = 0.001,\n    loading_bar: bool = False\n) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.save_model", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.save_model", "type": "function", "doc": "<p>Save the model of the <code>dlc2action.task.universal_task.Task</code> instance</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>save_path : str\n    the path to the saved file</p>\n", "signature": "(self, save_path: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.evaluate", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.evaluate", "type": "function", "doc": "<p>Evaluate the Task model</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data : torch.utils.data.DataLoader | dlc2action.data.dataset.BehaviorDataset, optional\n    the data to evaluate on (if not provided, evaluate on the Task validation dataset)\naugment_n : int, default 0\n    the number of augmentations to average results over\nverbose : bool, default True\n    if True, the process is reported to standard output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss : float\n    the average value of the loss function\nssl_loss : float\n    the average value of the SSL loss function\nmetric : dict\n    a dictionary of average values of metric functions</p>\n", "signature": "(\n    self,\n    data: Union[torch.utils.data.dataloader.DataLoader, dlc2action.data.dataset.BehaviorDataset, str] = None,\n    augment_n: int = 0,\n    verbose: bool = True\n) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.evaluate_prediction", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.evaluate_prediction", "type": "function", "doc": "<p>Compute metrics for a prediction</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>prediction : torch.Tensor\n    the prediction\ndata : torch.utils.data.DataLoader | dlc2action.data.dataset.BehaviorDataset, optional\n    the data the prediction was made for (if not provided, take the validation dataset)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss : float\n    the average value of the loss function\nmetric : dict\n    a dictionary of average values of metric functions</p>\n", "signature": "(\n    self,\n    prediction: torch.Tensor,\n    data: Union[torch.utils.data.dataloader.DataLoader, dlc2action.data.dataset.BehaviorDataset, str] = None\n) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.predict", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.predict", "type": "function", "doc": "<p>Make a prediction with the Task model</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data : torch.utils.data.DataLoader | dlc2action.data.dataset.BehaviorDataset, optional\n    the data to evaluate on (if not provided, evaluate on the Task validation dataset)\nraw_output : bool, default False\n    if <code>True</code>, the raw predicted probabilities are returned\napply_primary_function : bool, default True\n    if <code>True</code>, the primary predict function is applied (to map the model output into a shape corresponding to\n    the input)\naugment_n : int, default 0\n    the number of augmentations to average results over</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>prediction : torch.Tensor\n    a prediction for the input data</p>\n", "signature": "(\n    self,\n    data: Union[torch.utils.data.dataloader.DataLoader, dlc2action.data.dataset.BehaviorDataset, str],\n    raw_output: bool = False,\n    apply_primary_function: bool = True,\n    augment_n: int = 0,\n    embedding: bool = False\n) -> torch.Tensor", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.dataset", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.dataset", "type": "function", "doc": "<p>Get a dataset</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>mode : {'train', 'val', 'test'}\n    the dataset to get</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dataset : dlc2action.data.dataset.BehaviorDataset\n    the dataset</p>\n", "signature": "(self, mode: str = 'train') -> dlc2action.data.dataset.BehaviorDataset", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.generate_full_length_prediction", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.generate_full_length_prediction", "type": "function", "doc": "<p>Compile a prediction for the original input sequences</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset : dlc2action.data.dataset.BehaviorDataset | str, optional\n    the dataset to generate a prediction for (if <code>None</code>, generate for the <code>dlc2action.task.universal_task.Task</code>\n    instance validation dataset)\naugment_n : int, default 10\n    the number of augmentations to average results over</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>prediction : dict\n    a nested dictionary where first level keys are video ids, second level keys are clip ids and values\n    are prediction tensors</p>\n", "signature": "(\n    self,\n    dataset: Union[dlc2action.data.dataset.BehaviorDataset, str] = None,\n    augment_n: int = 10\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.generate_submission", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.generate_submission", "type": "function", "doc": "<p>Generate a MABe-22 style submission dictionary</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>frame_number_map_file : str\n    path to the frame number map file\ndataset : BehaviorDataset, optional\n    the dataset to generate a prediction for (if <code>None</code>, generate for the validation dataset)\naugment_n : int, default 10\n    the number of augmentations to average results over</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>submission : dict\n    a dictionary with frame number mapping and embeddings</p>\n", "signature": "(\n    self,\n    frame_number_map_file: str,\n    dataset: Union[dlc2action.data.dataset.BehaviorDataset, str] = None,\n    augment_n: int = 10\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.behaviors_dict", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.behaviors_dict", "type": "function", "doc": "<p>Get a behavior dictionary</p>\n\n<p>Keys are label indices and values are label names.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>behaviors_dict : dict\n    behavior dictionary</p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.count_classes", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.count_classes", "type": "function", "doc": "<p>Get a dictionary of class counts in different modes</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>bouts : bool, default False\n    if <code>True</code>, instead of frame counts segment counts are returned</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>class_counts : dict\n    a dictionary where first-level keys are \"train\", \"val\" and \"test\", second-level keys are\n    class names and values are class counts (in frames)</p>\n", "signature": "(self, bouts: bool = False) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.visualize_results", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.visualize_results", "type": "function", "doc": "<p>Visualize random predictions</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>save_path : str, optional\n    the path where the plot will be saved\nadd_legend : bool, default True\n    if True, legend will be added to the plot\nground_truth : bool, default True\n    if True, ground truth will be added to the plot\ncolormap : str, default 'Accent'\n    the <code>matplotlib</code> colormap to use\nhide_axes : bool, default True\n    if <code>True</code>, the axes will be hidden on the plot\nmin_classes : int, default 1\n    the minimum number of classes in a displayed interval\nwidth : float, default 10\n    the width of the plot\nwhole_video : bool, default False\n    if <code>True</code>, whole videos are plotted instead of segments\ntransparent : bool, default False\n    if <code>True</code>, the background on the plot is transparent\ndataset : BehaviorDataset | DataLoader | str | None, optional\n    the dataset to make the prediction for (if not provided, the validation dataset is used)\ndrop_classes : set, optional\n    a set of class names to not be displayed\nsearch_classes : set, optional\n    if given, only intervals where at least one of the classes is in ground truth will be shown</p>\n", "signature": "(\n    self,\n    save_path: str = None,\n    add_legend: bool = True,\n    ground_truth: bool = True,\n    colormap: str = 'viridis',\n    hide_axes: bool = False,\n    min_classes: int = 1,\n    width: float = 10,\n    whole_video: bool = False,\n    transparent: bool = False,\n    dataset: Union[dlc2action.data.dataset.BehaviorDataset, torch.utils.data.dataloader.DataLoader, str, NoneType] = None,\n    drop_classes: Set = None,\n    search_classes: Set = None,\n    smooth_interval_prediction: int = None\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.generate_uncertainty_score", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.generate_uncertainty_score", "type": "function", "doc": "<p>Generate frame-wise scores for active learning</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>classes : list\n    a list of class names or indices; their confidence scores will be computed separately and stacked\naugment_n : int, default 0\n    the number of augmentations to average over\nmethod : {\"least_confidence\", \"entropy\"}\n    the method used to calculate the scores from the probability predictions (<code>\"least_confidence\"</code>: <code>1 - p_i</code> if\n    <code>p_i &gt; 0.5</code> or <code>p_i</code> if <code>p_i &lt; 0.5</code>; <code>\"entropy\"</code>: <code>- p_i * log(p_i) - (1 - p_i) * log(1 - p_i)</code>)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>score_dicts : dict\n    a nested dictionary where first level keys are video ids, second level keys are clip ids and values\n    are score tensors</p>\n", "signature": "(\n    self,\n    classes: List,\n    augment_n: int = 0,\n    method: str = 'least_confidence',\n    predicted: torch.Tensor = None,\n    behaviors_dict: Dict = None\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.generate_bald_score", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.generate_bald_score", "type": "function", "doc": "<p>Generate frame-wise Bayesian Active Learning by Disagreement scores for active learning</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>classes : list\n    a list of class names or indices; their confidence scores will be computed separately and stacked\naugment_n : int, default 0\n    the number of augmentations to average over\nnum_models : int, default 10\n    the number of dropout masks to apply\nkernel_size : int, default 11\n    the size of the smoothing gaussian kernel</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>score_dicts : dict\n    a nested dictionary where first level keys are video ids, second level keys are clip ids and values\n    are score tensors</p>\n", "signature": "(\n    self,\n    classes: List,\n    augment_n: int = 0,\n    num_models: int = 10,\n    kernel_size: int = 11\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.task_dispatcher.TaskDispatcher.get_normalization_stats", "modulename": "dlc2action.task.task_dispatcher", "qualname": "TaskDispatcher.get_normalization_stats", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task", "modulename": "dlc2action.task.universal_task", "type": "module", "doc": "<p>Training and inference</p>\n"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel", "type": "class", "doc": "<p>Implements data parallelism at the module level.</p>\n\n<p>This container parallelizes the application of the given <code>module</code> by\nsplitting the input across the specified devices by chunking in the batch\ndimension (other objects will be copied once per device). In the forward\npass, the module is replicated on each device, and each replica handles a\nportion of the input. During the backwards pass, gradients from each replica\nare summed into the original module.</p>\n\n<p>The batch size should be larger than the number of GPUs used.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p>It is recommended to use <code>~torch.nn.parallel.DistributedDataParallel</code>,\ninstead of this class, to do multi-GPU training, even if there is only a single\nnode. See: :ref:<code>cuda-nn-ddp-instead</code> and :ref:<code>ddp</code>.</p>\n\n</div>\n\n<p>Arbitrary positional and keyword inputs are allowed to be passed into\nDataParallel but some types are specially handled. tensors will be\n<strong>scattered</strong> on dim specified (default 0). tuple, list and dict types will\nbe shallow copied. The other types will be shared among different threads\nand can be corrupted if written to in the model's forward pass.</p>\n\n<p>The parallelized <code>module</code> must have its parameters and buffers on\n<code>device_ids[0]</code> before running this <code>~torch.nn.DataParallel</code>\nmodule.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p>In each forward, <code>module</code> is <strong>replicated</strong> on each device, so any\nupdates to the running module in <code>forward</code> will be lost. For example,\nif <code>module</code> has a counter attribute that is incremented in each\n<code>forward</code>, it will always stay at the initial value because the update\nis done on the replicas which are destroyed after <code>forward</code>. However,\n<code>~torch.nn.DataParallel</code> guarantees that the replica on\n<code>device[0]</code> will have its parameters and buffers sharing storage with\nthe base parallelized <code>module</code>. So <strong>in-place</strong> updates to the\nparameters or buffers on <code>device[0]</code> will be recorded. E.g.,\n<code>~torch.nn.BatchNorm2d</code> and <code>~torch.nn.utils.spectral_norm</code>\nrely on this behavior to update the buffers.</p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p>Forward and backward hooks defined on <code>module</code> and its submodules\nwill be invoked <code>len(device_ids)</code> times, each with inputs located on\na particular device. Particularly, the hooks are only guaranteed to be\nexecuted in correct order with respect to operations on corresponding\ndevices. For example, it is not guaranteed that hooks set via\n<code>~torch.nn.Module.register_forward_pre_hook</code> be executed before\n<code>all</code> <code>len(device_ids)</code> <code>~torch.nn.Module.forward</code> calls, but\nthat each such hook be executed before the corresponding\n<code>~torch.nn.Module.forward</code> call of that device.</p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p>When <code>module</code> returns a scalar (i.e., 0-dimensional tensor) in\n<code>forward</code>, this wrapper will return a vector of length equal to\nnumber of devices used in data parallelism, containing the result from\neach device.</p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>There is a subtlety in using the\n<code>pack sequence -&gt; recurrent network -&gt; unpack sequence</code> pattern in a\n<code>~torch.nn.Module</code> wrapped in <code>~torch.nn.DataParallel</code>.\nSee :ref:<code>pack-rnn-unpack-with-data-parallelism</code> section in FAQ for\ndetails.</p>\n\n</div>\n\n<p>Args:\n    module (Module): module to be parallelized\n    device_ids (list of int or torch.device): CUDA devices (default: all devices)\n    output_device (int or torch.device): device location of output (default: device_ids[0])</p>\n\n<p>Attributes:\n    module (Module): the module to be parallelized</p>\n\n<p>Example::</p>\n\n<pre><code>&gt;&gt;&gt; net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n&gt;&gt;&gt; output = net(input_var)  # input_var can be on any device, including CPU\n</code></pre>\n", "bases": "torch.nn.parallel.data_parallel.DataParallel"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.__init__", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.__init__", "type": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "(self, *args, **kwargs)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.freeze_feature_extractor", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.freeze_feature_extractor", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.unfreeze_feature_extractor", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.unfreeze_feature_extractor", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.transform_labels", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.transform_labels", "type": "function", "doc": "<p></p>\n", "signature": "(self, device)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.logit_scale", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.logit_scale", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.main_task_off", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.main_task_off", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.state_dict", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.state_dict", "type": "function", "doc": "<p>Returns a dictionary containing a whole state of the module.</p>\n\n<p>Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to <code>None</code> are not included.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p>Currently <code>state_dict()</code> also accepts positional arguments for\n<code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.</p>\n\n</div>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p>Please avoid the use of argument <code>destination</code> as it is not\ndesigned for end-users.</p>\n\n</div>\n\n<p>Args:\n    destination (dict, optional): If provided, the state of module will\n        be updated into the dict and the same object is returned.\n        Otherwise, an <code>OrderedDict</code> will be created and returned.\n        Default: <code>None</code>.\n    prefix (str, optional): a prefix added to parameter and buffer\n        names to compose the keys in state_dict. Default: <code>''</code>.\n    keep_vars (bool, optional): by default the <code>~torch.Tensor</code> s\n        returned in the state dict are detached from autograd. If it's\n        set to <code>True</code>, detaching will not be performed.\n        Default: <code>False</code>.</p>\n\n<p>Returns:\n    dict:\n        a dictionary containing a whole state of the module</p>\n\n<p>Example::</p>\n\n<pre><code>&gt;&gt;&gt; module.state_dict().keys()\n['bias', 'weight']\n</code></pre>\n", "signature": "(self, *args, **kwargs)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.ssl_on", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.ssl_on", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.ssl_off", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.ssl_off", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.MyDataParallel.extract_features", "modulename": "dlc2action.task.universal_task", "qualname": "MyDataParallel.extract_features", "type": "function", "doc": "<p></p>\n", "signature": "(self, x, start=0)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task", "modulename": "dlc2action.task.universal_task", "qualname": "Task", "type": "class", "doc": "<p>A universal trainer class that performs training, evaluation and prediction for all types of tasks and data</p>\n"}, {"fullname": "dlc2action.task.universal_task.Task.__init__", "modulename": "dlc2action.task.universal_task", "qualname": "Task.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>train_dataloader : torch.utils.data.DataLoader\n    a training dataloader\nmodel : dlc2action.model.base_model.Model\n    a model\nloss : callable\n    a loss function\nnum_epochs : int, default 0\n    the number of epochs\ntransformer : dlc2action.transformer.base_transformer.Transformer, optional\n    a transformer\nssl_losses : list, optional\n    a list of SSL losses\nssl_weights : list, optional\n    a list of SSL weights (if not provided initializes to 1)\nlr : float, default 1e-3\n    learning rate\nmetrics : dict, optional\n    a list of metric functions\nval_dataloader : torch.utils.data.DataLoader, optional\n    a validation dataloader\noptimizer : torch.optim.Optimizer, optional\n    an optimizer (<code>Adam</code> by default)\ndevice : str, default 'cuda'\n    the device to train the model on\nverbose : bool, default True\n    if <code>True</code>, the process is described in standard output\nlog_file : str, optional\n    the path to a text file where the process will be logged\naugment_train : {1, 0}\n    number of augmentations to apply at training\naugment_val : int, default 0\n    number of augmentations to apply at validation\nvalidation_interval : int, default 1\n    every time this number of epochs passes, validation metrics are computed\npredict_function : callable, optional\n    a function that maps probabilities to class predictions (if not provided, a default is generated)\nprimary_predict_function : callable, optional\n    a function that maps model output to probabilities (if not provided, initialized as identity)\nexclusive : bool, default True\n    set to False for multi-label classification\nignore_tags : bool, default False\n    if <code>True</code>, samples with different meta tags will be mixed in batches\nthreshold : float, default 0.5\n    the threshold used for multi-label classification default prediction function\nmodel_save_path : str, optional\n    the path to the folder where model checkpoints will be saved (checkpoints will not be saved if the path\n    is not provided)\nmodel_save_epochs : int, default 5\n    the interval for saving the model checkpoints (the last epoch is always saved)\npseudolabel : bool, default False\n    if True, the pseudolabeling procedure will be applied\npseudolabel_start : int, default 100\n    pseudolabeling starts after this epoch\ncorrection_interval : int, default 1\n    after this number of epochs, if the pseudolabeling is on, the model is trained on the labeled data and\n    new pseudolabels are generated\npseudolabel_alpha_f : float, default 3\n    the maximum value of pseudolabeling alpha\nalpha_growth_stop : int, default 600\n    pseudolabeling alpha stops growing after this epoch</p>\n", "signature": "(\n    self,\n    train_dataloader: torch.utils.data.dataloader.DataLoader,\n    model: Union[torch.nn.modules.module.Module, dlc2action.model.base_model.Model],\n    loss: Callable[[torch.Tensor, torch.Tensor], float],\n    num_epochs: int = 0,\n    transformer: dlc2action.transformer.base_transformer.Transformer = None,\n    ssl_losses: List = None,\n    ssl_weights: List = None,\n    lr: float = 0.001,\n    metrics: Dict = None,\n    val_dataloader: torch.utils.data.dataloader.DataLoader = None,\n    test_dataloader: torch.utils.data.dataloader.DataLoader = None,\n    optimizer: torch.optim.optimizer.Optimizer = None,\n    device: str = 'cuda',\n    verbose: bool = True,\n    log_file: Optional[str] = None,\n    augment_train: int = 1,\n    augment_val: int = 0,\n    validation_interval: int = 1,\n    predict_function: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n    primary_predict_function: Callable = None,\n    exclusive: bool = True,\n    ignore_tags: bool = True,\n    threshold: float = 0.5,\n    model_save_path: str = None,\n    model_save_epochs: int = 5,\n    pseudolabel: bool = False,\n    pseudolabel_start: int = 100,\n    correction_interval: int = 2,\n    pseudolabel_alpha_f: float = 3,\n    alpha_growth_stop: int = 600,\n    parallel: bool = False,\n    skip_metrics: List = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.save_checkpoint", "modulename": "dlc2action.task.universal_task", "qualname": "Task.save_checkpoint", "type": "function", "doc": "<p>Save a general checkpoint</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>checkpoint_path : str\n    the path where the checkpoint will be saved</p>\n", "signature": "(self, checkpoint_path: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.load_from_checkpoint", "modulename": "dlc2action.task.universal_task", "qualname": "Task.load_from_checkpoint", "type": "function", "doc": "<p>Load from a checkpoint</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>checkpoint_path : str\n    the path to the checkpoint\nonly_model : bool, default False\n    if <code>True</code>, only the model state dictionary will be loaded (and not the epoch and the optimizer state\n    dictionary)\nload_strict : bool, default True\n    if <code>True</code>, any inconsistencies in state dictionaries are regarded as errors</p>\n", "signature": "(\n    self,\n    checkpoint_path,\n    only_model: bool = False,\n    load_strict: bool = True\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.save_model", "modulename": "dlc2action.task.universal_task", "qualname": "Task.save_model", "type": "function", "doc": "<p>Save the model state dictionary</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>save_path : str\n    the path where the state will be saved</p>\n", "signature": "(self, save_path: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.train", "modulename": "dlc2action.task.universal_task", "qualname": "Task.train", "type": "function", "doc": "<p>Train the task and return a log of epoch-average loss and metric</p>\n\n<p>You can use the autostop parameters to finish training when the parameters are not improving. It will be\nstopped if the average value of <code>autostop_metric</code> over the last <code>autostop_interval</code> epochs is smaller than\nthe average over the previous <code>autostop_interval</code> epochs + <code>autostop_threshold</code>. For example, if the\ncurrent epoch is 120 and <code>autostop_interval</code> is 50, the averages over epochs 70-120 and 20-70 will be compared.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>trial : Trial\n    an <code>optuna</code> trial (for hyperparameter searches)\noptimized_metric : str\n    the name of the metric being optimized (for hyperparameter searches)\nto_ram : bool, default False\n    if <code>True</code>, the dataset will be loaded in RAM (this speeds up the calculations but can lead to crashes\n    if the dataset is too large)\nautostop_interval : int, default 50\n    the number of epochs to average the autostop metric over\nautostop_threshold : float, default 0.001\n    the autostop difference threshold\nautostop_metric : str, optional\n    the autostop metric (can be any one of the tracked metrics of <code>'loss'</code>)\nmain_task_on : bool, default True\n    if <code>False</code>, the main task (action segmentation) will not be used in training\nssl_on : bool, default True\n    if <code>False</code>, the SSL task will not be used in training</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss_log: list\n    a list of float loss function values for each epoch\nmetrics_log: dict\n    a dictionary of metric value logs (first-level keys are 'train' and 'val', second-level keys are metric\n    names, values are lists of function values)</p>\n", "signature": "(\n    self,\n    trial: optuna.trial._trial.Trial = None,\n    optimized_metric: str = None,\n    to_ram: bool = False,\n    autostop_interval: int = 30,\n    autostop_threshold: float = 0.001,\n    autostop_metric: str = None,\n    main_task_on: bool = True,\n    ssl_on: bool = True,\n    temporal_subsampling_size: int = None,\n    loading_bar: bool = False\n) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.evaluate_prediction", "modulename": "dlc2action.task.universal_task", "qualname": "Task.evaluate_prediction", "type": "function", "doc": "<p>Compute metrics for a prediction</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>prediction : torch.Tensor\n    the prediction\ndata : torch.utils.data.DataLoader | dlc2action.data.dataset.BehaviorDataset, optional\n    the data the prediction was made for (if not provided, take the validation dataset)\nbatch_size : int, default 32\n    the batch size</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss : float\n    the average value of the loss function\nmetric : dict\n    a dictionary of average values of metric functions</p>\n", "signature": "(\n    self,\n    prediction: Union[torch.Tensor, Dict],\n    data: Union[torch.utils.data.dataloader.DataLoader, dlc2action.data.dataset.BehaviorDataset, str] = None,\n    batch_size: int = 32\n) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.evaluate", "modulename": "dlc2action.task.universal_task", "qualname": "Task.evaluate", "type": "function", "doc": "<p>Evaluate the Task model</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data : torch.utils.data.DataLoader | dlc2action.data.dataset.BehaviorDataset, optional\n    the data to evaluate on (if not provided, evaluate on the Task validation dataset)\naugment_n : int, default 0\n    the number of augmentations to average results over\nbatch_size : int, default 32\n    the batch size\nverbose : bool, default True\n    if True, the process is reported to standard output</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>loss : float\n    the average value of the loss function\nssl_loss : float\n    the average value of the SSL loss function\nmetric : dict\n    a dictionary of average values of metric functions</p>\n", "signature": "(\n    self,\n    data: Union[torch.utils.data.dataloader.DataLoader, dlc2action.data.dataset.BehaviorDataset, str] = None,\n    augment_n: int = 0,\n    batch_size: int = 32,\n    verbose: bool = True\n) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.predict", "modulename": "dlc2action.task.universal_task", "qualname": "Task.predict", "type": "function", "doc": "<p>Make a prediction with the Task model</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data : torch.utils.data.DataLoader | dlc2action.data.dataset.BehaviorDataset | str, optional\n    the data to evaluate on (if not provided, evaluate on the Task validation dataset)\nraw_output : bool, default False\n    if <code>True</code>, the raw predicted probabilities are returned\napply_primary_function : bool, default True\n    if <code>True</code>, the primary predict function is applied (to map the model output into a shape corresponding to\n    the input)\naugment_n : int, default 0\n    the number of augmentations to average results over\nbatch_size : int, default 32\n    the batch size\ntrain_mode : bool, default False\n    if <code>True</code>, the model is used in training mode (affects dropout and batch normalization layers)\nto_ram : bool, default False\n    if <code>True</code>, the dataset will be loaded in RAM (this speeds up the calculations but can lead to crashes\n    if the dataset is too large)\nembedding : bool, default False\n    if <code>True</code>, the output of feature extractor is returned, ignoring the prediction module of the model</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>prediction : torch.Tensor\n    a prediction for the input data</p>\n", "signature": "(\n    self,\n    data: Union[torch.utils.data.dataloader.DataLoader, dlc2action.data.dataset.BehaviorDataset, str] = None,\n    raw_output: bool = False,\n    apply_primary_function: bool = True,\n    augment_n: int = 0,\n    batch_size: int = 32,\n    train_mode: bool = False,\n    to_ram: bool = False,\n    embedding: bool = False\n) -> torch.Tensor", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.dataset", "modulename": "dlc2action.task.universal_task", "qualname": "Task.dataset", "type": "function", "doc": "<p>Get a dataset</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>mode : {'train', 'val', 'test}\n    the dataset to get</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dataset : dlc2action.data.dataset.BehaviorDataset\n    the dataset</p>\n", "signature": "(self, mode='train') -> dlc2action.data.dataset.BehaviorDataset", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.dataloader", "modulename": "dlc2action.task.universal_task", "qualname": "Task.dataloader", "type": "function", "doc": "<p>Get a dataloader</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>mode : {'train', 'val', 'test}\n    the dataset to get</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dataloader : torch.utils.data.DataLoader\n    the dataloader</p>\n", "signature": "(self, mode: str = 'train') -> torch.utils.data.dataloader.DataLoader", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.generate_full_length_prediction", "modulename": "dlc2action.task.universal_task", "qualname": "Task.generate_full_length_prediction", "type": "function", "doc": "<p>Compile a prediction for the original input sequences</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset : BehaviorDataset, optional\n    the dataset to generate a prediction for (if <code>None</code>, generate for the validation dataset)\nbatch_size : int, default 32\n    the batch size\naugment_n : int, default 10\n    the number of augmentations to average results over</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>prediction : dict\n    a nested dictionary where first level keys are video ids, second level keys are clip ids and values\n    are prediction tensors</p>\n", "signature": "(self, dataset=None, batch_size=32, augment_n=10)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.generate_submission", "modulename": "dlc2action.task.universal_task", "qualname": "Task.generate_submission", "type": "function", "doc": "<p>Generate a MABe-22 style submission dictionary</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset : BehaviorDataset, optional\n    the dataset to generate a prediction for (if <code>None</code>, generate for the validation dataset)\nbatch_size : int, default 32\n    the batch size\naugment_n : int, default 10\n    the number of augmentations to average results over</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>submission : dict\n    a dictionary with frame number mapping and embeddings</p>\n", "signature": "(\n    self,\n    frame_number_map_file,\n    dataset=None,\n    batch_size=32,\n    augment_n=10\n)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.visualize_results", "modulename": "dlc2action.task.universal_task", "qualname": "Task.visualize_results", "type": "function", "doc": "<p>Visualize random predictions</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>save_path : str, optional\n    the path where the plot will be saved\nadd_legend : bool, default True\n    if <code>True</code>, legend will be added to the plot\nground_truth : bool, default True\n    if <code>True</code>, ground truth will be added to the plot\ncolormap : str, default 'Accent'\n    the <code>matplotlib</code> colormap to use\nhide_axes : bool, default True\n    if <code>True</code>, the axes will be hidden on the plot\nmin_classes : int, default 1\n    the minimum number of classes in a displayed interval\nwidth : float, default 10\n    the width of the plot\nwhole_video : bool, default False\n    if <code>True</code>, whole videos are plotted instead of segments\ntransparent : bool, default False\n    if <code>True</code>, the background on the plot is transparent\ndataset : BehaviorDataset | DataLoader | str | None, optional\n    the dataset to make the prediction for (if not provided, the validation dataset is used)\ndrop_classes : set, optional\n    a set of class names to not be displayed\nsearch_classes : set, optional\n    if given, only intervals where at least one of the classes is in ground truth will be shown</p>\n", "signature": "(\n    self,\n    save_path: str = None,\n    add_legend: bool = True,\n    ground_truth: bool = True,\n    colormap: str = 'viridis',\n    hide_axes: bool = False,\n    min_classes: int = 1,\n    width: int = 10,\n    whole_video: bool = False,\n    transparent: bool = False,\n    dataset: Union[dlc2action.data.dataset.BehaviorDataset, torch.utils.data.dataloader.DataLoader, str, NoneType] = None,\n    drop_classes: Set = None,\n    search_classes: Set = None,\n    num_samples: int = 1,\n    smooth_interval_prediction: int = None,\n    behavior_name: str = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_ssl_transformations", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_ssl_transformations", "type": "function", "doc": "<p></p>\n", "signature": "(self, ssl_transformations)", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_ssl_losses", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_ssl_losses", "type": "function", "doc": "<p>Set SSL losses</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>ssl_losses : list\n    a list of callable SSL losses</p>\n", "signature": "(self, ssl_losses: list) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_log", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_log", "type": "function", "doc": "<p>Set the log file</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>log: str\n    the mew log file path</p>\n", "signature": "(self, log: str) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_keep_target_none", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_keep_target_none", "type": "function", "doc": "<p>Set the keep_target_none parameter of the transformer</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>keep_target_none : list\n    a list of bool values</p>\n", "signature": "(self, keep_target_none: List) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_generate_ssl_input", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_generate_ssl_input", "type": "function", "doc": "<p>Set the generate_ssl_input parameter of the transformer</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>generate_ssl_input : list\n    a list of bool values</p>\n", "signature": "(self, generate_ssl_input: list) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_model", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_model", "type": "function", "doc": "<p>Set a new model</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model: Model\n    the new model</p>\n", "signature": "(self, model: dlc2action.model.base_model.Model) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_dataloaders", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_dataloaders", "type": "function", "doc": "<p>Set new dataloaders</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>train_dataloader: torch.utils.data.DataLoader\n    the new train dataloader\nval_dataloader : torch.utils.data.DataLoader\n    the new validation dataloader\ntest_dataloader : torch.utils.data.DataLoader\n    the new test dataloader</p>\n", "signature": "(\n    self,\n    train_dataloader: torch.utils.data.dataloader.DataLoader,\n    val_dataloader: torch.utils.data.dataloader.DataLoader = None,\n    test_dataloader: torch.utils.data.dataloader.DataLoader = None\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_loss", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_loss", "type": "function", "doc": "<p>Set new loss function</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>loss: callable\n    the new loss function</p>\n", "signature": "(self, loss: Callable) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_metrics", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_metrics", "type": "function", "doc": "<p>Set new metric</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>metrics : dict\n    the new metric dictionary</p>\n", "signature": "(self, metrics: dict) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_transformer", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_transformer", "type": "function", "doc": "<p>Set a new transformer</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>transformer: Transformer\n    the new transformer</p>\n", "signature": "(\n    self,\n    transformer: dlc2action.transformer.base_transformer.Transformer\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.set_predict_functions", "modulename": "dlc2action.task.universal_task", "qualname": "Task.set_predict_functions", "type": "function", "doc": "<p>Set new predict functions</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>primary_predict_function : callable\n    the new primary predict function\npredict_function : callable\n    the new predict function</p>\n", "signature": "(\n    self,\n    primary_predict_function: Callable,\n    predict_function: Callable\n) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.count_classes", "modulename": "dlc2action.task.universal_task", "qualname": "Task.count_classes", "type": "function", "doc": "<p>Get a dictionary of class counts in different modes</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>bouts : bool, default False\n    if <code>True</code>, instead of frame counts segment counts are returned</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>class_counts : dict\n    a dictionary where first-level keys are \"train\", \"val\" and \"test\", second-level keys are\n    class names and values are class counts (in frames)</p>\n", "signature": "(self, bouts: bool = False) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.behaviors_dict", "modulename": "dlc2action.task.universal_task", "qualname": "Task.behaviors_dict", "type": "function", "doc": "<p>Get a behavior dictionary</p>\n\n<p>Keys are label indices and values are label names.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>behaviors_dict : dict\n    behavior dictionary</p>\n", "signature": "(self) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.update_parameters", "modulename": "dlc2action.task.universal_task", "qualname": "Task.update_parameters", "type": "function", "doc": "<p>Update training parameters from a dictionary</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>parameters : dict\n    the update dictionary</p>\n", "signature": "(self, parameters: Dict) -> None", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.generate_uncertainty_score", "modulename": "dlc2action.task.universal_task", "qualname": "Task.generate_uncertainty_score", "type": "function", "doc": "<p>Generate frame-wise scores for active learning</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>classes : list\n    a list of class names or indices; their confidence scores will be computed separately and stacked\naugment_n : int, default 0\n    the number of augmentations to average over\nbatch_size : int, default 32\n    the batch size\nmethod : {\"least_confidence\", \"entropy\"}\n    the method used to calculate the scores from the probability predictions (<code>\"least_confidence\"</code>: <code>1 - p_i</code> if\n    <code>p_i &gt; 0.5</code> or <code>p_i</code> if <code>p_i &lt; 0.5</code>; <code>\"entropy\"</code>: <code>- p_i * log(p_i) - (1 - p_i) * log(1 - p_i)</code>)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>score_dicts : dict\n    a nested dictionary where first level keys are video ids, second level keys are clip ids and values\n    are score tensors</p>\n", "signature": "(\n    self,\n    classes: List,\n    augment_n: int = 0,\n    batch_size: int = 32,\n    method: str = 'least_confidence',\n    predicted: torch.Tensor = None,\n    behaviors_dict: Dict = None\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.generate_bald_score", "modulename": "dlc2action.task.universal_task", "qualname": "Task.generate_bald_score", "type": "function", "doc": "<p>Generate frame-wise Bayesian Active Learning by Disagreement scores for active learning</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>classes : list\n    a list of class names or indices; their confidence scores will be computed separately and stacked\naugment_n : int, default 0\n    the number of augmentations to average over\nbatch_size : int, default 32\n    the batch size\nnum_models : int, default 10\n    the number of dropout masks to apply\nkernel_size : int, default 11\n    the size of the smoothing gaussian kernel</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>score_dicts : dict\n    a nested dictionary where first level keys are video ids, second level keys are clip ids and values\n    are score tensors</p>\n", "signature": "(\n    self,\n    classes: List,\n    augment_n: int = 0,\n    batch_size: int = 32,\n    num_models: int = 10,\n    kernel_size: int = 11\n) -> Dict", "funcdef": "def"}, {"fullname": "dlc2action.task.universal_task.Task.get_normalization_stats", "modulename": "dlc2action.task.universal_task", "qualname": "Task.get_normalization_stats", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> Optional[Dict]", "funcdef": "def"}, {"fullname": "dlc2action.transformer", "modulename": "dlc2action.transformer", "type": "module", "doc": "<h2 id=\"augmentations-and-feature-concatenation\">Augmentations and feature concatenation</h2>\n\n<p>Input data in <code>dlc2action</code> is stored in feature dictionaries right up until being passed to a model.\nThat makes it possible to define universal augmentations and SSL transformations that work on a range of datasets\nnaturally. Those dictionaries are generated by a <em>feature extractor</em> when a <code>dlc2action.data.base_store.InputStore</code>\ninstance is initialised. They are\nthen (optionally) processed by SSL transformations (see <code>dlc2action.ssl) and finally merged into tensors by\n*transformers*.</code>dlc2action.transformer.base_transformer.Transformer` instances\ncan also perform augmentations before the merging.</p>\n"}, {"fullname": "dlc2action.transformer.base_transformer", "modulename": "dlc2action.transformer.base_transformer", "type": "module", "doc": "<p>Abstract parent class for transformers</p>\n"}, {"fullname": "dlc2action.transformer.base_transformer.Transformer", "modulename": "dlc2action.transformer.base_transformer", "qualname": "Transformer", "type": "class", "doc": "<p>A base class for all transformers</p>\n\n<p>A transformer should apply augmentations and generate model input and training target tensors.</p>\n\n<p>All augmentation functions need to take <code>(main_input: dict, ssl_inputs: list, ssl_targets: list)</code>\nas input and return an output of the same format. Here <code>main_input</code> is a feature dictionary of the sample\ndata, <code>ssl_inputs</code> is a list of SSL input feature dictionaries and <code>ssl_targets</code> is a list of SSL target\nfeature dictionaries. The same augmentations are applied to all inputs and then <code>None</code> values are replaced\naccording to the rules set by <code>keep_target_none</code> and <code>generate_ssl_input</code> parameters and the feature\ndictionaries are compiled into tensors.</p>\n", "bases": "abc.ABC"}, {"fullname": "dlc2action.transformer.base_transformer.Transformer.__init__", "modulename": "dlc2action.transformer.base_transformer", "qualname": "Transformer.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>augmentations : list, optional\n    a list of string names of augmentations to use (if not provided, either no augmentations are applied or\n    (if use_default_augmentations is True) a default list is used\nuse_default_augmentations : bool, default False\n    if True and augmentations are not passed, default augmentations will be applied; otherwise no augmentations\ngenerate_ssl_input : list, optional\n    a list of bool values of the length of the number of SSL modules being used; if the corresponding bool value\n    is <code>True</code>, the ssl input will be generated as a new augmentation of main input (if not provided defaults to\n    <code>False</code> for each module)\nkeep_target_none : list, optional\n    a list of bool values of the length of the number of SSL modules being used; if the corresponding bool value\n    is <code>False</code> and the SSL target is <code>None</code>, the target is set to augmented main input (if not provided defaults\n    to <code>True</code> for each module)\nssl_augmentations : list, optional\n    a list of augmentation names to be applied with generating SSL input (when <code>generate_ssl_input</code> is True)\n    (if not provided, defaults to the main augmentations list)\ngraph_features : bool, default False\n    if <code>True</code>, all features in each frame can be meaningfully reshaped to <code>(#bodyparts, #features)</code>\nbodyparts_order : list, optional\n    a list of bodypart names, optional</p>\n", "signature": "(\n    self,\n    model_name: str,\n    augmentations: List = None,\n    use_default_augmentations: bool = False,\n    generate_ssl_input: List = None,\n    keep_target_none: List = None,\n    ssl_augmentations: List = None,\n    graph_features: bool = False,\n    bodyparts_order: List = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.transformer.base_transformer.Transformer.transform", "modulename": "dlc2action.transformer.base_transformer", "qualname": "Transformer.transform", "type": "function", "doc": "<p>Apply augmentations and generate tensors from feature dictionaries</p>\n\n<p>The same augmentations are applied to all the inputs (if they have the features known to the transformer).</p>\n\n<p>If <code>generate_ssl_input</code> is set to True for some of the SSL pairs, those SSL inputs will be generated as\nanother augmentation of main_input.\nUnless <code>keep_target_none</code> is set to True, <code>None</code> SSL targets will be replaced with augmented <code>main_input</code>.\nAll features are stacked together to form a tensor of shape <code>(#features, #frames)</code> that can be passed to\na model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>main_input : dict\n    the feature dictionary of the main input\nssl_inputs : list, optional\n    a list of feature dictionaries of SSL inputs (some or all can be None)\nssl_targets : list, optional\n    a list of feature dictionaries of SSL targets (some or all can be None)\naugment : bool, default True</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>main_input : torch.Tensor\n    the augmented tensor of the main input\nssl_inputs : list, optional\n    a list of augmented tensors of SSL inputs (some or all can be None)\nssl_targets : list, optional\n    a list of augmented tensors of SSL targets (some or all can be None)</p>\n", "signature": "(\n    self,\n    main_input: Dict,\n    ssl_inputs: List = None,\n    ssl_targets: List = None,\n    augment: bool = False,\n    subsample: List = None\n) -> Tuple", "funcdef": "def"}, {"fullname": "dlc2action.transformer.base_transformer.EmptyTransformer", "modulename": "dlc2action.transformer.base_transformer", "qualname": "EmptyTransformer", "type": "class", "doc": "<p>Empty transformer class that does not apply augmentations</p>\n", "bases": "Transformer"}, {"fullname": "dlc2action.transformer.kinematic", "modulename": "dlc2action.transformer.kinematic", "type": "module", "doc": "<p>Kinematic transformer</p>\n"}, {"fullname": "dlc2action.transformer.kinematic.KinematicTransformer", "modulename": "dlc2action.transformer.kinematic", "qualname": "KinematicTransformer", "type": "class", "doc": "<p>A transformer that augments the output of the Kinematic feature extractor</p>\n\n<p>The available augmentations are <code>'rotate'</code>, <code>'mirror'</code>, <code>'shift'</code>, <code>'add_noise'</code> and <code>'zoom'</code></p>\n", "bases": "dlc2action.transformer.base_transformer.Transformer"}, {"fullname": "dlc2action.transformer.kinematic.KinematicTransformer.__init__", "modulename": "dlc2action.transformer.kinematic", "qualname": "KinematicTransformer.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>augmentations : list, optional\n    list of augmentation names to use (\"rotate\", \"mirror\", \"shift\", \"add_noise\", \"zoom\")\nuse_default_augmentations : bool, default False\n    if <code>True</code> and augmentations are not passed, default augmentations will be applied; otherwise no augmentations\nrotation_limits : list, default [-pi/2, pi/2]\n    list of float rotation angle limits (<code>[low, high]``, or</code>[[low_x, high_x], [low_y, high_y], [low_z, high_z]]`\n    for 3D data)\nmirror_dim : set, default {0}\n    set of integer indices of dimensions that can be mirrored\nnoise_std : float, default 0.05\n    standard deviation of noise\nzoom_limits : list, default [0.5, 1.5]\n    list of float zoom limits ([low, high])\nmasking_probability : float, default 0.1\n    the probability of masking a joint\ndim : int, default 2\n    the dimensionality of the input data\n**kwargs : dict\n    other parameters for the base transformer class</p>\n", "signature": "(\n    self,\n    model_name: str,\n    augmentations: List = None,\n    use_default_augmentations: bool = False,\n    rotation_limits: List = None,\n    mirror_dim: Set = None,\n    noise_std: float = 0.05,\n    zoom_limits: List = None,\n    masking_probability: float = 0.05,\n    dim: int = 2,\n    graph_features: bool = False,\n    bodyparts_order: List = None,\n    canvas_shape: List = None,\n    move_around_image_center: bool = True,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "dlc2action.utils", "modulename": "dlc2action.utils", "type": "module", "doc": "<h2 id=\"utility-functions\">Utility functions</h2>\n\n<ul>\n<li><code>TensorDict</code> is a convenient data structure for keeping (and indexing) lists of feature dictionaries,</li>\n<li><code>apply_threshold</code>, <code>apply_threshold_hysteresis</code> and <code>apply_threshold_max</code> are utility functions for\n<code>dlc2action.data.dataset.BehaviorDataset.find_valleys</code>,</li>\n<li><code>strip_suffix</code> is used to get rid of suffices if a string (usually filename) ends with one of them,</li>\n<li><code>strip_prefix</code> is used to get rid of prefixes if a string (usually filename) starts with one of them,</li>\n<li><code>rotation_matrix_2d</code> and <code>rotation_matrix_3d</code> are used to generate rotation matrices by\n<code>dlc2action.transformer.base_transformer.Transformer</code> instances</li>\n</ul>\n"}, {"fullname": "dlc2action.utils.TensorDict", "modulename": "dlc2action.utils", "qualname": "TensorDict", "type": "class", "doc": "<p>A class that handles indexing in a dictionary of tensors of the same length</p>\n"}, {"fullname": "dlc2action.utils.TensorDict.__init__", "modulename": "dlc2action.utils", "qualname": "TensorDict.__init__", "type": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>obj : dict | iterable, optional\n    either a dictionary of torch.Tensor instances of the same length or an iterable of dictionaries with\n    the same keys (if not passed, a blank TensorDict is initialized)</p>\n", "signature": "(self, obj: Union[Dict, collections.abc.Iterable] = None)", "funcdef": "def"}, {"fullname": "dlc2action.utils.TensorDict.append", "modulename": "dlc2action.utils", "qualname": "TensorDict.append", "type": "function", "doc": "<p>Append an element</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>element : dict\n    a dictionary</p>\n", "signature": "(self, element: Dict) -> None", "funcdef": "def"}, {"fullname": "dlc2action.utils.TensorDict.remove", "modulename": "dlc2action.utils", "qualname": "TensorDict.remove", "type": "function", "doc": "<p>Remove indexed elements</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>indices : list\n    the indices to remove</p>\n", "signature": "(self, indices: List) -> None", "funcdef": "def"}, {"fullname": "dlc2action.utils.apply_threshold", "modulename": "dlc2action.utils", "qualname": "apply_threshold", "type": "function", "doc": "<p>Apply a hard threshold to a tensor and return indices of the intervals that passed</p>\n\n<p>If <code>error_mask</code> is not <code>None</code>, the elements marked <code>False</code> are treated as if they did not pass the threshold.\nIf <code>min_frames</code> is not 0, the intervals are additionally filtered by length.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tensor : torch.Tensor\n    the tensor to apply the threshold to\nthreshold : float\n    the threshold\nerror_mask : torch.Tensor, optional\n    a boolean real_lens to apply to the results\nmin_frames : int, default 0\n    the minimum number of frames in the resulting intervals (shorter intervals are discarded)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>indices_start : list\n    a list of indices of the first frames of the chosen intervals\nindices_end : list\n    a list of indices of the last frames of the chosen intervals</p>\n", "signature": "(\n    tensor: torch.Tensor,\n    threshold: float,\n    low: bool = True,\n    error_mask: torch.Tensor = None,\n    min_frames: int = 0,\n    smooth_interval: int = 0,\n    masked_intervals: List = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.utils.apply_threshold_hysteresis", "modulename": "dlc2action.utils", "qualname": "apply_threshold_hysteresis", "type": "function", "doc": "<p>Apply a hysteresis threshold to a tensor and return indices of the intervals that passed</p>\n\n<p>In the chosen intervals all values pass the soft threshold and at least one value passes the hard threshold.\nIf <code>error_mask</code> is not <code>None</code>, the elements marked <code>False</code> are treated as if they did not pass the threshold.\nIf <code>min_frames</code> is not 0, the intervals are additionally filtered by length.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tensor : torch.Tensor\n    the tensor to apply the threshold to\nsoft_threshold : float\n    the soft threshold\nhard_threshold : float\n    the hard threshold\nerror_mask : torch.Tensor, optional\n    a boolean real_lens to apply to the results\nmin_frames : int, default 0\n    the minimum number of frames in the resulting intervals (shorter intervals are discarded)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>indices_start : list\n    a list of indices of the first frames of the chosen intervals\nindices_end : list\n    a list of indices of the last frames of the chosen intervals</p>\n", "signature": "(\n    tensor: torch.Tensor,\n    soft_threshold: float,\n    hard_threshold: float,\n    low: bool = True,\n    error_mask: torch.Tensor = None,\n    min_frames: int = 0,\n    smooth_interval: int = 0,\n    masked_intervals: List = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.utils.apply_threshold_max", "modulename": "dlc2action.utils", "qualname": "apply_threshold_max", "type": "function", "doc": "<p>Apply a max hysteresis threshold to a tensor and return indices of the intervals that passed</p>\n\n<p>In the chosen intervals the values at the <code>main_class</code> index are larger than the others everywhere\nand at least one value at the <code>main_class</code> index passes the threshold.\nIf <code>error_mask</code> is not <code>None</code>, the elements marked <code>False</code>are treated as if they did not pass the threshold.\nIf min_frames is not 0, the intervals are additionally filtered by length.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tensor : torch.Tensor\n    the tensor to apply the threshold to (of shape <code>(#classes, #frames)</code>)\nthreshold : float\n    the threshold\nmain_class : int\n    the class that conditions the soft threshold\nerror_mask : torch.Tensor, optional\n    a boolean real_lens to apply to the results\nmin_frames : int, default 0\n    the minimum number of frames in the resulting intervals (shorter intervals are discarded)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>indices_start : list\n    a list of indices of the first frames of the chosen intervals (along dimension 1 of input tensor)\nindices_end : list\n    a list of indices of the last frames of the chosen intervals (along dimension 1 of input tensor)</p>\n", "signature": "(\n    tensor: torch.Tensor,\n    threshold: float,\n    main_class: int,\n    error_mask: torch.Tensor = None,\n    min_frames: int = 0,\n    smooth_interval: int = 0,\n    masked_intervals: List = None\n)", "funcdef": "def"}, {"fullname": "dlc2action.utils.strip_suffix", "modulename": "dlc2action.utils", "qualname": "strip_suffix", "type": "function", "doc": "<p>Strip a suffix from a string if it is contained in a list</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>text : str\n    the main string\nsuffix : iterable\n    the list of suffices to be stripped</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : str\n    the stripped string</p>\n", "signature": "(text: str, suffix: collections.abc.Iterable)", "funcdef": "def"}, {"fullname": "dlc2action.utils.strip_prefix", "modulename": "dlc2action.utils", "qualname": "strip_prefix", "type": "function", "doc": "<p>Strip a prefix from a string if it is contained in a list</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>text : str\n    the main string\nprefix : iterable\n    the list of prefixes to be stripped</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result : str\n    the stripped string</p>\n", "signature": "(text: str, prefix: collections.abc.Iterable)", "funcdef": "def"}, {"fullname": "dlc2action.utils.rotation_matrix_2d", "modulename": "dlc2action.utils", "qualname": "rotation_matrix_2d", "type": "function", "doc": "<p>Create a tensor of 2D rotation matrices from a tensor of angles</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>angles : torch.Tensor\n    a tensor of angles of arbitrary shape <code>(...)</code></p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>rotation_matrices : torch.Tensor\n    a tensor of 2D rotation matrices of shape <code>(..., 2, 2)</code></p>\n", "signature": "(angles: torch.Tensor) -> torch.Tensor", "funcdef": "def"}, {"fullname": "dlc2action.utils.rotation_matrix_3d", "modulename": "dlc2action.utils", "qualname": "rotation_matrix_3d", "type": "function", "doc": "<p>Create a tensor of 3D rotation matrices from a tensor of angles</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>alpha : torch.Tensor\n    a tensor of rotation angles around the x axis of arbitrary shape <code>(...)</code>\nbeta : torch.Tensor\n    a tensor of rotation angles around the y axis of arbitrary shape <code>(...)</code>\ngamma : torch.Tensor\n    a tensor of rotation angles around the z axis of arbitrary shape <code>(...)</code></p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>rotation_matrices : torch.Tensor\n    a tensor of 3D rotation matrices of shape <code>(..., 3, 3)</code></p>\n", "signature": "(alpha: torch.Tensor, beta: torch.Tensor, gamma: torch.Tensor)", "funcdef": "def"}, {"fullname": "dlc2action.utils.correct_path", "modulename": "dlc2action.utils", "qualname": "correct_path", "type": "function", "doc": "<p></p>\n", "signature": "(path, project_path)", "funcdef": "def"}, {"fullname": "dlc2action.utils.TensorList", "modulename": "dlc2action.utils", "qualname": "TensorList", "type": "class", "doc": "<p>A list of tensors that can send each element to a <code>torch</code> device</p>\n", "bases": "builtins.list"}, {"fullname": "dlc2action.utils.TensorList.to_device", "modulename": "dlc2action.utils", "qualname": "TensorList.to_device", "type": "function", "doc": "<p></p>\n", "signature": "(self, device: torch.device)", "funcdef": "def"}, {"fullname": "dlc2action.utils.get_intervals", "modulename": "dlc2action.utils", "qualname": "get_intervals", "type": "function", "doc": "<p>Get a list of True group beginning and end indices from a boolean tensor</p>\n", "signature": "(tensor: torch.Tensor) -> torch.Tensor", "funcdef": "def"}, {"fullname": "dlc2action.utils.smooth", "modulename": "dlc2action.utils", "qualname": "smooth", "type": "function", "doc": "<p>Get rid of jittering in a non-exclusive classification tensor</p>\n\n<p>First, remove intervals of 0 shorter than <code>smooth_interval</code>. Then, remove intervals of 1 shorter than\n<code>smooth_interval</code>.</p>\n", "signature": "(tensor: torch.Tensor, smooth_interval: int = 0) -> torch.Tensor", "funcdef": "def"}, {"fullname": "dlc2action.utils.GaussianSmoothing", "modulename": "dlc2action.utils", "qualname": "GaussianSmoothing", "type": "class", "doc": "<p>Apply gaussian smoothing on a 1d tensor.\nFiltering is performed seperately for each channel\nin the input using a depthwise convolution.\nArguments:\n    channels (int, sequence): Number of channels of the input tensors. Output will\n        have this number of channels as well.\n    kernel_size (int, sequence): Size of the gaussian kernel.\n    sigma (float, sequence): Standard deviation of the gaussian kernel.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "dlc2action.utils.GaussianSmoothing.__init__", "modulename": "dlc2action.utils", "qualname": "GaussianSmoothing.__init__", "type": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "(self, kernel_size: int = 15, sigma: float = 1.0)", "funcdef": "def"}, {"fullname": "dlc2action.utils.GaussianSmoothing.forward", "modulename": "dlc2action.utils", "qualname": "GaussianSmoothing.forward", "type": "function", "doc": "<p>Apply gaussian filter to input.\nArguments:\n    input (torch.Tensor): Input to apply gaussian filter on.\nReturns:\n    filtered (torch.Tensor): Filtered output.</p>\n", "signature": "(self, inputs: torch.Tensor) -> torch.Tensor", "funcdef": "def"}, {"fullname": "dlc2action.utils.argrelmax", "modulename": "dlc2action.utils", "qualname": "argrelmax", "type": "function", "doc": "<p>Calculate arguments of relative maxima.\nprob: np.array. boundary probability maps distributerd in [0, 1]\nprob shape is (T)\nignore the peak whose value is under threshold\nReturn:\n    Index of peaks for each batch</p>\n", "signature": "(prob: numpy.ndarray, threshold: float = 0.7) -> List[int]", "funcdef": "def"}, {"fullname": "dlc2action.utils.decide_boundary_prob_with_similarity", "modulename": "dlc2action.utils", "qualname": "decide_boundary_prob_with_similarity", "type": "function", "doc": "<p>Decide action boundary probabilities based on adjacent frame similarities.\nArgs:\n    x: frame-wise video features (N, C, T)\nReturn:\n    boundary: action boundary probability (N, 1, T)</p>\n", "signature": "(x: torch.Tensor) -> torch.Tensor", "funcdef": "def"}, {"fullname": "dlc2action.utils.PostProcessor", "modulename": "dlc2action.utils", "qualname": "PostProcessor", "type": "class", "doc": "<p></p>\n"}, {"fullname": "dlc2action.utils.PostProcessor.__init__", "modulename": "dlc2action.utils", "qualname": "PostProcessor.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    name: str,\n    boundary_th: int = 0.7,\n    theta_t: int = 15,\n    kernel_size: int = 15\n)", "funcdef": "def"}, {"fullname": "dlc2action.version", "modulename": "dlc2action.version", "type": "module", "doc": "<p>DLC2Action Toolbox\n\u00a9 A. Mathis Lab</p>\n"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();